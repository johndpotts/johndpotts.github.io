[
    {
        "_class": "assessment",
        "id": 45789976,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use SQS short polling to retrieve messages from your Amazon SQS queues</p>",
                "<p>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</p>",
                "<p>Use SQS long polling to retrieve messages from your Amazon SQS queues</p>",
                "<p>Use SQS message timer to retrieve messages from your Amazon SQS queues</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A high-frequency stock trading firm is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to address the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SQS long polling to retrieve messages from your Amazon SQS queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the differences between Short Polling vs Long Polling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SQS short polling to retrieve messages from your Amazon SQS queues</strong> - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.</p>\n\n<p><strong>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p><strong>Use SQS message timer to retrieve messages from your Amazon SQS queues</strong> - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A high-frequency stock trading firm is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS.\n\nAs a Developer Associate, which of the following options would you recommend to address the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790022,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Increase the amount of memory available to the Lambda functions</p>",
                "<p>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</p>",
                "<p>Use reserved concurrency to account for the compute-heavy workflows</p>",
                "<p>Use provisioned concurrency to account for the compute-heavy workflows</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A cybersecurity company is running a serverless backend with several compute-heavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest as the BEST solution to address the compute-heavy workloads?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the amount of memory available to the Lambda functions</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</strong> - When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the compute-heavy workflows.</p>\n\n<p><strong>Use reserved concurrency to account for the compute-heavy workflows</strong></p>\n\n<p><strong>Use provisioned concurrency to account for the compute-heavy workflows</strong></p>\n\n<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the compute-heavy workflows. So both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A cybersecurity company is running a serverless backend with several compute-heavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions.\n\nAs a Developer Associate, which of the following options would you suggest as the BEST solution to address the compute-heavy workloads?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790080,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Use ElastiCache to improve latency and throughput for read-heavy application workloads</p>",
                "<p>Use ElastiCache to improve latency and throughput for write-heavy application workloads</p>",
                "<p>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</p>",
                "<p>Use ElastiCache to improve performance of compute-intensive workloads</p>",
                "<p>Use ElastiCache to run highly complex JOIN queries</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases.</p>\n\n<p>As a Developer Associate, which of the following use-cases would you recommend as the BEST fit for ElastiCache? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong></p>\n\n<p><strong>Use ElastiCache to improve performance of compute-intensive workloads</strong></p>\n\n<p>Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p>Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.</p>\n\n<p>Overview of Amazon ElastiCache features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.</p>\n\n<p><strong>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.</p>\n\n<p><strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases.\n\nAs a Developer Associate, which of the following use-cases would you recommend as the BEST fit for ElastiCache? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790024,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</strong></p>\n\n<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p>\n\n<p>For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable versioning on both the buckets to facilitate the correct functioning of the website</strong> - This option is a distractor and versioning will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</strong> - It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</strong> - The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n",
            "answers": [
                "<p>Enable versioning on both the buckets to facilitate correct functioning of the website</p>",
                "<p>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</p>",
                "<p>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</p>",
                "<p>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</p>"
            ],
            "question": "<p>A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website.</p>\n\n<p>Which of the following solutions can be used to address this issue?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website.\n\nWhich of the following solutions can be used to address this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789970,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application</p>",
                "<p>Make the S3 bucket public so that the application can reference the image URL for display</p>",
                "<p>Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display</p>",
                "<p>Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>As part of their on-boarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an in-house web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>\"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application\"</p>\n\n<p>On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.</p>\n\n<p>You can also use an IAM instance profile to create a pre-signed URL. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration. So for the given use-case, the object key can be retrieved from the DynamoDB table, and then the application can generate the pre-signed URL using the IAM instance profile.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Make the S3 bucket public so that the application can reference the image URL for display\" - Making the S3 bucket public would violate the security and privacy requirements for the use-case, so this option is incorrect.</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display\"</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display\"</p>\n\n<p>It's a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As part of their on-boarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an in-house web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance.\n\nAs a Developer Associate, which of the following solutions would you suggest to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790070,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.</p>\n\n<p>As a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.</p>\n\n<p>For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given use-case, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
            "answers": [
                "<p>Amazon SQS</p>",
                "<p>AWS Kinesis Data Firehose</p>",
                "<p>AWS Kinesis Data Analytics</p>",
                "<p>AWS Kinesis Data Streams</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.\n\nAs a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789978,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
                "<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</p>",
                "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
                "<p>Server-Side Encryption with Secrets Manager</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants a fully managed solution to create, rotate and remove the encryption keys.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer-managed CMK that you have already created.</p>\n\n<p>Creating your own customer-managed CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customer-managed CMKs. You can also define access controls and audit the customer-managed CMKs that you use to protect your data.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine Server-Side Encryption with Secrets Manager to create, rotate, or disable the encryption keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants a fully managed solution to create, rotate and remove the encryption keys.\n\nAs a Developer Associate, which of the following would you recommend to address the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789962,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use AWS Step Functions state machines to orchestrate the workflow</p>",
                "<p>Use AWS Glue to orchestrate the workflow</p>",
                "<p>Use AWS Batch to orchestrate the workflow</p>",
                "<p>Use AWS Step Functions activities to orchestrate the workflow</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An e-commerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions.</p>\n\n<p>Which of the following is the BEST solution to meet these business requirements?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions state machines to orchestrate the workflow</strong></p>\n\n<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>The following are key features of AWS Step Functions:</p>\n\n<p>Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows.</p>\n\n<p>Please see this note for a simple example of a State Machine:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Step Functions activities to orchestrate the workflow</strong> - In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Glue to orchestrate the workflow</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Batch to orchestrate the workflow</strong> - AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "An e-commerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions.\n\nWhich of the following is the BEST solution to meet these business requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790072,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use CloudWatch Events to identify and notify any failures in the Lambda code</p>",
                "<p>Use CodeCommit to identify and notify any failures in the Lambda code</p>",
                "<p>The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs</p>",
                "<p>Use CodeDeploy to identify and notify any failures in the Lambda code</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>\"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs\"</p>\n\n<p>When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.</p>\n\n<p>Lambda function failures are commonly caused by:</p>\n\n<p>Permissions issues\nCode issues\nNetwork issues\nThrottling\nInvoke API 500 and 502 errors</p>\n\n<p>You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named <code>/aws/lambda/&lt;function name&gt;</code>.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Use CloudWatch Events to identify and notify any failures in the Lambda code\" - Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code.</p>\n\n<p>\"Use CodeCommit to identify and notify any failures in the Lambda code\"</p>\n\n<p>\"Use CodeDeploy to identify and notify any failures in the Lambda code\"</p>\n\n<p>AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem.</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.</p>\n\n<p>Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions.\n\nAs a Developer Associate, which of the following solutions would you suggest for this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789986,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</p>",
                "<p>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</p>",
                "<p>Use Lambda functions and DynamoDB to create a custom solution for user management</p>",
                "<p>Use Lambda functions and RDS to create a custom solution for user management</p>",
                "<p>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with log-in/sign-up functionality that also supports Multi-Factor Authentication (MFA)</p>\n\n<p>Which of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</strong></p>\n\n<p><strong>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</strong></p>\n\n<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p>\n\n<p>A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito user pools provide support for sign-up and sign-in services as well as security features such as multi-factor authentication (MFA).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda functions and DynamoDB to create a custom solution for user management</strong></p>\n\n<p><strong>Use Lambda functions and RDS to create a custom solution for user management</strong></p>\n\n<p>As the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect.</p>\n\n<p><strong>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</strong> - Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multi-factor authentication (MFA).</p>\n\n<p>Please see this for more details:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Security",
        "question_plain": "A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with log-in/sign-up functionality that also supports Multi-Factor Authentication (MFA)\n\nWhich of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789964,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Enable EC2 detailed monitoring</p>",
                "<p>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</p>",
                "<p>Simply get it from the CloudWatch Metrics</p>",
                "<p>Open a support ticket with AWS</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the us-east-1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds.</p>\n\n<p>Which of the following represents the BEST solution for the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</strong></p>\n\n<p>Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable EC2 detailed monitoring</strong> - As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.</p>\n\n<p><strong>Simply get it from the CloudWatch Metrics</strong> - You can get data from metrics. The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.</p>\n\n<p><strong>Open a support ticket with AWS</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the us-east-1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds.\n\nWhich of the following represents the BEST solution for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789980,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use EC2 User Data</p>",
                "<p>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</p>",
                "<p>Use an IAM role</p>",
                "<p>Create an S3 bucket policy that authorises public access</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3.</p>\n\n<p>How would you recommend the EC2 instance access the orders securely?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an IAM role</strong></p>\n\n<p>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>\n\n<p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.</p>\n\n<p>This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</strong> - While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file.</p>\n\n<p><strong>Use EC2 User Data</strong> - EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given use-case.</p>\n\n<p><strong>Create an S3 bucket policy that authorizes public access</strong> - While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3.\n\nHow would you recommend the EC2 instance access the orders securely?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790026,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Enable X-Ray sampling</p>",
                "<p>Use Filter Expressions in the X-Ray console</p>",
                "<p>Custom configuration for the X-Ray agents</p>",
                "<p>Implement a network sampling rule</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An IT company has its serverless stack integrated with AWS X-Ray. The developer at the company has noticed a high volume of data going into X-Ray and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Enable X-Ray sampling</strong></p>\n\n<p>To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests. X-Ray sampling is enabled directly from the AWS console, hence your application code does not need to change.</p>\n\n<p>You can also customize the X-Ray sampling rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Filter Expressions in the X-Ray console</strong> - When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the X-Ray console.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html</a></p>\n\n<p><strong>Custom configuration for the X-Ray agents</strong> - You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect.</p>\n\n<p><strong>Implement a network sampling rule</strong> - This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An IT company has its serverless stack integrated with AWS X-Ray. The developer at the company has noticed a high volume of data going into X-Ray and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue.\n\nAs a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790010,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Implement Amazon ElastiCache Redis in Cluster Mode</p>",
                "<p>Install Redis on an Amazon EC2 instance</p>",
                "<p>Implement Amazon ElastiCache Memcached</p>",
                "<p>Migrate the database to Amazon Redshift</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fully-managed caching layer has to be configured in front of RDS.</p>\n\n<p>Which of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement Amazon ElastiCache Redis in Cluster-Mode</strong> - One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.</p>\n\n<p>When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.</p>\n\n<p>Redis Cluster config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Install Redis on an Amazon EC2 instance</strong> - It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation.</p>\n\n<p><strong>Implement Amazon ElastiCache Memcached</strong> - Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case.</p>\n\n<p><strong>Migrate the database to Amazon Redshift</strong> - Amazon Redshift belongs to \"Big Data as a Service\" cloud facility, while Redis can be primarily classified under \"In-Memory Databases\". \"Data Warehousing\" is the primary reason why developers consider Amazon Redshift over the competitors, whereas \"Performance\" is the key factor in picking Redis.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fully-managed caching layer has to be configured in front of RDS.\n\nWhich of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790064,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Amazon Elastic Container Service (Amazon ECS) on EC2</p>",
                "<p>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</p>",
                "<p>Amazon Elastic Container Service (Amazon ECS) on Fargate</p>",
                "<p>AWS Elastic Beanstalk</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture.</p>\n\n<p>Which of the folloiwng is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on Fargate</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>ECS Fargate Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on EC2</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution.</p>\n\n<p><strong>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</strong> - Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the use-case talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/eks/\">https://aws.amazon.com/eks/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture.\n\nWhich of the folloiwng is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790048,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Amazon ElastiCache for Redis</p>",
                "<p>Use Amazon CloudFront</p>",
                "<p>Use Amazon S3 Caching</p>",
                "<p>Use Amazon S3 Transfer Acceleration</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag.</p>\n\n<p>Which service can mitigate this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront</strong></p>\n\n<p>Storing your static content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront.</p>\n\n<p>A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs.</p>\n\n<p>CloudFront Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ElastiCache for Redis</strong> - Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy.</p>\n\n<p><strong>Use Amazon S3 Caching</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Use Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag.\n\nWhich service can mitigate this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789974,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Opt for Rolling deployment</p>",
                "<p>Opt for Immutable deployment</p>",
                "<p>Opt for In-place deployment</p>",
                "<p>Opt for Blue/Green deployment</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should re-route traffic from your application's original environment to the new environment.</p>\n\n<p>Which of the following options will meet your deployment criteria?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for Blue/Green deployment</strong> - A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:</p>\n\n<ol>\n<li>AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.</li>\n<li>Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.</li>\n<li>EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for Rolling deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for Immutable deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for In-place deployment</strong> - Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should re-route traffic from your application's original environment to the new environment.\n\nWhich of the following options will meet your deployment criteria?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790082,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>You can get the Client IP addresses from server access logs</p>",
                "<p>Use the header X-Forwarded-For</p>",
                "<p>Use the header X-Forwarded-From</p>",
                "<p>You can get the Client IP addresses from Elastic Load Balancing logs</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your company uses an Application Load Balancer to route incoming end-user traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines.</p>\n\n<p>As part of new compliance rules, you need to capture the client's IP address. How will you achieve this?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the header X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can get the Client IP addresses from server access logs</strong> - As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer.</p>\n\n<p><strong>Use the header X-Forwarded-From</strong> - This is a made-up option and given as a distractor.</p>\n\n<p><strong>You can get the Client IP addresses from Elastic Load Balancing logs</strong> - Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company uses an Application Load Balancer to route incoming end-user traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines.\n\nAs part of new compliance rules, you need to capture the client's IP address. How will you achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790042,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</p>",
                "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</p>",
                "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</p>",
                "<p>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</code></strong> - This enables detailed monitoring for a running instance.</p>\n\n<p>EC2 detailed monitoring:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</code></strong> - This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</code></strong> - This is an invalid syntax</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</code></strong> - This is an invalid syntax</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790060,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Migrate your application to AWS Lambda</p>",
                "<p>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</p>",
                "<p>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</p>",
                "<p>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to auto-scale based on the metric of 'average RAM usage' for your Amazon EC2 instances.</p>\n\n<p>Which option provides the best solution?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</strong> - You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.</p>\n\n<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</strong> - This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm.</p>\n\n<p><strong>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</strong> - By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1-minute frequency window. But, you still need to create and collect the custom metric you wish to track.</p>\n\n<p><strong>Migrate your application to AWS Lambda</strong> - This option has been added as a distractor. You cannot use Lambda for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to auto-scale based on the metric of 'average RAM usage' for your Amazon EC2 instances.\n\nWhich option provides the best solution?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789982,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</p>",
                "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</p>",
                "<p>You should use Express Workflows for workloads with high event rates and short duration</p>",
                "<p>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</p>",
                "<p>Both Standard and Express Workflows support all service integrations, activities, and design patterns</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow.</p>\n\n<p>As a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</strong> - Standard Workflows on AWS Step Functions are more suitable for long-running, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a long-running media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.</p>\n\n<p><em>You should use Express Workflows for workloads with high event rates and short duration</em>* - You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</strong> - As Step functions support any human approval steps, so this option is incorrect.</p>\n\n<p><strong>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</strong> - Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year.</p>\n\n<p><strong>Both Standard and Express Workflows support all service integrations, activities, and design patterns</strong> - Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, job-run (.sync), and Callback patterns.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/features/\">https://aws.amazon.com/step-functions/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/\">https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow.\n\nAs a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790012,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3).</p>\n\n<p>Which of the following options can be used to control access to data stored on Amazon S3? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Bucket policies, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Access Control Lists (ACLs)</strong></p>\n\n<p>Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.</p>\n\n<p>IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do.</p>\n\n<p>With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address.</p>\n\n<p>With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.</p>\n\n<p>With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Permissions boundaries, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Permissions boundaries</strong></p>\n\n<p><strong>IAM database authentication, Bucket policies</strong></p>\n\n<p>Permissions boundary - A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own.</p>\n\n<p>IAM database authentication - IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3.</p>\n\n<p>Therefore, all three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Permissions boundaries, Identity and Access Management (IAM) policies</p>",
                "<p>Query String Authentication, Permissions boundaries</p>",
                "<p>Bucket policies, Identity and Access Management (IAM) policies</p>",
                "<p>Query String Authentication, Access Control Lists (ACLs)</p>",
                "<p>IAM database authentication, Bucket policies</p>"
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Security",
        "question_plain": "A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3).\n\nWhich of the following options can be used to control access to data stored on Amazon S3? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790020,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A company has hosted its website on an Amazon S3 bucket and have used another Amazon S3 bucket for storing the rest of the assets like images, fonts, etc.</p>\n\n<p>Which technique/mechanism will help the hosted website access its assets without access/permission issues?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>S3 Cross-Origin Resource Sharing (CORS)</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration. The CORS configuration is a document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You can add the CORS configuration as the cors subresource to the bucket.</p>\n\n<p>If you are configuring CORS in the S3 console, you must use JSON to create a CORS configuration. The new S3 console only supports JSON CORS configurations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.</p>\n\n<p><strong>S3 Access Analyzer</strong> - Access Analyzer for S3 is a feature that monitors your access policies, ensuring that the policies provide only the intended access to your S3 resources. Access Analyzer for S3 evaluates your bucket access policies and enables you to discover and swiftly remediate buckets with potentially unintended access.</p>\n\n<p><strong>S3 Access Points</strong> - Amazon S3 Access Points simplifies managing data access at scale for applications using shared data sets on S3. With S3 Access Points, you can now easily create hundreds of access points per bucket, representing a new way of provisioning access to shared data sets. Access Points provide a customized path into a bucket, with a unique hostname and access policy that enforces the specific permissions and network controls for any request made through the access point.</p>\n\n<p>None of these three options will help achieve the access required for the current use-case and hence are all incorrect options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ManageCorsUsing.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ManageCorsUsing.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>S3 Transfer Acceleration</p>",
                "<p>S3 Cross-Origin Resource Sharing (CORS)</p>",
                "<p>S3 Access Analyzer</p>",
                "<p>S3 Access Points</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A company has hosted its website on an Amazon S3 bucket and have used another Amazon S3 bucket for storing the rest of the assets like images, fonts, etc.\n\nWhich technique/mechanism will help the hosted website access its assets without access/permission issues?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790008,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</p>",
                "<p>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</p>",
                "<p>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</p>",
                "<p>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects.</p>\n\n<p>As a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</strong></p>\n\n<p>S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.</p>\n\n<p>S3 Object Ownership has two settings:\n1. Object writer – The uploading account will own the object.\n2. Bucket owner preferred – The bucket owner will own the object if the object is uploaded with the <code>bucket-owner-full-control</code> canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p><strong>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</strong> - Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.</p>\n\n<p><strong>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.</p>\n\n<p>None of the above features are useful for the current scenario and hence are incorrect options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects.\n\nAs a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790000,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</p>",
                "<p>Once replication is enabled on a bucket, all old and new objects will be replicated</p>",
                "<p>S3 lifecycle actions are not replicated with S3 replication</p>",
                "<p>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</p>",
                "<p>Replicated objects do not retain metadata</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets.</p>\n\n<p>Which of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</strong> - Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.</p>\n\n<p><strong>S3 lifecycle actions are not replicated with S3 replication</strong> - With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</strong> - Object tags can be replicated across AWS Regions using Cross-Region Replication. For customers with Cross-Region Replication already enabled, new permissions are required for tags to replicate.</p>\n\n<p><strong>Once replication is enabled on a bucket, all old and new objects will be replicated</strong> - Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated.</p>\n\n<p><strong>Replicated objects do not retain metadata</strong> - You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html\">https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets.\n\nWhich of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789992,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A developer is configuring an Amazon API Gateway as a front door to expose backend business logic. To keep the solution cost-effective, the developer has opted for HTTP APIs.</p>\n\n<p>Which of the following services are not available as an HTTP API via Amazon API Gateway?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Web Application Firewall (AWS WAF)</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale.</p>\n\n<p>API Gateway REST APIs: A collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway.</p>\n\n<p>API Gateway HTTP API: A collection of routes and methods that are integrated with backend HTTP endpoints or Lambda functions. You can deploy this collection in one or more stages. Each route can expose one or more API methods that have unique HTTP verbs supported by API Gateway.</p>\n\n<p>AWS Lambda, AWS Identity and Access Management (IAM) and Amazon Cognito services are all available as HTTP APIs. AWS Web Application Firewall (AWS WAF) however, is only available in REST APIs.</p>\n\n<p>Choosing between HTTP API or REST API:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda</strong></p>\n\n<p><strong>AWS Identity and Access Management (IAM)</strong></p>\n\n<p><strong>Amazon Cognito</strong></p>\n\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>AWS Web Application Firewall (AWS WAF)</p>",
                "<p>AWS Lambda</p>",
                "<p>AWS Identity and Access Management (IAM)</p>",
                "<p>Amazon Cognito</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A developer is configuring an Amazon API Gateway as a front door to expose backend business logic. To keep the solution cost-effective, the developer has opted for HTTP APIs.\n\nWhich of the following services are not available as an HTTP API via Amazon API Gateway?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789966,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>REST APIs</p>",
                "<p>WebSocket APIs</p>",
                "<p>HTTP APIs</p>",
                "<p>REST or HTTP APIs</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications.</p>\n\n<p>Which of the following types of APIs supported by the Amazon API Gateway is the right fit?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>WebSocket APIs</strong></p>\n\n<p>In a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms.</p>\n\n<p>For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content.</p>\n\n<p>You can use API Gateway WebSocket APIs to build secure, real-time communication applications without having to provision or manage any servers to manage connections or large-scale data exchanges. Targeted use cases include real-time applications such as the following:</p>\n\n<ol>\n<li>Chat applications</li>\n<li>Real-time dashboards such as stock tickers</li>\n<li>Real-time alerts and notifications</li>\n</ol>\n\n<p>API Gateway provides WebSocket API management functionality such as the following:</p>\n\n<ol>\n<li>Monitoring and throttling of connections and messages</li>\n<li>Using AWS X-Ray to trace messages as they travel through the APIs to backend services</li>\n<li>Easy integration with HTTP/HTTPS endpoints</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>REST or HTTP APIs</strong></p>\n\n<p><strong>REST APIs</strong> - An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user.</p>\n\n<p>For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller.</p>\n\n<p><strong>HTTP APIs</strong> - HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint.</p>\n\n<p>For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client.</p>\n\n<p>Server push mechanism is not possible in REST and HTTP APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications.\n\nWhich of the following types of APIs supported by the Amazon API Gateway is the right fit?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790002,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</p>",
                "<p>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</p>",
                "<p>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</p>",
                "<p>Update stage variable value from the stage name of test to that of prod</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A development team has deployed a REST API in Amazon API Gateway to two different stages - a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage.</p>\n\n<p>Which of the following represents the optimal solution for this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Update stage variable value from the stage name of test to that of prod</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.</p>\n\n<p>Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</strong> - An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage.</p>\n\n<p><em>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</em>* - This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime.</p>\n\n<p><strong>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</strong> - For each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching. And these settings can be changed/updated at any time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "A development team has deployed a REST API in Amazon API Gateway to two different stages - a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage.\n\nWhich of the following represents the optimal solution for this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790004,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS_PROXY</p>",
                "<p>MOCK</p>",
                "<p>HTTP_PROXY</p>",
                "<p>HTTP</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams.</p>\n\n<p>Which API integration type is best suited for this requirement?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>MOCK</strong></p>\n\n<p>This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.</p>\n\n<p>In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORS-related headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_PROXY</strong> - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.</p>\n\n<p><strong>HTTP_PROXY</strong> - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.</p>\n\n<p><strong>HTTP</strong> - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams.\n\nWhich API integration type is best suited for this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790086,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Amazon Simple Queue Service (SQS)</p>",
                "<p>Amazon Kinesis Firehose</p>",
                "<p>AWS Glue</p>",
                "<p>Amazon Kinesis Data Streams</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company.</p>\n\n<p>The company needs a solution for ingesting and analyzing the multiple terabytes of real-time data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues).</p>\n\n<p><strong>Amazon Kinesis Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.</p>\n\n<p>Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a cost-effective option compared to Firehose. Therefore, KDS is the right solution.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle real-time data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company.\n\nThe company needs a solution for ingesting and analyzing the multiple terabytes of real-time data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790062,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</p>",
                "<p>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</p>",
                "<p>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</p>",
                "<p>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to fix this behavior?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</strong> - If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</strong> -  This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster.</p>\n\n<p><strong>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</strong> - This is incorrect and has been added as a distractor.</p>\n\n<p><strong>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</strong> - This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster.\n\nAs a Developer Associate, which of the following solutions would you recommend to fix this behavior?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790044,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>There is nothing called Organization Trail. The master account can, however, enable CloudTrail logging, to keep track of all activities across AWS accounts</p>",
                "<p>By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events</p>",
                "<p>Member accounts do not have access to organization trail, neither do they have access to the Amazon S3 bucket that logs the files</p>",
                "<p>Member accounts will be able to see the Organization trail, but cannot modify or delete it</p>",
                "<p>By default, CloudTrail event log files are not encrypted</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A multi-national enterprise uses AWS Organizations to manage its users across different divisions. Even though CloudTrail is enabled on the member accounts, managers have noticed that access issues to CloudTrail logs across different divisions and AWS Regions is becoming a bottleneck in troubleshooting issues. They have decided to use the organization trail to keep things simple.</p>\n\n<p>What are the important points to remember when configuring an organization trail? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>If you have created an organization in AWS Organizations, you can also create a trail that will log all events for all AWS accounts in that organization. This is referred to as an organization trail.</p>\n\n<p><strong>By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events</strong> - This is a correct statement. AWS CloudTrail supports Amazon S3 Data Events, apart from bucket Events. You can record all API actions on S3 Objects and receive detailed information such as the AWS account of the caller, IAM user role of the caller, time of the API call, IP address of the API, and other details. All events are delivered to an S3 bucket and CloudWatch Events, allowing you to take programmatic actions on the events.</p>\n\n<p><strong>Member accounts will be able to see the organization trail, but cannot modify or delete it</strong> - Organization trails must be created in the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.</p>\n\n<p>Organization trail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There is nothing called Organization Trail. The master account can, however, enable CloudTrail logging, to keep track of all activities across AWS accounts</strong> - This statement is incorrect. AWS offers Organization Trail for easy management and monitoring.</p>\n\n<p><strong>Member accounts do not have access to the organization trail, neither do they have access to the Amazon S3 bucket that logs the files</strong> - This statement is only partially correct. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.</p>\n\n<p><strong>By default, CloudTrail event log files are not encrypted</strong> - This is an incorrect statement. By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/aws-cloudtrail-supports-s3-data-events/\">https://aws.amazon.com/about-aws/whats-new/2016/11/aws-cloudtrail-supports-s3-data-events/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/\">https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A multi-national enterprise uses AWS Organizations to manage its users across different divisions. Even though CloudTrail is enabled on the member accounts, managers have noticed that access issues to CloudTrail logs across different divisions and AWS Regions is becoming a bottleneck in troubleshooting issues. They have decided to use the organization trail to keep things simple.\n\nWhat are the important points to remember when configuring an organization trail? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790076,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>There could be short-lived TCP connections between clients and instances</p>",
                "<p>For Application Load Balancers, cross-zone load balancing is disabled by default</p>",
                "<p>Sticky sessions are enabled for the load balancer</p>",
                "<p>Instances of a specific capacity type aren’t equally distributed across Availability Zones</p>",
                "<p>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others.</p>\n\n<p>Why is this happening and how can it be fixed? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Sticky sessions are enabled for the load balancer</strong> - This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible.</p>\n\n<p><strong>Instances of a specific capacity type aren’t equally distributed across Availability Zones</strong> - A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent lower-capacity instance types from having too many outstanding requests. It’s a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.</p>\n\n<p>A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of higher-capacity instance types is desirable.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There could be short-lived TCP connections between clients and instances</strong> - This is an incorrect statement. Long-lived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Long-lived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for long-lived TCP connections that might be causing routing issues in the load balancer.</p>\n\n<p><strong>For Application Load Balancers, cross-zone load balancing is disabled by default</strong> - This is an incorrect statement. With Application Load Balancers, cross-zone load balancing is always enabled.</p>\n\n<p><strong>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</strong> - This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others.\n\nWhy is this happening and how can it be fixed? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789996,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</p>",
                "<p>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</p>",
                "<p>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</p>",
                "<p>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department.</p>\n\n<p>Which of the below options is NOT feasible for cross-account access of S3 bucket objects?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</strong> - This statement is incorrect and hence the right choice for this question. IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard <code>aws</code> partition. You also have an account in China (Beijing) in the <code>aws-cn</code> partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</strong> - Use bucket policies to manage cross-account control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.</p>\n\n<p><strong>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</strong> - Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.</p>\n\n<p><strong>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</strong> - Not all AWS services support resource-based policies. This means that you can use cross-account IAM roles to centralize permission management when providing cross-account access to multiple services. Using cross-account IAM roles simplifies provisioning cross-account access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows cross-account access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use cross-account IAM roles, the object ACL must be modified.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department.\n\nWhich of the below options is NOT feasible for cross-account access of S3 bucket objects?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790028,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The EC2 instance is missing IAM permissions to join the other clusters</p>",
                "<p>The ECS agent Docker image must be re-built to connect to the other clusters</p>",
                "<p>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</p>",
                "<p>The security groups on the EC2 instance are pointing to the wrong ECS cluster</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters.  Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name.</p>\n\n<p>What is the root cause of this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</strong> - In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.</p>\n\n<p>Sample config for ECS Container Agent:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is missing IAM permissions to join the other clusters</strong> - EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case.</p>\n\n<p><strong>The ECS agent Docker image must be re-built to connect to the other clusters</strong> -  Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here.</p>\n\n<p><strong>The security groups on the EC2 instance are pointing to the wrong ECS cluster</strong> - Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters.  Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name.\n\nWhat is the root cause of this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789960,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>EBS volumes support in-flight encryption but does not support encryption at rest</p>",
                "<p>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</p>",
                "<p>EBS volumes support both in-flight encryption and encryption at rest using KMS</p>",
                "<p>EBS volumes don't support any encryption</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use.</p>\n\n<p>Which of the following statements are true regarding data security on EBS?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p>\n\n<ol>\n<li><p>Data at rest inside the volume</p></li>\n<li><p>All data moving between the volume and the instance</p></li>\n<li><p>All snapshots created from the volume</p></li>\n<li><p>All volumes created from those snapshots</p></li>\n</ol>\n\n<p><strong>EBS volumes support both in-flight encryption and encryption at rest using KMS</strong> - This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes support in-flight encryption but do not support encryption at rest</strong> - This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted.</p>\n\n<p><strong>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</strong> - This is an incorrect statement. As discussed above, data at rest is also encrypted.</p>\n\n<p><strong>EBS volumes don't support any encryption</strong> - This is an incorrect statement. Amazon EBS encryption offers a straight-forward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use.\n\nWhich of the following statements are true regarding data security on EBS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790036,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>AWS Lambda</p>",
                "<p>AWS Autoscaling</p>",
                "<p>AWS Elastic Beanstalk</p>",
                "<p>AWS CodeBuild</p>",
                "<p>AWS Serverless Application Model (AWS SAM)</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business.</p>\n\n<p>Which of the following services rely on CloudFormation to provision resources (Select two)?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes.</p>\n\n<p><strong>AWS Serverless Application Model (AWS SAM)</strong> - You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services.</p>\n\n<p><strong>AWS Autoscaling</strong> - AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c",
            "e"
        ],
        "section": "Deployment",
        "question_plain": "AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business.\n\nWhich of the following services rely on CloudFormation to provision resources (Select two)?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790058,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Account-level throttling</p>",
                "<p>Use Mapping Templates</p>",
                "<p>Assign a Security Group to your API Gateway</p>",
                "<p>Restrict access by using CORS</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API.</p>\n\n<p>Which of the following actions should you take?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Restrict access by using CORS</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Account-level throttling</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steady-state request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Account-level throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario.</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.</p>\n\n<p><strong>Assign a Security Group to your API Gateway</strong> - API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API.\n\nWhich of the following actions should you take?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790018,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Lambda layers to connect to the internet and RDS separately</p>",
                "<p>Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet</p>",
                "<p>Use Environment variables to pass in the RDS connection string</p>",
                "<p>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have migrated an on-premise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted on-premise, has been moved to an Amazon Lambda function.</p>\n\n<p>Which of the following should you implement to connect AWS Lambda function to its RDS instance?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.</p>\n\n<p>Lambda VPC Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda layers to connect to the internet and RDS separately</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice.</p>\n\n<p><strong>Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet</strong> - This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet.</p>\n\n<p><strong>Use Environment variables to pass in the RDS connection string</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/\">https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have migrated an on-premise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted on-premise, has been moved to an Amazon Lambda function.\n\nWhich of the following should you implement to connect AWS Lambda function to its RDS instance?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790050,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Setup RDS Read Replicas</p>",
                "<p>Setup ElastiCache in front of RDS</p>",
                "<p>Move to Amazon Redshift</p>",
                "<p>Switch application code to AWS Lambda for better performance</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (RDS) database that retrieves player’s scores and stats. The company is using RDS database instance type db.m5.12xlarge, which is not cost-effective for their budget.  They would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs.</p>\n\n<p>As a Developer, which of the following solutions do you recommend?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Setup ElastiCache in front of RDS</strong></p>\n\n<p>Amazon ElastiCache is an ideal front-end for data stores such as Amazon RDS, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup RDS Read Replicas</strong> - Adding read replicas would further add to the database costs and will not help in reducing latency when compared to a caching solution. So this option is ruled out.</p>\n\n<p><strong>Move to Amazon Redshift</strong> - Redshift is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more. If the company is looking at cost-cutting, moving to Redshift from RDS is not an option.</p>\n\n<p><strong>Switch application code to AWS Lambda for better performance</strong> - AWS Lambda can help in running data processing workflows. But, data still needs to be read from RDS and hence we need a solution to speed up the data reads and not before/after processing.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/database-caching/\">https://aws.amazon.com/caching/database-caching/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (RDS) database that retrieves player’s scores and stats. The company is using RDS database instance type db.m5.12xlarge, which is not cost-effective for their budget.  They would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs.\n\nAs a Developer, which of the following solutions do you recommend?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789998,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudWatch monitoring is lagging</p>",
                "<p>Configured IAM policy is wrong</p>",
                "<p>Write Capacity Units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration</p>",
                "<p>You have a hot partition</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCU’s) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'.</p>\n\n<p>What is the likely cause of this error?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>You have a hot partition</strong></p>\n\n<p>It's not always possible to distribute read and write activity evenly. When data access is imbalanced, a \"hot\" partition can receive a higher volume of read and write traffic compared to other partitions.\nTo better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity.</p>\n\n<p>ProvisionedThroughputExceededException explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p>Hot partition explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch monitoring is lagging</strong> - The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling.</p>\n\n<p><strong>Configured IAM policy is wrong</strong> - The error is not associated with authorization but to exceeding something pre-configured value. So, it's clearly not a permissions issue.</p>\n\n<p><strong>Write-capacity units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration</strong> - This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCU’s) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'.\n\nWhat is the likely cause of this error?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790040,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Mapping Templates</p>",
                "<p>Enable API Gateway Caching</p>",
                "<p>Use Stage Variables</p>",
                "<p>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have a three-tier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API.</p>\n\n<p>What can you do to improve performance?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable API Gateway Caching</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.</p>\n\n<p><strong>Use Stage Variables</strong> - Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have a three-tier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API.\n\nWhat can you do to improve performance?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790006,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The IAM user stands in an invalid state, because of conflicting policies</p>",
                "<p>The user will get access because it has an explicit allow</p>",
                "<p>The user will be denied access because one of the policies has an explicit deny on it</p>",
                "<p>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action.</p>\n\n<p>When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The user will be denied access because the policy has an explicit deny on it</strong> - User will be denied access because any explicit deny overrides the allow.</p>\n\n<p>Policy Evaluation explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IAM user stands in an invalid state, because of conflicting policies</strong> - This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.</p>\n\n<p><strong>The user will get access because it has an explicit allow</strong> - As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.</p>\n\n<p><strong>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</strong> - If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action.\n\nWhen the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789972,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use environment variables to pass operational parameters</p>",
                "<p>Assign more RAM to the function</p>",
                "<p>Move the Amazon S3 client initialization, out of your function handler</p>",
                "<p>Enable X-Ray integration</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance.</p>\n\n<p>Which of the following actions will help you implement the recommendation?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Move the Amazon S3 client initialization, out of your function handler</strong> - AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, don’t use the execution context to store user data, events, or other information with security implications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use environment variables to pass operational parameters</strong> - This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hard-coding useful information. But, this is not the right answer for the current use-case, since it talks about reusing context.</p>\n\n<p><strong>Assign more RAM to the function</strong> - Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer.</p>\n\n<p><strong>Enable X-Ray integration</strong> - You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current use-case, we already know the bottleneck that needs to be fixed and that is the context reuse.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance.\n\nWhich of the following actions will help you implement the recommendation?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790084,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Use cross-Region Read Replicas</p>",
                "<p>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</p>",
                "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>",
                "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</p>",
                "<p>Use database cloning feature of the RDS DB cluster</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>As a Senior Developer, you manage 10 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery.</p>\n\n<p>Which of the following features will help you prepare for database disaster recovery? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If it’s a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</strong> - This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "As a Senior Developer, you manage 10 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery.\n\nWhich of the following features will help you prepare for database disaster recovery? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790032,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Elastic IP</p>",
                "<p>Enable RDS read replicas</p>",
                "<p>Enable Load Balancer stickiness</p>",
                "<p>Add an ElastiCache Cluster</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have deployed a traditional 3-tier web application architecture with a Classic Load Balancer, an Auto Scaling group, and an Amazon Relational Database Service (RDS) database. Users are reporting that they have to re-authenticate into the website often.</p>\n\n<p>Which of the following represents a scalable solution to make the application tier stateless and outsource the session information?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Add an ElastiCache Cluster</strong> - To provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an ElastiCache service offering which is an In-Memory Key/Value store such as Redis and Memcached.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Elastic IP</strong> - An Elastic IP is a way to give your server a static IP address but won't solve the issue with session data.</p>\n\n<p><strong>Enable RDS read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. A replica continually synchronizes with the master database and hence might add a bit of lag based on how occupied the master database is. It will also add inconsistencies in the current scenario.</p>\n\n<p><strong>Enable Load Balancer stickiness</strong> - Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context. But, ElastiCache is much more scalable and better suited in the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "You have deployed a traditional 3-tier web application architecture with a Classic Load Balancer, an Auto Scaling group, and an Amazon Relational Database Service (RDS) database. Users are reporting that they have to re-authenticate into the website often.\n\nWhich of the following represents a scalable solution to make the application tier stateless and outsource the session information?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790074,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use the MultiPart API</p>",
                "<p>Get a service limit increase from AWS</p>",
                "<p>Use gzip compression</p>",
                "<p>Use the SQS Extended Client</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has.</p>\n\n<p>What can be done to make the queue accept messages of a larger size?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the SQS Extended Client</strong> - To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the MultiPart API</strong> - This is an incorrect statement. There is no multi-part API for Amazon Simple Queue Service.</p>\n\n<p><strong>Get a service limit increase from AWS</strong> - While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages.</p>\n\n<p><strong>Use gzip compression</strong> - You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has.\n\nWhat can be done to make the queue accept messages of a larger size?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790016,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS CloudWatch Log Agent</p>",
                "<p>CodeDeploy Agent</p>",
                "<p>Integrate with AWS CodePipeline</p>",
                "<p>Have a load balancer in front of your instances</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space.</p>\n\n<p>Which of the following will allow you to implement this?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>\"CodeDeploy Agent\"</p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudWatch Log Agent</strong> - The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case.</p>\n\n<p><strong>Integrate with AWS CodePipeline</strong> - AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance.</p>\n\n<p><strong>Have a load balancer in front of your instances</strong> - Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space.\n\nWhich of the following will allow you to implement this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790030,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Enable X-Ray Sampling</p>",
                "<p>Send only the required data from client-side</p>",
                "<p>Custom configuration of the X-Ray agents</p>",
                "<p>Enable X-Ray Deep linking to send only the most useful data to X-Ray</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company has their entire stack integrated with AWS X-Ray. A manager at the company noticed the skyrocketing AWS monthly usage charges for the X-Ray service. He tracked the abnormal bills to the high volume of input data going into X-Ray. As a Developer, you have been given the responsibility to fix this issue as quickly as possible, with minimal disruptions.</p>\n\n<p>Which of the below is the optimal choice for this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable X-Ray Sampling</strong> - By customizing sampling rules, you can control the amount of data that you record, and modify sampling behavior on the fly without modifying or redeploying your code. Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Send only the required data from client-side</strong> - Theoretically, it is possible to filter out unwanted data and send only what is deemed necessary. But, this requires custom code changes and management overhead of tracking and maintaining the code through its life cycle. So, this option is not an optimal solution for the current scenario.</p>\n\n<p><strong>Custom configuration of the X-Ray agents</strong> - Such a custom configuration is not possible and this option is an invalid one, given only as a distractor.</p>\n\n<p><strong>Enable X-Ray Deep linking to send only the most useful data to X-Ray</strong> - The AWS X-Ray console enables you to view service maps and traces for requests that your applications serve. The console's service map is a visual representation of the JSON service graph that X-Ray generates from the trace data generated by your applications. Deep linking refers to the use of routes and queries to deep-link into specific traces or filtered views of traces and the service map. This is not a useful feature for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-deeplinks.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-deeplinks.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company has their entire stack integrated with AWS X-Ray. A manager at the company noticed the skyrocketing AWS monthly usage charges for the X-Ray service. He tracked the abnormal bills to the high volume of input data going into X-Ray. As a Developer, you have been given the responsibility to fix this issue as quickly as possible, with minimal disruptions.\n\nWhich of the below is the optimal choice for this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790014,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</p>",
                "<p>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</p>",
                "<p>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</p>",
                "<p>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A development team has created a new IAM user that has <code>s3:putObject</code> permission to write to an S3 bucket. This S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the <code>PutObject</code> API.</p>\n\n<p>As a Developer Associate, how would you resolve this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</strong> - You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS. SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:</p>\n\n<ol>\n<li><p>Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.</p></li>\n<li><p>AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.</p></li>\n<li><p>Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.</p></li>\n<li><p>Amazon S3 stores the encrypted data key as metadata with the encrypted data.</p></li>\n</ol>\n\n<p>The error message indicates that your IAM user or role needs permission for the <code>kms:GenerateDataKey</code> action. This permission is required for buckets that use default encryption with a custom AWS KMS key.</p>\n\n<p>In the JSON policy documents, look for policies related to AWS KMS access. Review statements with \"Effect\": \"Allow\" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket's AWS KMS key. If this permission is missing, then add the permission to the appropriate policy.</p>\n\n<p>In the JSON policy documents, look for statements with \"Effect\": \"Deny\". Then, confirm that those statements don't deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</strong> - This is an invalid action given only as a distractor.</p>\n\n<p><strong>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption.</p>\n\n<p><strong>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A development team has created a new IAM user that has s3:putObject permission to write to an S3 bucket. This S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the PutObject API.\n\nAs a Developer Associate, how would you resolve this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789968,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
                "<p>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</p>",
                "<p>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
                "<p>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>The development team at a company wants to encrypt a 111 GB object using AWS KMS.</p>\n\n<p>Which of the following represents the best solution?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - <code>GenerateDataKey</code> API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><code>GenerateDataKey</code> returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.</p>\n\n<p>To encrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the <code>GenerateDataKey</code> operation to get a data key.</p></li>\n<li><p>Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.</p></li>\n<li><p>Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.</p></li>\n</ol>\n\n<p>To decrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.</p></li>\n<li><p>Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p><strong>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</strong> - <code>Encrypt</code> API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:</p>\n\n<ol>\n<li><p>To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.</p></li>\n<li><p>To move encrypted data from one AWS Region to another.</p></li>\n</ol>\n\n<p>Neither of the two is useful for the given scenario.</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</strong> - <code>GenerateDataKeyWithoutPlaintext</code> API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify.</p>\n\n<p><code>GenerateDataKeyWithoutPlaintext</code> is identical to the <code>GenerateDataKey</code> operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a company wants to encrypt a 111 GB object using AWS KMS.\n\nWhich of the following represents the best solution?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790066,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Create one CodePipeline for your entire flow and add a manual approval step</p>",
                "<p>Create multiple CodePipelines for each environment and link them using AWS Lambda</p>",
                "<p>Create deeply integrated AWS CodePipelines for each environment</p>",
                "<p>Use CodePipeline with Amazon Virtual Private Cloud</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process.</p>\n\n<p>Which of the following approaches supports the given workflow?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one CodePipeline for your entire flow and add a manual approval step</strong> - You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create multiple CodePipelines for each environment and link them using AWS Lambda</strong> - You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service.</p>\n\n<p><strong>Create deeply integrated AWS CodePipelines for each environment</strong> - You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs.</p>\n\n<p><strong>Use CodePipeline with Amazon Virtual Private Cloud</strong> - AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process.\n\nWhich of the following approaches supports the given workflow?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789958,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</p>",
                "<p>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</p>",
                "<p>Enable auto-scaling in the SQS queue</p>",
                "<p>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A company’s e-commerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes.</p>\n\n<p>What step will you suggest as a Developer Associate?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</strong></p>\n\n<p>Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Info on Queue Quotas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</strong> - This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity.</p>\n\n<p><strong>Enable auto-scaling in the SQS queue</strong> - SQS queues are, by definition, auto-scalable and do not need any configuration changes for auto-scaling.</p>\n\n<p><strong>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</strong> - This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Standard to FIFO queue conversion:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company’s e-commerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes.\n\nWhat step will you suggest as a Developer Associate?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789984,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</p>",
                "<p>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</p>",
                "<p>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</p>",
                "<p>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An intern at an IT company is getting started with AWS Cloud and wants to understand the following Amazon S3 bucket access policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAllS3Buckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListAllMyBuckets\"],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketLevelActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*/*\"\n        },\n        {\n            \"Sid\": \"RequireMFAForProductionBucket\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::Production/*\",\n                \"arn:aws:s3:::Production\"\n            ],\n            \"Condition\": {\n                \"NumericGreaterThanIfExists\": {\"aws:MultiFactorAuthAge\": \"1800\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>As a Developer Associate, can you help him identify what the policy is for?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</strong> - This example shows how you might create a policy that allows an Amazon S3 user to access any bucket, including updating, adding, and deleting objects. However, it explicitly denies access to the Production bucket if the user has not signed in using multi-factor authentication (MFA) within the last thirty minutes. This policy grants the permissions necessary to perform this action in the console or programmatically using the AWS CLI or AWS API.</p>\n\n<p>This policy never allows programmatic access to the Production bucket using long-term user access keys. This is accomplished using the aws:MultiFactorAuthAge condition key with the NumericGreaterThanIfExists condition operator. This policy condition returns true if MFA is not present or if the age of the MFA is greater than 30 minutes. In those situations, access is denied. To access the Production bucket programmatically, the S3 user must use temporary credentials that were generated in the last 30 minutes using the GetSessionToken API operation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</strong></p>\n\n<p><strong>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</strong></p>\n\n<p><strong>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An intern at an IT company is getting started with AWS Cloud and wants to understand the following Amazon S3 bucket access policy:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAllS3Buckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListAllMyBuckets\"],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketLevelActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*/*\"\n        },\n        {\n            \"Sid\": \"RequireMFAForProductionBucket\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::Production/*\",\n                \"arn:aws:s3:::Production\"\n            ],\n            \"Condition\": {\n                \"NumericGreaterThanIfExists\": {\"aws:MultiFactorAuthAge\": \"1800\"}\n            }\n        }\n    ]\n}\n\n\nAs a Developer Associate, can you help him identify what the policy is for?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790056,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Choose a high-performance instance type for your CodeBuild instances</p>",
                "<p>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</p>",
                "<p>Run CodeBuild in an Auto Scaling group</p>",
                "<p>Enable CodeBuild Auto Scaling</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An organization is moving its on-premises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel.</p>\n\n<p>Which of the following options should the organization choose for their requirement?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</strong> - AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a high-performance instance type for your CodeBuild instances</strong> - For the current requirement, this is will not make any difference.</p>\n\n<p><strong>Run CodeBuild in an Auto Scaling Group</strong> - AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up.</p>\n\n<p><strong>Enable CodeBuild Auto Scaling</strong> - This has been added as a distractor. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "An organization is moving its on-premises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel.\n\nWhich of the following options should the organization choose for their requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789988,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</p>",
                "<p>Enable RDS automatic backups</p>",
                "<p>Enable RDS Read replicas</p>",
                "<p>Enable RDS Multi-AZ</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your company has a three-year contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements.</p>\n\n<p>Which of the following solutions can help you meet your requirements?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</strong> - There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select “Schedule” as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select “Lambda Function” as the Target. Your Lambda will have the necessary code for snapshot functionality.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable RDS automatic backups</strong> - You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.</p>\n\n<p><strong>Enable RDS Read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy read-only data workloads. These are not suitable for the given use-case.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - Multi-AZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company has a three-year contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements.\n\nWhich of the following solutions can help you meet your requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790078,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use of IAM Roles</p>",
                "<p>Use of Security Groups</p>",
                "<p>Use of Access Control Lists (ACLs)</p>",
                "<p>Use of Bucket Policies</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A recruit has created an Amazon Simple Storage Service (S3) bucket. He needs assistance in getting the security principles right for this bucket.</p>\n\n<p>Which of the following is NOT a security practice for access control to S3 buckets?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use of Security Groups</strong> - A security group acts as a virtual firewall for your instances to control incoming and outgoing traffic. S3 is a managed object storage service. Security Groups are not meant for S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use of IAM Roles</strong> - For applications on Amazon EC2 or other AWS services to access Amazon S3 resources, they must include valid AWS credentials in their AWS API requests. IAM role is the perfect option in such scenarios. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an Amazon EC2 instance or AWS service such as AWS Lambda. The role supplies temporary permissions that applications can use when they make calls to other AWS resources.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q57-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#roles\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#roles</a></p>\n\n<p><strong>Use of Access Control Lists (ACLs)</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. When a request is received against a resource, Amazon S3 checks the corresponding ACL to verify that the requester has the necessary access permissions.</p>\n\n<p><strong>Use of Bucket Policies</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A recruit has created an Amazon Simple Storage Service (S3) bucket. He needs assistance in getting the security principles right for this bucket.\n\nWhich of the following is NOT a security practice for access control to S3 buckets?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790034,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The Load Balancer does not have stickiness enabled</p>",
                "<p>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</p>",
                "<p>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</p>",
                "<p>The Load Balancer does not have TLS enabled</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You are running a cloud file storage website with an Internet-facing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to re-authenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment.</p>\n\n<p>What could be the reason?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The Load Balancer does not have stickiness enabled</strong> - Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</strong> - This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management.</p>\n\n<p><strong>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</strong> - This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to the server. If needed, the server can read IP addresses from this data.</p>\n\n<p><strong>The Load Balancer does not have TLS enabled</strong> - To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets. This does not help in session management.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "You are running a cloud file storage website with an Internet-facing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to re-authenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment.\n\nWhat could be the reason?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789990,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Single Instance Worker node</p>",
                "<p>Load-balancing, Autoscaling environment</p>",
                "<p>Dedicated worker environment</p>",
                "<p>Single Instance with Elastic IP</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration.</p>\n\n<p>Which of the following Elastic Beanstalk environment should you choose to meet this requirement?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><strong>Dedicated worker environment</strong> - If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>A long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Single Instance Worker node</strong> - Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice.</p>\n\n<p><strong>Load-balancing, Autoscaling environment</strong> - A load-balancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a load-balancing, autoscaling environment. This is not the right environment for the given use-case since it will add costs to the overall solution.</p>\n\n<p><strong>Single Instance with Elastic IP</strong> - A single-instance environment contains one Amazon EC2 instance with an Elastic IP address. A single-instance environment doesn't have a load balancer, which can help you reduce costs compared to a load-balancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration.\n\nWhich of the following Elastic Beanstalk environment should you choose to meet this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790052,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Tags to distinguish the different versions</p>",
                "<p>Use environment variables</p>",
                "<p>Use AWS Lambda aliases</p>",
                "<p>Deploy your Lambda in a VPC</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version.</p>\n\n<p>Which solution should you opt for?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Lambda aliases</strong> - A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function.\nEvent sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Tags to distinguish the different versions</strong> - You can tag Lambda functions to organize them by owner, project or department. Tags are freeform key-value pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given use-case.</p>\n\n<p><strong>Use environment variables</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given use-case.</p>\n\n<p><strong>Deploy your Lambda in a VPC</strong> - Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version.\n\nWhich solution should you opt for?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790038,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use S3</p>",
                "<p>Use Environment variables</p>",
                "<p>Use Stage Variables</p>",
                "<p>Use SSM Parameter Store</p>"
            ],
            "question": "<p>Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to re-deploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime.</p>\n\n<p>What option allows you to do this?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SSM Parameter Store</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given use-case, as the DevOps team does not want to re-deploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Environment variables</strong> - Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the use-case requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API.</p>\n\n<p><strong>Use Stage Variables</strong> - You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here.</p>\n\n<p><strong>Use S3</strong> - S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to re-deploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime.\n\nWhat option allows you to do this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790068,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</p>",
                "<p>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</p>",
                "<p>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</p>",
                "<p>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names.</p>\n\n<p>What is the total set size/number of environment variables you can create for AWS Lambda?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</strong></p>\n\n<p>An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names.\n\nWhat is the total set size/number of environment variables you can create for AWS Lambda?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790054,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Lambda Authorizer</p>",
                "<p>Use IAM permissions with sigv4</p>",
                "<p>Use Cognito User Pools</p>",
                "<p>Use API Gateway User Pools</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed.</p>\n\n<p>Which authentication option should you use for your API Gateway layer?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User Pools</strong> - As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda Authorizer</strong>- A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level.</p>\n\n<p><strong>Use IAM permissions with sigv4</strong> - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help.</p>\n\n<p><strong>Use API Gateway User Pools</strong> - This is a made-up option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html\">https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed.\n\nWhich authentication option should you use for your API Gateway layer?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45789994,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>The Lambda function invocation is synchronous</p>",
                "<p>The event has been processed successfully</p>",
                "<p>The Lambda function invocation is asynchronous</p>",
                "<p>The event fails all processing attempts</p>",
                "<p>The Lambda function invocation failed only once but succeeded thereafter</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions.</p>\n\n<p>As a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The Lambda function invocation is asynchronous</strong> - When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to dead-letter queue if you have configured one.</p>\n\n<p><strong>The event fails all processing attempt</strong> - A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function invocation is synchronous</strong> - When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response.</p>\n\n<p><strong>The event has been processed successfully</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p><strong>The event processing failed only once but succeeded thereafter</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions.\n\nAs a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790046,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Change the scaling metric of auto-scaling policy to network bytes</p>",
                "<p>Increase the minimum instance capacity of the Auto Scaling Group to 2</p>",
                "<p>Configure ASG fast failover</p>",
                "<p>Enable RDS Multi-AZ</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime.</p>\n\n<p>What can you do to ensure that your application remains highly available?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the minimum instance capacity of the Auto Scaling Group to 2</strong> -</p>\n\n<p>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances- one in each AZ, making the architecture disaster-proof and hence highly available.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the scaling metric of auto-scaling policy to network bytes</strong> - With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above.</p>\n\n<p><strong>Configure ASG fast failover</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime.\n\nWhat can you do to ensure that your application remains highly available?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752150,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue</p>",
                "<p>The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>MessageRetentionPeriod</code> attribute</p>",
                "<p>You can't change the queue type after you create it</p>",
                "<p>The visibility timeout value for the queue is in seconds, which defaults to 30 seconds</p>",
                "<p>Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>You can't change the queue type after you create it</strong> - You can't change the queue type after you create it and you can't convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p><strong>The visibility timeout value for the queue is in seconds, which defaults to 30 seconds</strong> - The visibility timeout for the queue is in seconds. Valid values are: An integer from 0 to 43,200 (12 hours), the Default value is 30.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue</strong> - The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.</p>\n\n<p><strong>The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>MessageRetentionPeriod</code> attribute</strong> - The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>DelaySeconds</code> attribute. <code>MessageRetentionPeriod</code> attribute controls the length of time, in seconds, for which Amazon SQS retains a message.</p>\n\n<p><strong>Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag</strong> - Queue tags are case-sensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue on creation, you must have the <code>sqs:CreateQueue</code> and <code>sqs:TagQueue</code> permissions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service.</p>\n\n<p>As a developer associate, which of the following would you identify as correct regarding the <code>CreateQueue</code> API? (Select two)</p>\n"
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service.\n\nAs a developer associate, which of the following would you identify as correct regarding the CreateQueue API? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752218,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:GetObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"false\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}</strong> - This bucket policy denies upload object (s3:PutObject) permission if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS. To ensure that a particular AWS KMS CMK be used to encrypt the objects in a bucket, you can use the <code>s3:x-amz-server-side-encryption-aws-kms-key-id</code> condition key. To specify the AWS KMS CMK, you must use a key Amazon Resource Name (ARN) that is in the \"arn:aws:kms:region:acct-id:key/key-id\" format.</p>\n\n<p>When you upload an object, you can specify the AWS KMS CMK using the <code>x-amz-server-side-encryption-aws-kms-key-id</code> header. If the header is not present in the request, Amazon S3 assumes the AWS-managed CMK.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}</strong> - The condition is incorrect in this policy. The condition should use StringNotEquals.</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:GetObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n            }\n         }\n      }\n   ]\n}</strong> - AES256 is used for Amazon S3-managed encryption keys (SSE-S3). Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"false\"\n            }\n         }\n      }\n   ]\n}</strong> - The condition is incorrect in this policy. The condition should use <code>\"s3:x-amz-server-side-encryption\":\"aws:kms\"</code>.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the <code>x-amz-server-side-encryption</code> header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket  - <code>examplebucket</code>.</p>\n\n<p>Which of the following policies is the right fit for the given requirement?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket  - examplebucket.\n\nWhich of the following policies is the right fit for the given requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752196,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Docker volumes</p>",
                "<p>Bind mounts</p>",
                "<p>AWS Gateway Storage volumes</p>",
                "<p>Amazon EFS volumes</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EFS volumes</strong> - EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. Amazon EFS volumes are supported for tasks hosted on Fargate or Amazon EC2 instances.</p>\n\n<p>You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference volume mounts on the container instance to use the file system.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Docker volumes</strong> -  A Docker-managed volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also referred to as plugins) are used to integrate the volumes with external storage systems, such as Amazon EBS. The built-in local volume driver or a third-party volume driver can be used. Docker volumes are only supported when running tasks on Amazon EC2 instances.</p>\n\n<p><strong>Bind mounts</strong> - A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported for tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage, and hence these are a wrong choice for this use case.</p>\n\n<p><strong>AWS Storage Gateway volumes</strong> - This is an incorrect choice, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/\">https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks.</p>\n\n<p>Which of the following solutions is the best choice for these workloads?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks.\n\nWhich of the following solutions is the best choice for these workloads?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752172,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</p>",
                "<p>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</p>",
                "<p>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</p>",
                "<p>Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.</p>\n\n<p>By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.</p>\n\n<p>If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect as already discussed.</p>\n\n<p><strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</strong> - This is a made-up option and just added as a distractor.</p>\n\n<p><strong>Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The team is, however, unable to connect to the service running on the Amazon EC2 instance.</p>\n\n<p>As a developer associate, which of the following will you recommend to fix this issue?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A development team has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The team is, however, unable to connect to the service running on the Amazon EC2 instance.\n\nAs a developer associate, which of the following will you recommend to fix this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752146,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>PurgeQueue</p>",
                "<p>DeleteQueue</p>",
                "<p>RemoveQueue</p>",
                "<p>RemovePermission</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>DeleteQueue</strong> - Deletes the queue specified by the QueueUrl, regardless of the queue's contents.  When you delete a queue, any messages in the queue are no longer available.</p>\n\n<p>When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds might succeed. For example, a SendMessage request might succeed, but after 60 seconds the queue and the message you sent no longer exist.</p>\n\n<p>When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>PurgeQueue</strong> - Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve any messages deleted from a queue. The queue however remains.</p>\n\n<p><strong>RemoveQueue</strong> - This is an invalid option, given only as a distractor.</p>\n\n<p><strong>RemovePermission</strong> - Revokes any permissions in the queue policy that matches the specified Label parameter.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing.</p>\n\n<p>Which SQS API should be used for this requirement?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing.\n\nWhich SQS API should be used for this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752174,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS Marketplace</p>",
                "<p>AWS AppSync</p>",
                "<p>AWS Service Catalog</p>",
                "<p>AWS Serverless Application Repository (SAR)</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Serverless Application Repository (SAR)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it. Instead, you can use pre-built applications from the Serverless Application Repository in your serverless architectures, helping you and your teams reduce duplicated work, ensure organizational best practices, and get to market faster. Integration with AWS Identity and Access Management (IAM) provides resource-level control of each application, enabling you to publicly share applications with everyone or privately share them with specific AWS accounts.</p>\n\n<p>Deploying applications using SAR:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/serverless/serverlessrepo/\">https://aws.amazon.com/serverless/serverlessrepo/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Marketplace</strong> - The AWS Marketplace enables qualified partners to market and sell their software to AWS Customers. AWS Marketplace is an online software store that helps customers find, buy, and immediately start using the software and services that run on AWS. AWS Marketplace is designed for Independent Software Vendors (ISVs), Value-Added Resellers (VARs), and Systems Integrators (SIs) who have software products they want to offer to customers in the cloud.</p>\n\n<p><strong>AWS AppSync</strong> - AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like AWS DynamoDB, Lambda, and more. Organizations choose to build APIs with GraphQL because it helps them develop applications faster, by giving front-end developers the ability to query multiple databases, microservices, and APIs with a single GraphQL endpoint.</p>\n\n<p><strong>AWS Service Catalog</strong> - AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata. This helps you achieve consistent governance and meet your compliance requirements while enabling users to quickly deploy only the approved IT services they need.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/serverlessrepo/\">https://aws.amazon.com/serverless/serverlessrepo/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A company has built its technology stack on AWS serverless architecture for managing all its business functions. To expedite development for a new business requirement, the company is looking at using pre-built serverless applications.</p>\n\n<p>Which AWS service represents the easiest solution to address this use-case?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A company has built its technology stack on AWS serverless architecture for managing all its business functions. To expedite development for a new business requirement, the company is looking at using pre-built serverless applications.\n\nWhich AWS service represents the easiest solution to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752160,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS CloudFormation</p>",
                "<p>AWS Cloud Development Kit (CDK)</p>",
                "<p>AWS Serverless Application Model (SAM)</p>",
                "<p>AWS CodeDeploy</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Cloud Development Kit (CDK)</strong> - The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.</p>\n\n<p>Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domain-specific languages. AWS CDK uses the familiarity and expressive power of programming languages such as JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudFormation</strong> - When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection. But, CloudFormation by itself is not sufficient for the current use case.</p>\n\n<p><strong>AWS Serverless Application Model (SAM)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it.</p>\n\n<p>AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your serverless infrastructure in concise declarative templates, SAM is the better fit. If you want to define your AWS infrastructure in a familiar programming language, as is the requirement in the current use case, AWS CDK is the right fit.</p>\n\n<p><strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy can be used with AWS CDK for deployments.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/cdk/faqs/\">https://aws.amazon.com/cdk/faqs/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages.</p>\n\n<p>Which AWS service/tool should the team use for the given use-case?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages.\n\nWhich AWS service/tool should the team use for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752178,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</p>",
                "<p>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</p>",
                "<p>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</p>",
                "<p>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</strong></p>\n\n<p>The standard AWS CDK development workflow is similar to the workflow you're already familiar as a developer. There are a few extra steps:</p>\n\n<ol>\n<li><p>Create the app from a template provided by AWS CDK - Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory for your app. Now initialize the app using the <code>cdk init</code> command, specifying the desired template (\"app\") and programming language. The <code>cdk init</code> command creates a number of files and folders inside the created home directory to help you organize the source code for your AWS CDK app.</p></li>\n<li><p>Add code to the app to create resources within stacks - Add custom code as is needed for your application.</p></li>\n<li><p>Build the app (optional) - In most programming environments, after making changes to your code, you'd build (compile) it. This isn't strictly necessary with the AWS CDK—the Toolkit does it for you so you can't forget. But you can still build manually whenever you want to catch syntax and type errors.</p></li>\n<li><p>Synthesize one or more stacks in the app to create an AWS CloudFormation template - Synthesize one or more stacks in the app to create an AWS CloudFormation template. The synthesis step catches logical errors in defining your AWS resources. If your app contains more than one stack, you'd need to specify which stack(s) to synthesize.</p></li>\n<li><p>Deploy one or more stacks to your AWS account - It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment. If your code has security implications, you'll see a summary of these and need to confirm them before deployment proceeds. <code>cdk deploy</code> is used to deploy the stack using CloudFormation templates. This command displays progress information as your stack is deployed. When it's done, the command prompt reappears.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</strong></p>\n\n<p><strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</strong></p>\n\n<p>For both these options, you cannot use AWS CloudFormation to create the app. So these options are incorrect.</p>\n\n<p><strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</strong> - You cannot have the build step after deployment. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html\">https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html</a></p>\n",
            "question": "<p>As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK).</p>\n\n<p>Which of the following represents the correct order of steps to be followed for creating an app using AWS CDK?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK).\n\nWhich of the following represents the correct order of steps to be followed for creating an app using AWS CDK?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752208,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances</p>",
                "<p>The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances</p>",
                "<p>The deployment was either run with immutable updates or in traffic splitting mode</p>",
                "<p>When a canary deployment fails, it resets the EC2 burst balances to zero</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The deployment was either run with immutable updates or in traffic splitting mode</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.</p>\n\n<p>Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.</p>\n\n<p>Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:</p>\n\n<ol>\n<li>Managed platform updates with instance replacement enabled</li>\n<li>Immutable updates</li>\n<li>Deployments with immutable updates or traffic splitting enabled</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances</strong> - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.</p>\n\n<p><strong>The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances</strong> - The traditional All-at-once deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.</p>\n\n<p><strong>When a canary deployment fails, it resets the EC2 burst balances to zero</strong> - This is incorrect and given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost.</p>\n\n<p>Which of the following options can lead to this behavior?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost.\n\nWhich of the following options can lead to this behavior?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752192,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Elastic Beanstalk will not replace the failed instances</p>",
                "<p>Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment</p>",
                "<p>Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console</p>",
                "<p>Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Elastic Beanstalk will replace them with instances running the application version from the most recent successful deployment</strong></p>\n\n<p>When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you enable connection draining, Elastic Beanstalk drains existing connections from the Amazon EC2 instances in each batch before beginning the deployment.</p>\n\n<p>If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old version. You can identify the version running on the instances in your environment on the health page in the console. This page displays the deployment ID of the most recent deployment that was executed on each instance in your environment. If you terminate instances from the failed deployment, Elastic Beanstalk replaces them with instances running the application version from the most recent successful deployment.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Elastic Beanstalk will not replace the failed instances</strong></p>\n\n<p><strong>Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment</strong></p>\n\n<p><strong>Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment.</p>\n\n<p>What will be the status of these failed instances post termination?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment.\n\nWhat will be the status of these failed instances post termination?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752124,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Outputs</p>",
                "<p>Resources</p>",
                "<p>Conditions</p>",
                "<p>Parameters</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Parameters</strong></p>\n\n<p>Parameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p>The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.</p>\n\n<p>You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.</p>\n\n<p>Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.</p>\n\n<p>Please review this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n\n<p>Please visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Resources</strong> - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.</p>\n\n<p><strong>Conditions</strong> - You actually define conditions in this section of the CloudFormation template</p>\n\n<p><strong>Outputs</strong> - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions.</p>\n\n<p>Which section of a CloudFormation template does not allow for conditions?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions.\n\nWhich section of a CloudFormation template does not allow for conditions?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752230,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>By default, user data is executed every time an EC2 instance is re-started</p>",
                "<p>When an instance is running, you can update user data by using root user credentials</p>",
                "<p>By default, scripts entered as user data are executed with root user privileges</p>",
                "<p>By default, user data runs only during the boot cycle when you first launch an instance</p>",
                "<p>By default, scripts entered as user data do not have root user privileges for executing</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p>User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.</p>\n\n<p><strong>By default, scripts entered as user data are executed with root user privileges</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.</p>\n\n<p><strong>By default, user data runs only during the boot cycle when you first launch an instance</strong> - By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, user data is executed every time an EC2 instance is re-started</strong> - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance.</p>\n\n<p><strong>When an instance is running, you can update user data by using root user credentials</strong> - You can't change the user data if the instance is running (even by using root user credentials), but you can view it.</p>\n\n<p><strong>By default, scripts entered as user data do not have root user privileges for executing</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The developer team has been told to use the user data feature of EC2 instances.</p>\n\n<p>Which of the following are true about the user data EC2 configuration? ( Select two)</p>\n"
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The developer team has been told to use the user data feature of EC2 instances.\n\nWhich of the following are true about the user data EC2 configuration? ( Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752114,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If the deployment goes wrong, reset the alias to point to the current version</p>",
                "<p>Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number in the ARN</p>",
                "<p>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new version. If the deployment goes wrong, reset the alias to point all traffic to the current version</p>",
                "<p>Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the Lambda function for handling 10% of the traffic. If the deployment goes wrong, reset the new alias to point all traffic to the most recent working alias of the Lambda function</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new version. If the deployment goes wrong, reset the alias to point all traffic to the current version</strong></p>\n\n<p>You can use versions to manage the deployment of your AWS Lambda functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. You can change the function code and settings only on the unpublished version of a function. When you publish a version, the code and most of the settings are locked to ensure a consistent experience for users of that version.</p>\n\n<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. You can use routing configuration on an alias to send a portion of traffic to a Lambda function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If the deployment goes wrong, reset the alias to point to the current version</strong> - In this case, the application uses an alias to send all traffic to the new version which does not meet the requirement of sending only a certain portion of the traffic to the new Lambda version. In addition, if the deployment goes wrong, the application would see a downtime. Hence this option is incorrect.</p>\n\n<p><strong>Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number in the ARN</strong> - In this case, the application sends all traffic to the new version which does not meet the requirement of sending only a certain portion of the traffic to the new Lambda version. In addition, if the deployment goes wrong, the application would see a downtime. Hence this option is incorrect.</p>\n\n<p><strong>Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the Lambda function for handling 10% of the traffic. If the deployment goes wrong, reset the new alias to point all traffic to the most recent working alias of the Lambda function</strong> - This option has been added as a distractor. The alias for a Lambda function can only point to a Lambda function version. It cannot point to another alias.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team at a social media company uses AWS Lambda for its serverless stack on AWS Cloud. For a new deployment, the Team Lead wants to send only a certain portion of the traffic to the new Lambda version. In case the deployment goes wrong, the solution should also support the ability to roll back to a previous version of the Lambda function, with MIMINUM downtime for the application.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to address this use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A development team at a social media company uses AWS Lambda for its serverless stack on AWS Cloud. For a new deployment, the Team Lead wants to send only a certain portion of the traffic to the new Lambda version. In case the deployment goes wrong, the solution should also support the ability to roll back to a previous version of the Lambda function, with MIMINUM downtime for the application.\n\nAs a Developer Associate, which of the following options would you recommend to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752120,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>KMS stores the CMK, and receives data from the clients, which it encrypts and sends back</p>",
                "<p>KMS receives CMK from the client at every Encrypt call, and encrypts the data with that</p>",
                "<p>KMS sends the CMK to the client, which performs the encryption and then deletes the CMK</p>",
                "<p>KMS generates a new CMK for each Encrypt call and encrypts the data with it</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>KMS stores the CMK, and receives data from the clients, which it encrypts and sends back</strong></p>\n\n<p>A customer master key (CMK) is a logical representation of a master key. The CMK includes metadata, such as the key ID, creation date, description, and key state. The CMK also contains the key material used to encrypt and decrypt data. You can generate CMKs in KMS, in an AWS CloudHSM cluster, or import them from your key management infrastructure.</p>\n\n<p>AWS KMS supports symmetric and asymmetric CMKs. A symmetric CMK represents a 256-bit key that is used for encryption and decryption. An asymmetric CMK represents an RSA key pair that is used for encryption and decryption or signing and verification (but not both), or an elliptic curve (ECC) key pair that is used for signing and verification.</p>\n\n<p>AWS KMS supports three types of CMKs: customer-managed CMKs, AWS managed CMKs, and AWS owned CMKs.</p>\n\n<p>Overview of Customer master keys (CMKs):\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>KMS receives CMK from the client at every encrypt call, and encrypts the data with that</strong> - You can import your own CMK (Customer Master Key) but it is done once and then you can encrypt/decrypt as needed.</p>\n\n<p><strong>KMS sends the CMK to the client, which performs the encryption and then deletes the CMK</strong> - KMS does not send CMK to the client, KMS itself encrypts, and then decrypts the data.</p>\n\n<p><strong>KMS generates a new CMK for each Encrypt call and encrypts the data with it</strong> - KMS does not generate a new key each time but you can have KMS rotate the keys for you. Best practices discourage extensive reuse of encryption keys so it is good practice to generate new keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>Which of the following best describes how KMS Encryption works?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "Which of the following best describes how KMS Encryption works?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752236,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Immutable</p>",
                "<p>Rolling</p>",
                "<p>All at once</p>",
                "<p>Rolling with additional batches</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Rolling</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Immutable</strong> - The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</p>\n\n<p><strong>All at once</strong> - This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.</p>\n\n<p><strong>Rolling with additional batches</strong> - This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.</p>\n\n<p>Which deployment meets this requirement without incurring additional costs?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.\n\nWhich deployment meets this requirement without incurring additional costs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752212,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudTrail</p>",
                "<p>CloudWatch Metrics</p>",
                "<p>X-Ray</p>",
                "<p>Config</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudTrail</strong></p>\n\n<p>With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to maintain a history of resource configuration changes.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>You may see scenario-based questions asking you to select one of CloudWatch vs CloudTrail vs Config. Just remember this thumb rule -</p>\n\n<p>Think resource performance monitoring, events, and alerts; think CloudWatch.</p>\n\n<p>Think account-specific activity and audit; think CloudTrail.</p>\n\n<p>Think resource-specific history, audit, and compliance; think Config.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch Metrics</strong> - CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.</p>\n\n<p>Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch cannot help determine the source for KMS API calls.</p>\n\n<p><strong>X-Ray</strong> - AWS X-Ray helps developers analyze and debug distributed applications. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray cannot help determine the source for KMS API calls.</p>\n\n<p><strong>Config</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. Config cannot help determine the source for KMS API calls.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A financial services company is undergoing a compliance audit by the regulator. The company has hundreds of IAM users that make API calls but specifically it needs to be determined who is making KMS API calls.</p>\n\n<p>Which of the following services should the audit team use?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A financial services company is undergoing a compliance audit by the regulator. The company has hundreds of IAM users that make API calls but specifically it needs to be determined who is making KMS API calls.\n\nWhich of the following services should the audit team use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752128,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use SQS to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                "<p>Use SNS to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                "<p>Use Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                "<p>Use Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong></p>\n\n<p>Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SNS to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.</p>\n\n<p><strong>Use SQS to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.</p>\n\n<p><strong>Use Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics.</p>\n\n<p>Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A company has hired you as an AWS Certified Developer Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs.</p>\n\n<p>Which solution will you recommend to address this use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "A company has hired you as an AWS Certified Developer Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs.\n\nWhich solution will you recommend to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752182,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use CloudFront Usage Plans</p>",
                "<p>Use AWS Billing Usage Plans</p>",
                "<p>Use API Gateway Usage Plans</p>",
                "<p>Use AWS Lambda Custom Authorizers</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use API Gateway Usage Plans</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.</p>\n\n<p>You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints.</p>\n\n<p>Overview of API Gateway Usage Plans and API keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Billing Usage Plans</strong> - AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs. There is no such thing as AWS Billing Usage Plans. You cannot use AWS Billing to set up public APIs for the application.</p>\n\n<p><strong>Use CloudFront Usage Plans</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. There is no such thing as CloudFront Usage Plans. You cannot use CloudFront to set up public APIs for the application.</p>\n\n<p><strong>Use AWS Lambda Custom Authorizers</strong> - Lambda is a separate service than Gateway API, therefore, it cannot be used to determine the API usage limits.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A SaaS company runs a HealthCare web application that is used worldwide by users. There have been requests by mobile developers to expose public APIs for the application-specific functionality. You decide to make the APIs available to mobile developers as product offerings.</p>\n\n<p>Which of the following options will allow you to do that?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A SaaS company runs a HealthCare web application that is used worldwide by users. There have been requests by mobile developers to expose public APIs for the application-specific functionality. You decide to make the APIs available to mobile developers as product offerings.\n\nWhich of the following options will allow you to do that?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752148,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Elastic Reserved instances</p>",
                "<p>Standard Reserved instances</p>",
                "<p>Convertible Reserved instances</p>",
                "<p>Scheduled Reserved instances</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>Reserved Instances offer significant savings on Amazon EC2 costs compared to On-Demand Instance pricing. A Reserved Instance can be purchased for a one-year or three-year commitment, with the three-year commitment offering a bigger discount. Reserved instances come with two offering classes - Standard or Convertible.</p>\n\n<p><strong>Convertible Reserved instances</strong> - A Convertible Reserved Instance can be exchanged during the term for another Convertible Reserved Instance with new attributes including instance family, instance type, platform, scope, or tenancy. This is the best fit for the current requirement.</p>\n\n<p>Please review this note on the EC2 Reserved Instance types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q19-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Elastic Reserved instances</strong> - This option has been added as a distractor. There are no Elastic Reserved Instance types.</p>\n\n<p><strong>Standard Reserved instances</strong> - With Standard Reserved Instances, some attributes, such as instance size, can be modified during the term; however, the instance family cannot be modified. You cannot exchange a Standard Reserved Instance, only modify it</p>\n\n<p><strong>Scheduled Reserved instances</strong> - Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance so that you know it is available when you need it.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A company is looking at optimizing their Amazon EC2 instance costs. Few instances are sure to run for a few years, but the instance type might change based on business requirements.</p>\n\n<p>Which EC2 instance purchasing option should they opt to meet the reduced cost criteria?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company is looking at optimizing their Amazon EC2 instance costs. Few instances are sure to run for a few years, but the instance type might change based on business requirements.\n\nWhich EC2 instance purchasing option should they opt to meet the reduced cost criteria?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752126,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Output Values in CloudFormation must have unique names across all Regions</p>",
                "<p>Exported Output Values in CloudFormation must have unique names across all Regions</p>",
                "<p>Exported Output Values in CloudFormation must have unique names within a single Region</p>",
                "<p>Output Values in CloudFormation must have unique names within a single Region</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Exported Output Values in CloudFormation must have unique names within a single Region\"</p>\n\n<p>Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>A CloudFormation template has an optional Outputs section which declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find.</p>\n\n<p>You can use the Export Output Values to export the name of the resource output for a cross-stack reference. For each AWS account, export names must be unique within a region. In this case, we would have a conflict within us-east-2.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Output Values in CloudFormation must have unique names across all Regions\"</p>\n\n<p>\"Exported Output Values in CloudFormation must have unique names across all Regions\"</p>\n\n<p>\"Output Values in CloudFormation must have unique names within a single Region\"</p>\n\n<p>These three options contradict the explanation provided earlier, hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>Your global organization has an IT infrastructure that is deployed using CloudFormation on AWS Cloud. One employee, in us-east-1 Region, has created a stack 'Application1' and made an exported output with the name 'ELBDNSName'. Another employee has created a stack for a different application 'Application2' in us-east-2 Region and also exported an output with the name 'ELBDNSName'. The first employee wanted to deploy the CloudFormation stack 'Application1' in us-east-2, but it got an error. What is the cause of the error?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "Your global organization has an IT infrastructure that is deployed using CloudFormation on AWS Cloud. One employee, in us-east-1 Region, has created a stack 'Application1' and made an exported output with the name 'ELBDNSName'. Another employee has created a stack for a different application 'Application2' in us-east-2 Region and also exported an output with the name 'ELBDNSName'. The first employee wanted to deploy the CloudFormation stack 'Application1' in us-east-2, but it got an error. What is the cause of the error?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752166,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Dedicated Instances</p>",
                "<p>Spot Instances</p>",
                "<p>Dedicated Hosts</p>",
                "<p>On-Demand Instances</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A cybersecurity firm wants to run their applications on single-tenant hardware to meet security guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A cybersecurity firm wants to run their applications on single-tenant hardware to meet security guidelines.\n\nWhich of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752152,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CodeCommit</p>",
                "<p>CloudFormation</p>",
                "<p>CodeDeploy</p>",
                "<p>Systems Manager</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudFormation</strong></p>\n\n<p>AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>AWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.</p>\n\n<p>Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes. AWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data.</p>\n\n<p>Benefits of Elastic Beanstalk:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeCommit</strong> - AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. You can use the Elastic Beanstalk Command Line Interface (CLI) to deploy your application directly from an AWS CodeCommit repository. CodeCommit cannot be used to provision your resources for the Elastic Beanstalk application.</p>\n\n<p><strong>CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy cannot be used to provision your resources for the Elastic Beanstalk application.</p>\n\n<p><strong>Systems Manager</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager cannot be used to provision your resources for the Elastic Beanstalk application.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You're a developer doing contract work for the media sector. Since you work alone, you opt for technologies that require little maintenance, which allows you to focus more on your coding. You have chosen AWS Elastic Beanstalk to assist with the deployment of your applications. While reading online documentation you find that Elastic Beanstalk relies on another AWS service to provision your resources.</p>\n\n<p>Which of the following represents this AWS service?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "You're a developer doing contract work for the media sector. Since you work alone, you opt for technologies that require little maintenance, which allows you to focus more on your coding. You have chosen AWS Elastic Beanstalk to assist with the deployment of your applications. While reading online documentation you find that Elastic Beanstalk relies on another AWS service to provision your resources.\n\nWhich of the following represents this AWS service?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752132,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>ElastiCache bundled with the application source code</p>",
                "<p>ElastiCache defined in <code>.ebextensions/</code></p>",
                "<p>RDS database defined in <code>.ebextensions/</code></p>",
                "<p>ElastiCache database defined externally and referenced through environment variables</p>",
                "<p>RDS database defined externally and referenced through environment variables</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache defined in <code>.ebextensions/</code></strong> - Any resources created as part of your <code>.ebextensions</code> is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><strong>RDS database defined externally and referenced through environment variables</strong> - To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.</p>\n\n<p>Using Elastic Beanstalk with Amazon RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ElastiCache bundled with the application source code</strong> - ElastiCache is an AWS service and cannot be bundled with the source code.</p>\n\n<p><strong>RDS database defined in <code>.ebextensions/</code></strong> - The lifetime of the RDS instance gets tied to the lifetime of the Elastic Beanstalk environment, so this option is incorrect.</p>\n\n<p><strong>ElastiCache database defined externally and referenced through environment variables</strong> - For the given use-case, the client is fine with losing user session data and hence defining it in .ebextensions/ is more appropriate.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-environment-resources-elasticache.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-environment-resources-elasticache.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be re-created for every deployment.</p>\n\n<p>Which of the following configurations will allow you to achieve this? (Select two)</p>\n"
        },
        "correct_response": [
            "b",
            "e"
        ],
        "section": "Deployment",
        "question_plain": "You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be re-created for every deployment.\n\nWhich of the following configurations will allow you to achieve this? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752170,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use AWS CloudTrail to get a record of actions taken by a user</p>",
                "<p>Use SSM Parameter Store List feature to get a record of actions taken by a user</p>",
                "<p>Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user</p>",
                "<p>Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CloudTrail to get a record of actions taken by a user</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.</p>\n\n<p>AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Systems Manager. Using the information collected by AWS CloudTrail, you can determine the request that was made to Systems Manager, the IP address from which the request was made, who made the request, when it was made, and additional details.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SSM Parameter Store List feature to get a record of actions taken by a user</strong> - This option has been added as a distractor.</p>\n\n<p><strong>Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user</strong> - CloudWatch Logs can be integrated but that will not help determine who issued API calls.</p>\n\n<p><strong>Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user</strong> - S3 Access Logs can be integrated but that will not help determine who issued API calls.</p>\n",
            "relatedLectureIds": "",
            "question": "<p>Your company has stored all application secrets in SSM Parameter Store. The audit team has requested to get a report to better understand when and who has issued API calls against SSM Parameter Store.</p>\n\n<p>Which of the following options can be used to produce your report?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your company has stored all application secrets in SSM Parameter Store. The audit team has requested to get a report to better understand when and who has issued API calls against SSM Parameter Store.\n\nWhich of the following options can be used to produce your report?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752122,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>SSM Parameter Store</p>",
                "<p>Secrets Manager</p>",
                "<p>Systems Manager</p>",
                "<p>KMS</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Systems Manager cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You are running workloads on AWS and have embedded RDS database connection strings within each web server hosting your applications. After failing a security audit, you are looking at a different approach to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>Which AWS service can you use to address this use-case?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "You are running workloads on AWS and have embedded RDS database connection strings within each web server hosting your applications. After failing a security audit, you are looking at a different approach to store your secrets securely and automatically rotate the database credentials.\n\nWhich AWS service can you use to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752226,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Multi-Factor Authentication (MFA)</p>",
                "<p>Access keys</p>",
                "<p>Key pairs</p>",
                "<p>Root user credentials</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Key pairs</strong> - Key pairs consist of a public key and a private key. You use the private key to create a digital signature, and then AWS uses the corresponding public key to validate the signature. Key pairs are used only for Amazon EC2 and Amazon CloudFront. AWS does not provide key pairs for your account; you must create them. You can create Amazon EC2 key pairs from the Amazon EC2 console, CLI, or API. Key pairs make a robust combination for accessing an instance securely, a better option than using passwords.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Multi-Factor Authentication (MFA)</strong> - Multi-factor authentication (MFA) provides an extra level of security that you can apply to your AWS account. With MFA enabled, when you sign in to the AWS website, you are prompted for your user name and password, and an authentication code from an MFA device. Together, they provide increased security for your AWS account settings and resources. Its an added layer of protection for AWS account users.</p>\n\n<p><strong>Access keys</strong> - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. These credentials are for accessing AWS services programmatically and not for accessing the EC2 instance directly.</p>\n\n<p><strong>Root user credentials</strong> - Root user credentials are the Email ID and password used to create the AWS account. This user has full privileges on the account created and has access to all services under his account. The root user can create access keys or key pairs from his account. But, the root account credentials cannot directly be used to access EC2 instances or create digital signatures.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A developer at a company is trying to create a digital signature for SSH'ing into the Amazon EC2 instances.</p>\n\n<p>Which of the following entities can be used to facilitate this use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A developer at a company is trying to create a digital signature for SSH'ing into the Amazon EC2 instances.\n\nWhich of the following entities can be used to facilitate this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752200,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region</p>",
                "<p>Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified</p>",
                "<p>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group</p>",
                "<p>For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets</p>",
                "<p>An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified</strong> - This is not valid for Auto Scaling groups. Auto Scaling groups cannot span across multiple Regions.</p>\n\n<p><strong>An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region</strong> - This is not valid for Auto Scaling groups. An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region.</p>\n\n<p>Amazon EC2 Auto Scaling Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region</strong> - This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.</p>\n\n<p><strong>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group</strong> -  When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.</p>\n\n<p><strong>For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets</strong> - For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions.</p>\n\n<p>Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)</p>\n"
        },
        "correct_response": [
            "b",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions.\n\nWhich of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752206,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</p>",
                "<p>The route for the health check is misconfigured</p>",
                "<p>The EBS volumes have been improperly mounted</p>",
                "<p>Your web-app has a runtime that is not supported by the Application Load Balancer</p>",
                "<p>You need to attach Elastic IP to the EC2 instances</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options</p>\n\n<p><strong>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</strong></p>\n\n<p><strong>The route for the health check is misconfigured</strong></p>\n\n<p>You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.</p>\n\n<p>Application Load Balancer Configuration for Security Groups and Health Check Routes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EBS volumes have been improperly mounted</strong> - You can access the website using the IP address which means there is no issue with the EBS volumes. So this option is not correct.</p>\n\n<p><strong>Your web-app has a runtime that is not supported by the Application Load Balancer</strong> - There is no connection between a web app and the application load balancer. This option has been added as a distractor.</p>\n\n<p><strong>You need to attach Elastic IP to the EC2 instances</strong> - This option is a distractor as Elastic IPs do not need to be assigned to EC2 instances while using an Application Load Balancer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You have created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when you enter the IP address of the EC2 instances in your web browser, you can access your website.</p>\n\n<p>What could be the reason your instances are being marked as unhealthy? (Select two)</p>\n"
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You have created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when you enter the IP address of the EC2 instances in your web browser, you can access your website.\n\nWhat could be the reason your instances are being marked as unhealthy? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752186,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudTrail logs</p>",
                "<p>CloudWatch metrics</p>",
                "<p>ALB request tracing</p>",
                "<p>ALB access logs</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ALB access logs</strong> - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.</p>\n\n<p>Access logs for your Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudTrail logs</strong> - Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which API calls were made, the source IP address where the API call came from, who made the call, when the call was made, and so on.</p>\n\n<p><strong>CloudWatch metrics</strong> - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time-series data, known as metrics. You can use metrics to verify that your system is performing as expected. This is the right feature if you wish to track a certain metric.</p>\n\n<p><strong>ALB request tracing</strong> - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An organization has offices across multiple locations and the technology team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.</p>\n\n<p>Which feature of the Load Balancer will help collect the required information?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An organization has offices across multiple locations and the technology team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.\n\nWhich feature of the Load Balancer will help collect the required information?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752222,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>10</p>",
                "<p>25</p>",
                "<p>20</p>",
                "<p>15</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.</p>\n\n<p>The nodes for a load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. With Application Load Balancers, cross-zone load balancing is always enabled.</p>\n\n<p><strong>10</strong> - When cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets (present in both AZs).</p>\n\n<p>Cross-Zone Load Balancing Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>25</strong> - If cross-zone load balancing is disabled, each of the two targets in AZ1 will receive 25% of the traffic. Because the load balancer is only able to send to the targets registered in AZ1 (AZ2 instances are not accessible for load balancer on AZ1)</p>\n\n<p><strong>20</strong> - Invalid option, given only as a distractor.</p>\n\n<p><strong>15</strong> - Invalid option, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has cross-zone load balancing enabled in its configuration.</p>\n\n<p>What percentage traffic will each of the instances in AZ1 receive?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has cross-zone load balancing enabled in its configuration.\n\nWhat percentage traffic will each of the instances in AZ1 receive?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752234,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>S3</p>",
                "<p>ECR</p>",
                "<p>ECS</p>",
                "<p>EBS</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ECR</strong></p>\n\n<p>Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.</p>\n\n<p>How ECR Works:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/Product-Page-Diagram_Amazon-ECR.bf2e7a03447ed3aba97a70e5f4aead46a5e04547.png\">\nvia - <a href=\"https://aws.amazon.com/ecr/\">https://aws.amazon.com/ecr/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3</strong> - ECR uses Amazon S3 for storage to make your Docker images highly available and accessible. But S3 itself cannot be used to deploy Docker images.</p>\n\n<p><strong>ECS</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p>You cannot use ECS to store and deploy Docker images.</p>\n\n<p>ECS Overview:\n<img src=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-fargate.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><strong>EBS</strong> - Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. EBS cannot be used to store and deploy Docker images.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ecr/\">https://aws.amazon.com/ecr/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The development team at an IT company would like to provision their own Docker images that can be used as input sources for CodeBuild. These images will contain cached dependencies as well as special tooling for builds that are proprietary to the company.</p>\n\n<p>Which of the following services can be used to store and deploy these Docker images?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "The development team at an IT company would like to provision their own Docker images that can be used as input sources for CodeBuild. These images will contain cached dependencies as well as special tooling for builds that are proprietary to the company.\n\nWhich of the following services can be used to store and deploy these Docker images?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752240,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>The development team at an IT company wants to make changes to a current application written in Node.js and deployed on a Linux server. The team lead would like to decouple the application into microservices, package the application to a Docker container which is then run on the AWS infrastructure.</p>\n\n<p>Which AWS service is best suited for this change?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ECS</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p>Amazon ECS can be used to create a consistent deployment and build experience, manage, and scale batch and Extract-Transform-Load (ETL) workloads, and build sophisticated application architectures on a microservices model.</p>\n\n<p>ECS Overview:\n<img src=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-fargate.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ECR</strong> - Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.</p>\n\n<p>How ECR Works:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/Product-Page-Diagram_Amazon-ECR.bf2e7a03447ed3aba97a70e5f4aead46a5e04547.png\">\nvia - <a href=\"https://aws.amazon.com/ecr/\">https://aws.amazon.com/ecr/</a></p>\n\n<p><strong>Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Although Lambda can be used to run Docker containers but ECS is best suited for the given use case.</p>\n\n<p><strong>Step Functions</strong> - AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly. Step Functions cannot be used to run Docker containers.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/ecr/\">https://aws.amazon.com/ecr/</a></p>\n",
            "answers": [
                "<p>ECS</p>",
                "<p>Step Functions</p>",
                "<p>Lambda</p>",
                "<p>ECR</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "ECS",
        "question_plain": "The development team at an IT company wants to make changes to a current application written in Node.js and deployed on a Linux server. The team lead would like to decouple the application into microservices, package the application to a Docker container which is then run on the AWS infrastructure.\n\nWhich AWS service is best suited for this change?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752184,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudFormation</p>",
                "<p>CodeDeploy</p>",
                "<p>Elastic Beanstalk</p>",
                "<p>Serverless Application Model</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Elastic Beanstalk</strong></p>\n\n<p>AWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.</p>\n\n<p>AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.</p>\n\n<p>Benefits of Elastic Beanstalk:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudFormation</strong> - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><strong>CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.</p>\n\n<p><strong>Serverless Application Model</strong> - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.</p>\n\n<p>As a Developer Associate, which AWS service would you recommend for the given use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.\n\nAs a Developer Associate, which AWS service would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752138,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Create an A record</p>",
                "<p>Create a CNAME record</p>",
                "<p>Create a PTR record</p>",
                "<p>Create an Alias Record</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CNAME record</strong></p>\n\n<p>A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).</p>\n\n<p>CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.</p>\n\n<p>Please review the major differences between CNAME and Alias Records:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an A record</strong> - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create a PTR record</strong> - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create an Alias Record</strong> - Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53.</p>\n\n<p>What Route 53 record should you create?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53.\n\nWhat Route 53 record should you create?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752228,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>35 seconds</p>",
                "<p>0 seconds</p>",
                "<p>60 seconds</p>",
                "<p>30 seconds</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>Burstable performance instances, which are T3, T3a, and T2 instances, are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are the only instance types that use credits for CPU usage.</p>\n\n<p><strong>0 seconds</strong> - AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>35 seconds</strong></p>\n\n<p><strong>60 seconds</strong></p>\n\n<p><strong>30 seconds</strong></p>\n\n<p>These three options contradict the explanation provided earlier, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance - T2.micro - for 35 seconds and stopped the instance.</p>\n\n<p>At the end of the month, what is the instance usage duration that the company is charged for?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance - T2.micro - for 35 seconds and stopped the instance.\n\nAt the end of the month, what is the instance usage duration that the company is charged for?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752154,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Deploy an interceptor shell script</p>",
                "<p>Use API Gateway Mapping Templates</p>",
                "<p>Use an API Gateway stage variable</p>",
                "<p>Use a Lambda custom interceptor</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use API Gateway Mapping Templates</strong> - In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, vice versa is also possible. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.</p>\n\n<p>Suppose we have an API for managing fruit and vegetable inventory in the produce department of a supermarket. When a manager queries the backend for the current inventory, the server sends back the following response payload:</p>\n\n<pre><code>{\n  \"department\": \"produce\",\n  \"categories\": [\n    \"fruit\",\n    \"vegetables\"\n  ],\n  \"bins\": [\n    {\n      \"category\": \"fruit\",\n      \"type\": \"apples\",\n      \"price\": 1.99,\n      \"unit\": \"pound\",\n      \"quantity\": 232\n    },\n    {\n      \"category\": \"fruit\",\n      \"type\": \"bananas\",\n      \"price\": 0.19,\n      \"unit\": \"each\",\n      \"quantity\": 112\n    },\n    {\n      \"category\": \"vegetables\",\n      \"type\": \"carrots\",\n      \"price\": 1.29,\n      \"unit\": \"bag\",\n      \"quantity\": 57\n    }\n  ]\n}\n</code></pre>\n\n<p>When the backend returns the query results shown above, the manager of the produce department might be interested in reading them, as follows:</p>\n\n<pre><code>{\n  \"choices\": [\n    {\n      \"kind\": \"apples\",\n      \"suggestedPrice\": \"1.99 per pound\",\n      \"available\": 232\n    },\n    {\n      \"kind\": \"bananas\",\n      \"suggestedPrice\": \"0.19 per each\",\n      \"available\": 112\n    },\n    {\n      \"kind\": \"carrots\",\n      \"suggestedPrice\": \"1.29 per bag\",\n      \"available\": 57\n    }\n  ]\n}\n</code></pre>\n\n<p>To enable this, we need to provide API Gateway with a mapping template to translate the data from the backend format like so:</p>\n\n<pre><code>#set($inputRoot = $input.path('$'))\n{\n  \"choices\": [\n#foreach($elem in $inputRoot.bins)\n    {\n      \"kind\": \"$elem.type\",\n      \"suggestedPrice\": \"$elem.price per $elem.unit\",\n      \"available\": $elem.quantity\n    }#if($foreach.hasNext),#end\n\n#end\n  ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy an interceptor shell script</strong> - This option has been added as a distractor.</p>\n\n<p><strong>Use an API Gateway stage variable</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. This feature is not useful for the current use case.</p>\n\n<p><strong>Use a Lambda custom interceptor</strong> - This is a made-up option. Lambda cannot intercept the response for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You are a developer working on AWS Lambda functions that are invoked via REST API's using Amazon API Gateway. Currently, when a GET request is invoked by the consumer, the entire data-set returned by the Lambda function is visible. Your team lead asked you to format the data response.</p>\n\n<p>Which feature of the API Gateway can be used to solve this issue?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "You are a developer working on AWS Lambda functions that are invoked via REST API's using Amazon API Gateway. Currently, when a GET request is invoked by the consumer, the entire data-set returned by the Lambda function is visible. Your team lead asked you to format the data response.\n\nWhich feature of the API Gateway can be used to solve this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752232,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</code></p>",
                "<p><code>!FindInMap [ MapName, TopLevelKey ]</code></p>",
                "<p><code>!FindInMap [ MapName ]</code></p>",
                "<p><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]</code></p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. YAML Syntax for the full function name:  Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]</p>\n\n<p>Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</p>\n\n<p>Where,</p>\n\n<p>MapName - Is the logical name of a mapping declared in the Mappings section that contains the keys and values.\nTopLevelKey - The top-level key name. Its value is a list of key-value pairs.\nSecondLevelKey - The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.</p>\n\n<p>Consider the following YAML template:</p>\n\n<pre><code>Mappings:\n  RegionMap:\n    us-east-1:\n      HVM64: \"ami-0ff8a91507f77f867\"\n      HVMG2: \"ami-0a584ac55a7631c0c\"\n    us-west-1:\n      HVM64: \"ami-0bdb828fd58c52235\"\n      HVMG2: \"ami-066ee5fd4a9ef77f1\"\n    eu-west-1:\n      HVM64: \"ami-047bb4163c506cd98\"\n      HVMG2: \"ami-31c2f645\"\n    ap-southeast-1:\n      HVM64: \"ami-08569b978cc4dfa10\"\n      HVMG2: \"ami-0be9df32ae9f92309\"\n    ap-northeast-1:\n      HVM64: \"ami-06cd52961ce9f0d85\"\n      HVMG2: \"ami-053cdd503598e4a9d\"\nResources:\n  myEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      ImageId: !FindInMap\n        - RegionMap\n        - !Ref 'AWS::Region'\n        - HVM64\n      InstanceType: m1.small\n</code></pre>\n\n<p>The example template contains an AWS::EC2::Instance resource whose ImageId property is set by the FindInMap function.</p>\n\n<p>MapName is set to the map of interest, \"RegionMap\" in this example. TopLevelKey is set to the region where the stack is created, which is determined by using the \"AWS::Region\" pseudo parameter. SecondLevelKey is set to the desired architecture, \"HVM64\" for this example.</p>\n\n<p>FindInMap returns the AMI assigned to FindInMap. For a HVM64 instance in us-east-1, FindInMap would return \"ami-0ff8a91507f77f867\".</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey ]</code></strong></p>\n\n<p><strong><code>!FindInMap [ MapName ]</code></strong></p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]</code></strong></p>\n\n<p>These three options contradict the explanation provided above, hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI.</p>\n\n<p>How will you invoke the <code>!FindInMap</code> function to fulfill this use case?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI.\n\nHow will you invoke the !FindInMap function to fulfill this use case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752118,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Latency</p>",
                "<p>Geolocation</p>",
                "<p>Failover</p>",
                "<p>Weighted</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Geolocation</strong></p>\n\n<p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights</p>\n\n<p>You can create a default record that handles both queries from IP addresses that aren't mapped to any location and queries that come from locations that you haven't created geolocation records for. If you don't create a default record, Route 53 returns a \"no answer\" response for queries from those locations.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Failover</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.</p>\n\n<p><strong>Latency</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.</p>\n\n<p><strong>Weighted</strong> - Use this policy to route traffic to multiple resources in proportions that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A media company has created a video streaming application and it would like their Brazilian users to be served by the company's Brazilian servers. Other users around the globe should not be able to access the servers through DNS queries.</p>\n\n<p>Which Route 53 routing policy meets this requirement?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A media company has created a video streaming application and it would like their Brazilian users to be served by the company's Brazilian servers. Other users around the globe should not be able to access the servers through DNS queries.\n\nWhich Route 53 routing policy meets this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752198,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudTrail</p>",
                "<p>VPC Flow Logs</p>",
                "<p>CloudWatch Events</p>",
                "<p>X-Ray</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>X-Ray</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Flow Logs</strong>: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to debug and trace data across accounts.</p>\n\n<p><strong>CloudWatch Events</strong>: Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. These help to trigger notifications based on changes happening in AWS services. You cannot use CloudWatch Events to debug and trace data across accounts.</p>\n\n<p><strong>CloudTrail</strong>: With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to debug and trace data across accounts.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A multi-national company has multiple business units with each unit having its own AWS account. The development team at the company would like to debug and trace data across accounts and visualize it in a centralized account.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for the given use-case?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A multi-national company has multiple business units with each unit having its own AWS account. The development team at the company would like to debug and trace data across accounts and visualize it in a centralized account.\n\nAs a Developer Associate, which of the following solutions would you suggest for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752216,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>IAM policy principal</p>",
                "<p>IAM policy variables</p>",
                "<p>IAM policy condition</p>",
                "<p>IAM policy resource</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>IAM policy variables</strong></p>\n\n<p>Instead of creating individual policies for each user, you can use policy variables and create a single policy that applies to multiple users (a group policy). Policy variables act as placeholders. When you make a request to AWS, the placeholder is replaced by a value from the request when the policy is evaluated.</p>\n\n<p>As an example, the following policy gives each of the users in the group full programmatic access to a user-specific object (their own \"home directory\") in Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM policy principal</strong> - You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource (In IAM, a principal is a person or application that can make a request for an action or operation on an AWS resource. The principal is authenticated as the AWS account root user or an IAM entity to make requests to AWS). You cannot use the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies.</p>\n\n<p><strong>IAM policy condition</strong> - The Condition element (or Condition block) lets you specify conditions for when a policy is in effect, like so - <code>\"Condition\" : { \"StringEquals\" : { \"aws:username\" : \"johndoe\" }}</code>. This can not be used to address the requirements of the given use-case.</p>\n\n<p><strong>IAM policy resource</strong> - The Resource element specifies the object or objects that the statement covers. You specify a resource using an ARN. This can not be used to address the requirements of the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/\">https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The manager at an IT company wants to set up member access to user-specific folders in an Amazon S3 bucket - <code>bucket-a</code>. So, user x can only access files in his folder - <code>bucket-a/user/user-x/</code> and user y can only access files in her folder - <code>bucket-a/user/user-y/</code> and so on.</p>\n\n<p>As a Developer Associate, which of the following IAM constructs would you recommend so that the policy snippet can be made generic for all team members and the manager does not need to create separate IAM policy for each team member?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "The manager at an IT company wants to set up member access to user-specific folders in an Amazon S3 bucket - bucket-a. So, user x can only access files in his folder - bucket-a/user/user-x/ and user y can only access files in her folder - bucket-a/user/user-y/ and so on.\n\nAs a Developer Associate, which of the following IAM constructs would you recommend so that the policy snippet can be made generic for all team members and the manager does not need to create separate IAM policy for each team member?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752134,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>IAM permissions with sigv4</p>",
                "<p>Lambda Authorizer</p>",
                "<p>Cognito User Pools</p>",
                "<p>API Gateway User Pools</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Lambda Authorizer\"</p>\n\n<p>An Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.</p>\n\n<p>\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.</p>\n\n<p>\"API Gateway User Pools\" - This is a made-up option, added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend for the given use-case?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\n\nAs a Developer Associate, which of the following options would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752162,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Budget forecast has been created from an account that does not have enough privileges</p>",
                "<p>Amazon CloudWatch could be down and hence alerts are not being sent</p>",
                "<p>Account has to be part of AWS Organizations to receive AWS Budgets alerts</p>",
                "<p>AWS requires approximately 5 weeks of usage data to generate budget forecasts</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>AWS Budgets lets customers set custom budgets and receive alerts if their costs or usage exceed (or are forecasted to exceed) their budgeted amount.</p>\n\n<p><strong>AWS requires approximately 5 weeks of usage data to generate budget forecasts</strong> - AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Budget forecast has been created from an account that does not have enough privileges</strong> - This is an incorrect statement. If the user account does not have enough privileges, the user will not be able to create the budget at all.</p>\n\n<p><strong>Amazon CloudWatch could be down and hence alerts are not being sent</strong> - Amazon CloudWatch is fully managed by AWS, this option has been added as a distractor.</p>\n\n<p><strong>Account has to be part of AWS Organizations to receive AWS Budget alerts</strong> - This is an incorrect statement. Stand-alone accounts too can create budgets and being part of an Organization is not mandatory to use AWS Budgets.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-best-practices.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-best-practices.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A multi-national company has just moved to AWS Cloud and it has configured forecast-based AWS Budgets alerts for cost management. However, no alerts have been received even though the account and the budgets have been created almost three weeks ago.</p>\n\n<p>What could be the issue with the AWS Budgets configuration?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A multi-national company has just moved to AWS Cloud and it has configured forecast-based AWS Budgets alerts for cost management. However, no alerts have been received even though the account and the budgets have been created almost three weeks ago.\n\nWhat could be the issue with the AWS Budgets configuration?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752220,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>EC2 X-Ray Daemon</p>",
                "<p>X-Ray sampling</p>",
                "<p>EC2 Instance Role</p>",
                "<p>CloudTrail</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>X-Ray sampling</strong></p>\n\n<p>By customizing sampling rules, you can control the amount of data that you record, and modify sampling behavior on the fly without modifying or redeploying your code. Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria. X-Ray SDK applies a sampling algorithm to determine which requests get traced however because our application is failing to send data to X-Ray it does not help in determining the cause of failure.</p>\n\n<p>X-Ray Overview:\n<img src=\"https://docs.aws.amazon.com/xray/latest/devguide/images/architecture-dataflow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 X-Ray Daemon</strong> - The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon logs could help with figuring out the problem.</p>\n\n<p><strong>EC2 Instance Role</strong> - The X-Ray daemon uses the AWS SDK to upload trace data to X-Ray, and it needs AWS credentials with permission to do that. On Amazon EC2, the daemon uses the instance's instance profile role automatically. Eliminates API permission issues (in case the role doesn't have IAM permissions to write data to the X-Ray service)</p>\n\n<p><strong>CloudTrail</strong> - With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You can check CloudTrail to see if any API call is being denied on X-Ray.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.</p>\n\n<p>Which of the following does <strong>NOT</strong> help with debugging the issue?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\n\nWhich of the following does NOT help with debugging the issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752144,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>Separate public traffic from private traffic</p>",
                "<p>Build a highly available system</p>",
                "<p>The Load Balancer communicates with the underlying EC2 instances using their public IPs</p>",
                "<p>Improve vertical scalability of the system</p>",
                "<p>Deploy EC2 instances across multiple AWS Regions</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p>A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones. The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets. When the load balancer detects an unhealthy target, it stops routing traffic to that target. It then resumes routing traffic to that target when it detects that the target is healthy again.</p>\n\n<p>Elastic Load Balancing supports three types of load balancers:</p>\n\n<p>Application Load Balancers</p>\n\n<p>Network Load Balancers</p>\n\n<p>Classic Load Balancers</p>\n\n<p><strong>Separate public traffic from private traffic</strong> - The nodes of an internet-facing load balancer have public IP addresses. Load balancers route requests to your targets using private IP addresses. Therefore, your targets do not need public IP addresses to receive requests from users over the internet.</p>\n\n<p><strong>Build a highly available system</strong> - Elastic Load Balancing provides fault tolerance for your applications by automatically balancing traffic across targets – Amazon EC2 instances, containers, IP addresses, and Lambda functions – in multiple Availability Zones while ensuring only healthy targets receive traffic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Load Balancer communicates with the underlying EC2 instances using their public IPs</strong> - This is an incorrect statement. The Load Balancer communicates with the underlying EC2 instances using their private IPs.</p>\n\n<p><strong>Improve vertical scalability of the system</strong> - This is an incorrect statement. Elastic Load Balancers can connect with Auto Scaling groups to provide horizontal scaling.</p>\n\n<p><strong>Deploy EC2 instances across multiple AWS Regions</strong> - A Load Balancer can target EC2 instances only within an AWS Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An E-commerce business, has its applications built on a fleet of Amazon EC2 instances, spread across various Regions and AZs. The technical team has suggested using Elastic Load Balancers for better architectural design.</p>\n\n<p>What characteristics of an Elastic Load Balancer make it a winning choice? (Select two)</p>\n"
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An E-commerce business, has its applications built on a fleet of Amazon EC2 instances, spread across various Regions and AZs. The technical team has suggested using Elastic Load Balancers for better architectural design.\n\nWhat characteristics of an Elastic Load Balancer make it a winning choice? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752136,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Enable CodeBuild timeouts</p>",
                "<p>Use AWS Lambda</p>",
                "<p>Use AWS CloudWatch Events</p>",
                "<p>Use VPC Flow Logs</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CodeBuild timeouts</strong></p>\n\n<p>A build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).</p>\n\n<p>The following rules apply when you run multiple builds:</p>\n\n<p>When possible, builds run concurrently. The maximum number of concurrently running builds can vary.</p>\n\n<p>Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.</p>\n\n<p>A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.</p>\n\n<p>By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.</p>\n\n<p><strong>Use AWS CloudWatch Events</strong> - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.</p>\n\n<p><strong>Use VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.</p>\n\n<p>Which of the following options represents the best solution to address this use-case?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.\n\nWhich of the following options represents the best solution to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752176,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use backlog per instance metric with target tracking scaling policy</p>",
                "<p>Use Docker swarm</p>",
                "<p>Use ECS service scheduler</p>",
                "<p>Use ECS step scaling policy</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use backlog per instance metric with target tracking scaling policy</strong> - If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively.</p>\n\n<p>The issue with using a CloudWatch Amazon SQS metric like <code>ApproximateNumberOfMessagesVisible</code> for target tracking is that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. That's because the number of messages in your SQS queue does not solely define the number of instances needed. The number of instances in your Auto Scaling group can be driven by multiple factors, including how long it takes to process a message and the acceptable amount of latency (queue delay).</p>\n\n<p>The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain. You can calculate these numbers as follows:</p>\n\n<p>Backlog per instance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet's running capacity, which for an Auto Scaling group is the number of instances in the InService state, to get the backlog per instance.</p>\n\n<p>Acceptable backlog per instance: To calculate your target value, first determine what your application can accept in terms of latency. Then, take the acceptable latency value and divide it by the average time that an EC2 instance takes to process a message.</p>\n\n<p>To illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Docker swarm</strong> - A Docker swarm is a container orchestration tool, meaning that it allows the user to manage multiple containers deployed across multiple host machines. A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services).</p>\n\n<p><strong>Use ECS service scheduler</strong> - Amazon ECS provides a service scheduler (for long-running tasks and applications), the ability to run tasks manually (for batch jobs or single run tasks), with Amazon ECS placing tasks on your cluster for you. You can specify task placement strategies and constraints that allow you to run tasks in the configuration you choose, such as spread out across Availability Zones. It is also possible to integrate with custom or third-party schedulers.</p>\n\n<p><strong>Use ECS step scaling policy</strong> - Although Amazon ECS Service Auto Scaling supports using Application Auto Scaling step scaling policies, AWS recommends using target tracking scaling policies instead. For example, if you want to scale your service when CPU utilization falls below or rises above a certain level, create a target tracking scaling policy based on the CPU utilization metric provided by Amazon ECS.</p>\n\n<p>With step scaling policies, you create and manage the CloudWatch alarms that trigger the scaling process. If the target tracking alarms don't work for your use case, you can use step scaling. You can also use target tracking scaling with step scaling for an advanced scaling policy configuration. For example, you can configure a more aggressive response when utilization reaches a certain level.</p>\n\n<p>Step Scaling scales your cluster on various lengths of steps based on different ranges of thresholds. Target tracking on the other hand intelligently picks the smart lengths needed for the given configuration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An e-commerce company has developed an API that is hosted on Amazon ECS. Variable traffic spikes on the application are causing order processing to take too long. The application processes orders using Amazon SQS queues. The <code>ApproximateNumberOfMessagesVisible</code> metric spikes at very high values throughout the day which triggers the CloudWatch alarm. Other ECS metrics for the API containers are well within limits.</p>\n\n<p>As a Developer Associate, which of the following will you recommend for improving performance while keeping costs low? </p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "An e-commerce company has developed an API that is hosted on Amazon ECS. Variable traffic spikes on the application are causing order processing to take too long. The application processes orders using Amazon SQS queues. The ApproximateNumberOfMessagesVisible metric spikes at very high values throughout the day which triggers the CloudWatch alarm. Other ECS metrics for the API containers are well within limits.\n\nAs a Developer Associate, which of the following will you recommend for improving performance while keeping costs low?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752168,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Deploy the function into multiple AWS Regions</p>",
                "<p>Deploy the function with its CPU allocation set to the maximum amount</p>",
                "<p>Deploy the function using Lambda layers</p>",
                "<p>Deploy the function with its memory allocation set to the maximum amount</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the function with its memory allocation set to the maximum amount</strong> - Lambda allocates CPU power in proportion to the amount of memory configured. Memory is the amount of memory available to your Lambda function at runtime. You can increase or decrease the memory and CPU power allocated to your function using the Memory (MB) setting. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second).</p>\n\n<p>Configuring function memory from AWS console:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the function into multiple AWS Regions</strong> - Deploying the Lambda function to multiple AWS Regions does not increase the compute capacity or CPU utilization capacity of Lambda. So, this option is irrelevant.</p>\n\n<p><strong>Deploy the function using Lambda layers</strong> - A Lambda layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. Layers do not increase the computational capacity of Lambda.</p>\n\n<p><strong>Deploy the function with its CPU allocation set to the maximum amount</strong> - This statement is given as a distractor. <code>CPU allocation</code> is an invalid parameter. As discussed above, the CPU is allocated in proportion to the memory allocated to the function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team wants to deploy an AWS Lambda function that requires significant CPU utilization.</p>\n\n<p>As a Developer Associate, which of the following would you suggest for reducing the average runtime of the function?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A development team wants to deploy an AWS Lambda function that requires significant CPU utilization.\n\nAs a Developer Associate, which of the following would you suggest for reducing the average runtime of the function?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752202,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>The users might have another policy that restricts them from accessing the Billing information</p>",
                "<p>You need to activate IAM user access to the Billing and Cost Management console for all the users who need access</p>",
                "<p>Only root user has access to AWS Billing and Cost Management console</p>",
                "<p>IAM user should be created under AWS Billing and Cost Management and not under AWS account to have access to Billing console</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>You need to activate IAM user access to the Billing and Cost Management console for all the users who need access</strong> - By default, IAM users do not have access to the AWS Billing and Cost Management console. You or your account administrator must grant users access. You can do this by activating IAM user access to the Billing and Cost Management console and attaching an IAM policy to your users. Then, you need to activate IAM user access for IAM policies to take effect. You only need to activate IAM user access once.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The users might have another policy that restricts them from accessing the Billing information</strong> - This is an incorrect option, as deduced from the given use-case.</p>\n\n<p><strong>Only root user has access to AWS Billing and Cost Management console</strong> - This is an incorrect statement. AWS Billing and Cost Management access can be provided to any user through user activation and policies, as discussed above.</p>\n\n<p><strong>IAM user should be created under AWS Billing and Cost Management and not under the AWS account to have access to Billing console</strong> - IAM is a feature of your AWS account. All IAM users are created and managed from a single place, irrespective of the services they wish to you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console.</p>\n\n<p>What could be the reason for this issue?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console.\n\nWhat could be the reason for this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752188,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>SMS text message-based MFA</p>",
                "<p>Hardware MFA device</p>",
                "<p>U2F security key</p>",
                "<p>Virtual MFA devices</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>SMS text message-based MFA</strong> - A type of MFA in which the IAM user settings include the phone number of the user's SMS-compatible mobile device. When the user signs in, AWS sends a six-digit numeric code by SMS text message to the user's mobile device. The user is required to type that code on a second webpage during sign-in. SMS-based MFA is available only for IAM users, you cannot use this type of MFA with the AWS account root user.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Hardware MFA device</strong> - This hardware device generates a six-digit numeric code. The user must type this code from the device on a second webpage during sign-in. Each MFA device assigned to a user must be unique. A user cannot type a code from another user's device to be authenticated. Can be used for root user authentication.</p>\n\n<p><strong>U2F security key</strong> - A device that you plug into a USB port on your computer. U2F is an open authentication standard hosted by the FIDO Alliance. When you enable a U2F security key, you sign in by entering your credentials and then tapping the device instead of manually entering a code.</p>\n\n<p><strong>Virtual MFA devices</strong> - A software app that runs on a phone or other device and emulates a physical device. The device generates a six-digit numeric code. The user must type a valid code from the device on a second webpage during sign-in. Each virtual MFA device assigned to a user must be unique. A user cannot type a code from another user's virtual MFA device to authenticate.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>In addition to regular sign-in credentials, AWS supports Multi-Factor Authentication (MFA) for accounts with privileged access.</p>\n\n<p>Which of the following MFA mechanisms is NOT for root user authentication?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "In addition to regular sign-in credentials, AWS supports Multi-Factor Authentication (MFA) for accounts with privileged access.\n\nWhich of the following MFA mechanisms is NOT for root user authentication?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752214,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CloudFront Key Pairs</p>",
                "<p>EC2 Instance Key Pairs</p>",
                "<p>IAM User Access Keys</p>",
                "<p>IAM User passwords</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>For Amazon CloudFront, you use key pairs to create signed URLs for private content, such as when you want to distribute restricted content that someone paid for.</p>\n\n<p><strong>CloudFront Key Pairs</strong> - IAM users can't create CloudFront key pairs. You must log in using root credentials to create key pairs.</p>\n\n<p>To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. AWS recommends that you use trusted key groups with signed URLs and signed cookies instead of using CloudFront key pairs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 Instance Key Pairs</strong> - You use key pairs to access Amazon EC2 instances, such as when you use SSH to log in to a Linux instance. These key pairs can be created from the IAM user login and do not need root user access.</p>\n\n<p><strong>IAM User Access Keys</strong> - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. IAM users can create their own Access Keys, does not need root access.</p>\n\n<p><strong>IAM User passwords</strong> - Every IAM user has access to his own credentials and can reset the password whenever they need to.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>Which of the following security credentials can only be created by the AWS Account root user? </p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "Which of the following security credentials can only be created by the AWS Account root user?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752238,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS::StackName</p>",
                "<p>AWS::NoValue</p>",
                "<p>AWS::Region</p>",
                "<p>AWS::AccountId</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS::AccountId</strong></p>\n\n<p>Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>Pseudo parameters are parameters that are predefined by AWS CloudFormation. You do not declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.</p>\n\n<p>AWS::AccountId returns the AWS account ID of the account in which the stack is being created.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS::NoValue</strong> - This removes the corresponding resource property when specified as a return value in the Fn::If intrinsic function.</p>\n\n<p><strong>AWS::Region</strong> - Returns a string representing the AWS Region in which the encompassing resource is being created, such as us-west-2.</p>\n\n<p><strong>AWS::StackName</strong> - Returns the name of the stack as specified with the aws cloudformation create-stack command, such as \"teststack\".</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>Your company has configured AWS Organizations to manage multiple AWS accounts. Within each AWS account, there are many CloudFormation scripts running. Your manager has requested that each script output the account number of the account the script was executed in.</p>\n\n<p>Which Pseudo parameter will you use to get this information?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Your company has configured AWS Organizations to manage multiple AWS accounts. Within each AWS account, there are many CloudFormation scripts running. Your manager has requested that each script output the account number of the account the script was executed in.\n\nWhich Pseudo parameter will you use to get this information?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752156,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Git credentials</p>",
                "<p>IAM username and password</p>",
                "<p>SSH Keys</p>",
                "<p>AWS Access Keys</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>IAM username and password</strong> - IAM username and password credentials cannot be used to access CodeCommit.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Git credentials</strong> - These are IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p>\n\n<p><strong>SSH Keys</strong> - Are locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p>\n\n<p><strong>AWS access keys</strong> - You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud.</p>\n\n<p>Which of the following credential types is NOT supported by IAM for CodeCommit?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud.\n\nWhich of the following credential types is NOT supported by IAM for CodeCommit?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752180,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Cognito Sync</p>",
                "<p>API Gateway</p>",
                "<p>Cognito Identity Pools</p>",
                "<p>Cognito User Pools</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Cognito User Pools\"</p>\n\n<p>After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.</p>\n\n<p>Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.</p>\n\n<p>The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"API Gateway\" - If you are processing tokens server-side and using other programming languages not supported in AWS it may be a good choice. Other than that, go with a service already providing the functionality.</p>\n\n<p>\"Cognito Identity Pools\" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p>\"Cognito Sync\" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).</p>\n\n<p>Which AWS service will help you with token handling and management?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).\n\nWhich AWS service will help you with token handling and management?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752142,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>AWS Secrets Manager</p>",
                "<p>AWS Certificate Manager</p>",
                "<p>AWS Systems Manager</p>",
                "<p>IAM</p>",
                "<p>AWS CloudFormation</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Certificate Manager</strong> - AWS Certificate Manager (ACM) is the preferred tool to provision, manage, and deploy server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically.</p>\n\n<p><strong>IAM</strong> - IAM is used as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. It cannot be used to discover and protect your sensitive data in AWS.</p>\n\n<p><strong>AWS Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks such as running commands, managing patches, and configuring servers across AWS Cloud as well as on-premises infrastructure.</p>\n\n<p><strong>AWS CloudFormation</strong> - AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all Regions and accounts. Think infrastructure as code; think CloudFormation. You cannot use CloudFormation for running commands or managing patches on servers.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>To enable HTTPS connections for his web application deployed on the AWS Cloud, a developer is in the process of creating server certificate.</p>\n\n<p>Which AWS entities can be used to deploy SSL/TLS server certificates? (Select two)</p>\n"
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Security",
        "question_plain": "To enable HTTPS connections for his web application deployed on the AWS Cloud, a developer is in the process of creating server certificate.\n\nWhich AWS entities can be used to deploy SSL/TLS server certificates? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752140,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS Trusted Advisor</p>",
                "<p>IAM Access Analyzer</p>",
                "<p>Access Advisor feature on IAM console</p>",
                "<p>Amazon Inspector</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Access Advisor feature on IAM console</strong>- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing efforts by focusing only on roles that are in use.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Trusted Advisor</strong> - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices on cost optimization, security, fault tolerance, service limits, and performance improvement.</p>\n\n<p><strong>IAM Access Analyzer</strong> - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.</p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor-view-data.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor-view-data.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team lead is responsible for managing access for her IAM principals. At the start of the cycle, she has granted excess privileges to users to keep them motivated for trying new things. She now wants to ensure that the team has only the minimum permissions required to finish their work.</p>\n\n<p>Which of the following will help her identify unused IAM roles and remove them without disrupting any service?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A development team lead is responsible for managing access for her IAM principals. At the start of the cycle, she has granted excess privileges to users to keep them motivated for trying new things. She now wants to ensure that the team has only the minimum permissions required to finish their work.\n\nWhich of the following will help her identify unused IAM roles and remove them without disrupting any service?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752164,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS::Serverless::UserPool</p>",
                "<p>AWS::Serverless::Function</p>",
                "<p>AWS::Serverless::Api</p>",
                "<p>AWS::Serverless::SimpleTable</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS::Serverless::UserPool</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p>SAM supports the following resource types:</p>\n\n<p>AWS::Serverless::Api</p>\n\n<p>AWS::Serverless::Application</p>\n\n<p>AWS::Serverless::Function</p>\n\n<p>AWS::Serverless::HttpApi</p>\n\n<p>AWS::Serverless::LayerVersion</p>\n\n<p>AWS::Serverless::SimpleTable</p>\n\n<p>AWS::Serverless::StateMachine</p>\n\n<p>UserPool applies to the Cognito service which is used for authentication for mobile app and web. There is no resource named UserPool in the Serverless Application Model.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS::Serverless::Function</strong> - This resource creates a Lambda function, IAM execution role, and event source mappings that trigger the function.</p>\n\n<p><strong>AWS::Serverless::Api</strong> - This creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints. It is useful for advanced use cases where you want full control and flexibility when you configure your APIs.</p>\n\n<p><strong>AWS::Serverless::SimpleTable</strong> - This creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You are a developer in a manufacturing company that has several servers on-site. The company decides to move new development to the cloud using serverless technology. You decide to use the AWS Serverless Application Model (AWS SAM) and work with an AWS SAM template file to represent your serverless architecture.</p>\n\n<p>Which of the following is NOT a valid serverless resource type?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "You are a developer in a manufacturing company that has several servers on-site. The company decides to move new development to the cloud using serverless technology. You decide to use the AWS Serverless Application Model (AWS SAM) and work with an AWS SAM template file to represent your serverless architecture.\n\nWhich of the following is NOT a valid serverless resource type?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752130,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Access Advisor feature on IAM console</p>",
                "<p>S3 Object Lock</p>",
                "<p>S3 Analytics</p>",
                "<p>IAM Access Analyzer</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>IAM Access Analyzer</strong> - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.</p>\n\n<p>You can set the scope for the analyzer to an organization or an AWS account. This is your zone of trust. The analyzer scans all of the supported resources within your zone of trust. When Access Analyzer finds a policy that allows access to a resource from outside of your zone of trust, it generates an active finding.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access Advisor feature on IAM console</strong> - To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. This does not provide information about non-IAM entities such as S3, hence it's not a correct choice here.</p>\n\n<p><strong>S3 Object Lock</strong> - S3 Object Lock enables you to store objects using a \"Write Once Read Many\" (WORM) model. S3 Object Lock can help prevent accidental or inappropriate deletion of data, it is not the right choice for the current scenario.</p>\n\n<p><strong>S3 Analytics</strong> - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. You cannot use S3 Analytics to identify unintended access to your S3 resources.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A Developer has been entrusted with the job of securing certain S3 buckets that are shared by a large team of users. Last time, a bucket policy was changed, the bucket was erroneously available for everyone, outside the organization too.</p>\n\n<p>Which feature/service will help the developer identify similar security issues with minimum effort?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A Developer has been entrusted with the job of securing certain S3 buckets that are shared by a large team of users. Last time, a bucket policy was changed, the bucket was erroneously available for everyone, outside the organization too.\n\nWhich feature/service will help the developer identify similar security issues with minimum effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752224,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>CodeBuild</p>",
                "<p>CodeDeploy</p>",
                "<p>Elastic Beanstalk</p>",
                "<p>CodePipeline</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeDeploy</strong></p>\n\n<p>AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p>\n\n<p>The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.</p>\n\n<p>CodeDeploy offers lot of control over deployment steps. Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/\">https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It cannot be used to deploy applications.</p>\n\n<p><strong>Elastic Beanstalk</strong> - AWS Elastic Beanstalk offers hooks but not as much control as CodeDeploy. Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p>\n\n<p><strong>CodePipeline</strong> - CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change. CodePipeline by itself cannot deploy applications.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the blue-green deployment.</p>\n\n<p>Which service will help you achieve that?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the blue-green deployment.\n\nWhich service will help you achieve that?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752190,
        "assessment_type": "multi-select",
        "prompt": {
            "answers": [
                "<p>AWS Organizations Service Control Policy (SCP)</p>",
                "<p>Permissions boundary</p>",
                "<p>Access control list (ACL)</p>",
                "<p>Resource-based policy</p>",
                "<p>Identity-based policy</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Organizations Service Control Policy (SCP)</strong> – Use an AWS Organizations Service Control Policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.</p>\n\n<p><strong>Permissions boundary</strong> - Permissions boundary is a managed policy that is used for an IAM entity (user or role). The policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions.</p>\n\n<p>Overview of Policy Types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access control list (ACL)</strong> - Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal.</p>\n\n<p><strong>Resource-based policy</strong> - Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies.</p>\n\n<p><strong>Identity-based policy</strong> - Help attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>A development team lead is configuring policies for his team at an IT company.</p>\n\n<p>Which of the following policy types only limit permissions but cannot grant permissions (Select two)?</p>\n"
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Security",
        "question_plain": "A development team lead is configuring policies for his team at an IT company.\n\nWhich of the following policy types only limit permissions but cannot grant permissions (Select two)?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752194,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>'Conditions' section of the template</p>",
                "<p>'Resources' section of the template</p>",
                "<p>'Parameters' section of the template</p>",
                "<p>'Dependencies' section of the template</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>Templates include several major sections. The Resources section is the only required section.</p>\n\n<p>Sample CloudFormation YAML template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n\n<p><strong>'Dependencies' section of the template</strong> - As you can see, there is no section called 'Dependencies' in the template. Although dependencies can be mentioned, there is no section itself for dependencies.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>'Conditions' section of the template</strong> - This optional section includes conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.</p>\n\n<p><strong>'Resources' section of the template</strong> - This is the only required section and specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.</p>\n\n<p><strong>'Parameters' section of the template</strong> - This optional section is helpful in passing Values to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The Technical Lead of your team has reviewed a CloudFormation YAML template written by a new recruit and specified that an invalid section has been added to the template.</p>\n\n<p>Which of the following represents an invalid section of the CloudFormation template?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "The Technical Lead of your team has reviewed a CloudFormation YAML template written by a new recruit and specified that an invalid section has been added to the template.\n\nWhich of the following represents an invalid section of the CloudFormation template?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752116,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Deploy using 'Immutable' deployment policy</p>",
                "<p>Deploy using 'Rolling with additional batch' deployment policy</p>",
                "<p>Deploy using 'All at once' deployment policy</p>",
                "<p>Deploy using 'Rolling' deployment policy</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>AWS Elastic Beanstalk offers several deployment policies and settings. Choosing the right deployment policy for your application is a tradeoff based on a few considerations and depends on your business needs.</p>\n\n<p><strong>Deploy using 'Rolling with additional batch' deployment policy</strong> - With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy using 'Immutable' deployment policy</strong> - A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.</p>\n\n<p><strong>Deploy using 'All at once' deployment policy</strong> - This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.</p>\n\n<p><strong>Deploy using 'Rolling' deployment policy</strong> - With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>As an AWS Certified Developer Associate, you been asked to create an AWS Elastic Beanstalk environment to handle deployment for an application that has high traffic and high availability needs. You need to deploy the new version using Beanstalk while making sure that performance and availability are not affected.</p>\n\n<p>Which of the following is the MOST optimal way to do this while keeping the solution cost-effective?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "As an AWS Certified Developer Associate, you been asked to create an AWS Elastic Beanstalk environment to handle deployment for an application that has high traffic and high availability needs. You need to deploy the new version using Beanstalk while making sure that performance and availability are not affected.\n\nWhich of the following is the MOST optimal way to do this while keeping the solution cost-effective?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752158,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>You should create a new IAM user just for that other region</p>",
                "<p>You need to override the default region by using aws configure</p>",
                "<p>Use the --region parameter</p>",
                "<p>Use boto3 dependency injection</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the --region parameter</strong>: If the region parameter is not set, then the CLI command is executed against the default AWS region.</p>\n\n<p>You can also review all general options for AWS CLI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options\">https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You need to override the default region by using aws configure</strong> - This is not the most optimal way as you will have to change it again to reset the default region.</p>\n\n<p><strong>You should create a new IAM user just for that other region</strong> - This is not the most optimal way as you would need to manage two IAM user profiles.</p>\n\n<p><strong>Use boto3 dependency injection</strong> - With the CLI you do not use boto3. This option is a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options\">https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is us-east-1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the us-east-2 region.</p>\n\n<p>What of the following is the MOST optimal solution to address this use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is us-east-1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the us-east-2 region.\n\nWhat of the following is the MOST optimal solution to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752210,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>It represents a Lambda function definition</p>",
                "<p>It represents an intrinsic function</p>",
                "<p>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</p>",
                "<p>Presence of <code>Transform</code> section indicates it is a CloudFormation Parameter</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template.</p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, presence of \"Transform\" section indicates, the document is a SAM template.</p>\n\n<p>Sample CloudFormation YAML template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to 'Transform' section.</p>\n\n<p><strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, like so:</p>\n\n<p>Parameters in YAML:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>As an AWS Certified Developer Associate, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains <code>Transform: 'AWS::Serverless-2016-10-31'</code>.</p>\n\n<p>What does the <code>Transform</code> section in the document represent?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As an AWS Certified Developer Associate, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains Transform: 'AWS::Serverless-2016-10-31'.\n\nWhat does the Transform section in the document represent?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752204,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p><code>.ebextensions_&lt;mysettings&gt;.config</code></p>",
                "<p><code>.ebextensions/&lt;mysettings&gt;.config</code></p>",
                "<p><code>.config/&lt;mysettings&gt;.ebextensions</code></p>",
                "<p><code>.config_&lt;mysettings&gt;.ebextensions</code></p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>.ebextensions/&lt;mysettings&gt;.config</code></strong> : You can add AWS Elastic Beanstalk configuration files (<code>.ebextensions</code>) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>.ebextensions_&lt;mysettings&gt;.config</code></strong></p>\n\n<p><strong><code>.config/&lt;mysettings&gt;.ebextensions</code></strong></p>\n\n<p><strong><code>.config_&lt;mysettings&gt;.ebextensions</code></strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.</p>\n\n<p>When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.\n\nWhen creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38752242,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>AWS Organizations Service Control Policies (SCP)</p>",
                "<p>Trust policy</p>",
                "<p>Access control list (ACL)</p>",
                "<p>Permissions boundary</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\nResource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.</p>\n\n<p><strong>Trust policy</strong> - Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Organizations Service Control Policies (SCP)</strong> - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.</p>\n\n<p><strong>Access control list (ACL)</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.</p>\n\n<p><strong>Permissions boundary</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>As part of his development work, an AWS Certified Developer Associate is creating policies and attaching them to IAM identities. After creating necessary Identity-based policies, he is now creating Resource-based policies.</p>\n\n<p>Which is the only resource-based policy that the IAM service supports?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As part of his development work, an AWS Certified Developer Associate is creating policies and attaching them to IAM identities. After creating necessary Identity-based policies, he is now creating Resource-based policies.\n\nWhich is the only resource-based policy that the IAM service supports?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487008,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</p>",
                "<p>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</p>",
                "<p>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</p>",
                "<p>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</p>"
            ],
            "question": "<p>The development team at a HealthCare company has deployed EC2 instances in AWS Account A. These instances need to access patient data with Personally Identifiable Information (PII) on multiple S3 buckets in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</strong></p>\n\n<p>You can give EC2 instances in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as S3 buckets. You need to create an IAM role in Account B and set Account A as a trusted entity. Then attach a policy to this IAM role such that it delegates access to Amazon S3 like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::awsexamplebucket1\",\n                \"arn:aws:s3:::awsexamplebucket1/*\",\n                \"arn:aws:s3:::awsexamplebucket2\",\n                \"arn:aws:s3:::awsexamplebucket2/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Then you can create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::AccountB_ID:role/ROLENAME\"\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</strong> - This option contradicts the explanation provided earlier in the explanation, hence this option is incorrect.</p>\n\n<p><strong>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</strong> - Copying the AMI is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</strong> - Just adding a bucket policy in Account B is not enough, as you also need to create an IAM policy in Account A to access S3 objects in Account B.</p>\n\n<p>Please review this reference material for a deep-dive on cross-account access to objects that are in Amazon S3 buckets -\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a HealthCare company has deployed EC2 instances in AWS Account A. These instances need to access patient data with Personally Identifiable Information (PII) on multiple S3 buckets in another AWS Account B.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487028,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Create a custom solution using EC2 and DynamoDB to facilitate sign up and user management for the mobile app</p>",
                "<p>Create a custom solution using Lambda and DynamoDB to facilitate sign up and user management for the mobile app</p>",
                "<p>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</p>",
                "<p>Use Cognito User pools to facilitate sign up and user management for the mobile app</p>"
            ],
            "question": "<p>The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User pools to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.</p>\n\n<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p><strong>Create a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p><strong>Create a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>As the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees.\n\nAs a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486978,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</p>",
                "<p>Add an Application Load Balancer in front of the Lambda functions</p>",
                "<p>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</p>",
                "<p>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</p>"
            ],
            "question": "<p>The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.</p>\n\n<p>You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.</p>\n\n<p>Please see this note for more details on provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</strong> - To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.</p>\n\n<p><strong>Add an Application Load Balancer in front of the Lambda functions</strong> - This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic.</p>\n\n<p><strong>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</strong> - It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike.\n\nAs a Developer Associate, which of the following solutions would you recommend to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487010,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                "<p>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                "<p>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</p>",
                "<p>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</p>"
            ],
            "question": "<p>The development team at an analytics company is using SQS queues for decoupling the various components of application architecture. As the consumers need additional time to process SQS messages, the development team wants to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to the development team?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</strong> - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</strong> - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at an analytics company is using SQS queues for decoupling the various components of application architecture. As the consumers need additional time to process SQS messages, the development team wants to postpone the delivery of new messages to the queue for a few seconds.\n\nAs a Developer Associate, which of the following solutions would you recommend to the development team?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486992,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Parameters</p>",
                "<p>Mappings</p>",
                "<p>Globals</p>",
                "<p>Transform</p>"
            ],
            "question": "<p>Other than the <code>Resources</code> section, which of the following sections in a Serverless Application Model (SAM) Template is mandatory?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Transform</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>Serverless Application Model (SAM) Templates include several major sections. Transform and Resources are the only required sections.</p>\n\n<p>Please review this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Parameters</strong></p>\n\n<p><strong>Mappings</strong></p>\n\n<p><strong>Globals</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Other than the Resources section, which of the following sections in a Serverless Application Model (SAM) Template is mandatory?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487060,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Configure the data producer to retry with an exponential backoff</p>",
                "<p>Increase the number of shards within your data streams to provide enough capacity</p>",
                "<p>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</p>",
                "<p>Use Amazon SQS instead of Kinesis Data Streams</p>",
                "<p>Use Kinesis enhanced fan-out for Kinesis Data Streams</p>"
            ],
            "question": "<p>A data analytics company is processing real-time Internet-of-Things (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception.</p>\n\n<p>Which of the following actions would help in addressing this issue? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the data producer to retry with an exponential backoff</strong></p>\n\n<p><strong>Increase the number of shards within your data streams to provide enough capacity</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.</p>\n\n<p>How Kinesis Data Streams Work\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.</p>\n\n<p>If this is due to a temporary rise of the data stream’s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests.</p>\n\n<p>If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</strong> - Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p><strong>Use Amazon SQS instead of Kinesis Data Streams</strong> - This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the use-case.</p>\n\n<p><strong>Use Kinesis enhanced fan-out for Kinesis Data Streams</strong> - You should use enhanced fan-out if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fan-out will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p>Please review this note for more details on enhanced fan-out for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A data analytics company is processing real-time Internet-of-Things (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception.\n\nWhich of the following actions would help in addressing this issue? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487012,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Deploy the new application version using 'All at once' deployment policy</p>",
                "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
                "<p>Deploy the new application version using 'Immutable' deployment policy</p>",
                "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>"
            ],
            "question": "<p>The development team at an e-commerce company completed the last deployment for their application at a reduced capacity because of the deployment policy. The application took a performance hit because of the traffic spike due to an on-going sale.</p>\n\n<p>Which of the following represents the BEST deployment option for the upcoming application version such that it maintains at least the FULL capacity of the application and MINIMAL impact of failed deployment?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new application version using 'Immutable' deployment policy</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. In case of deployment failure, the new instances are terminated, so the impact is minimal.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. Also in case of deployment failure, the application sees a downtime, so this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "The development team at an e-commerce company completed the last deployment for their application at a reduced capacity because of the deployment policy. The application took a performance hit because of the traffic spike due to an on-going sale.\n\nWhich of the following represents the BEST deployment option for the upcoming application version such that it maintains at least the FULL capacity of the application and MINIMAL impact of failed deployment?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486996,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</p>",
                "<p>Complete both operations on Amazon RedShift in a single transaction block</p>",
                "<p>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
                "<p>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
                "<p>Complete both operations on RDS MySQL in a single transaction block</p>"
            ],
            "question": "<p>A social gaming application supports the transfer of gift vouchers between users. When a user hits a certain milestone on the leaderboard, they earn a gift voucher that can be redeemed or transferred to another user. The development team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.</p>\n\n<p>Which of the following solutions represent the best-fit options to meet the requirements for the given use-case? (Select two)</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><strong>Complete both operations on RDS MySQL in a single transaction block</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</strong> - DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. Read consistency does not facilitate DynamoDB transactions and this option has been added as a distractor.</p>\n\n<p><strong>Complete both operations on Amazon RedShift in a single transaction block</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used to manage database transactions.</p>\n\n<p><strong>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It cannot be used to manage database transactions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A social gaming application supports the transfer of gift vouchers between users. When a user hits a certain milestone on the leaderboard, they earn a gift voucher that can be redeemed or transferred to another user. The development team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.\n\nWhich of the following solutions represent the best-fit options to meet the requirements for the given use-case? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486974,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use ConsistentRead = true while doing GetItem operation for any item</p>",
                "<p>Use ConsistentRead = true while doing UpdateItem operation for any item</p>",
                "<p>Use ConsistentRead = true while doing PutItem operation for any item</p>",
                "<p>Use ConsistentRead = false while doing PutItem operation for any item</p>"
            ],
            "question": "<p>The technology team at an investment bank uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.</p>\n\n<p>Which of the following actions would make sure that only the last updated value of any item is used in the application?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ConsistentRead = true while doing GetItem operation for any item</strong></p>\n\n<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>\n\n<p>Eventually Consistent Reads</p>\n\n<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>\n\n<p>Strongly Consistent Reads</p>\n\n<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p>\n\n<p>DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = true while doing PutItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = false while doing PutItem operation for any item</strong></p>\n\n<p>As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The technology team at an investment bank uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.\n\nWhich of the following actions would make sure that only the last updated value of any item is used in the application?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487000,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</p>",
                "<p>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</p>",
                "<p>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</p>",
                "<p>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</p>"
            ],
            "question": "<p>The development team at a multi-national retail company wants to support trusted third-party authenticated users from the supplier organizations to create and update records in specific DynamoDB tables in the company's AWS account.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p>\n\n<p>Public providers: Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools).</p>\n\n<p>Amazon Cognito User Pools</p>\n\n<p>Open ID Connect Providers (Identity Pools)</p>\n\n<p>SAML Identity Providers (Identity Pools)</p>\n\n<p>Developer Authenticated Identities (Identity Pools)</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</strong> - A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Cognito User Pools cannot be used to obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB.</p>\n\n<p><strong>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</strong></p>\n\n<p><strong>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</strong></p>\n\n<p>Both these options involve setting up IAM resources such as IAM users or IAM groups just to provide access to DynamoDB tables. As the users are already trusted third-party authenticated users, Cognito Identity Pool can address this use-case in an elegant way.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a multi-national retail company wants to support trusted third-party authenticated users from the supplier organizations to create and update records in specific DynamoDB tables in the company's AWS account.\n\nAs a Developer Associate, which of the following solutions would you suggest for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487006,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Lambda supports both Windows and Linux-based container images</p>",
                "<p>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</p>",
                "<p>You can test the containers locally using the Lambda Runtime API</p>",
                "<p>You can deploy Lambda function as a container image, with a maximum size of 15 GB</p>",
                "<p>You must create the Lambda function from the same account as the container registry in Amazon ECR</p>"
            ],
            "question": "<p>A developer wants to package the code and dependencies for the application-specific Lambda functions as container images to be hosted on Amazon Elastic Container Registry (ECR).</p>\n\n<p>Which of the following options are correct for the given requirement? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</strong> - To deploy a container image to Lambda, the container image must implement the Lambda Runtime API. The AWS open-source runtime interface clients implement the API. You can add a runtime interface client to your preferred base image to make it compatible with Lambda.</p>\n\n<p><strong>You must create the Lambda function from the same account as the container registry in Amazon ECR</strong> - You can package your Lambda function code and dependencies as a container image, using tools such as the Docker CLI. You can then upload the image to your container registry hosted on Amazon Elastic Container Registry (Amazon ECR). Note that you must create the Lambda function from the same account as the container registry in Amazon ECR.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda supports both Windows and Linux-based container images</strong> - Lambda currently supports only Linux-based container images.</p>\n\n<p><strong>You can test the containers locally using the Lambda Runtime API</strong> - You can test the containers locally using the Lambda Runtime Interface Emulator.</p>\n\n<p><strong>You can deploy Lambda function as a container image, with a maximum size of 15 GB</strong> - You can deploy Lambda function as container image with the maximum size of 10GB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\">https://docs.aws.amazon.com/lambda/latest/dg/images-create.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b",
            "e"
        ],
        "section": "Deployment",
        "question_plain": "A developer wants to package the code and dependencies for the application-specific Lambda functions as container images to be hosted on Amazon Elastic Container Registry (ECR).\n\nWhich of the following options are correct for the given requirement? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486976,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</p>",
                "<p>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</p>",
                "<p>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</p>",
                "<p>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</p>"
            ],
            "question": "<p>A development team is building a game where players can buy items with virtual coins. For every virtual coin bought by a user, both the players table as well as the items table in DynamodDB need to be updated simultaneously using an all-or-nothing operation.</p>\n\n<p>As a developer associate, how will you implement this functionality?</p>\n",
            "explanation": "<p>Correct option:\n<strong>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</strong></p>\n\n<p>With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing <code>TransactWriteItems</code> or <code>TransactGetItems</code> operation.</p>\n\n<p><code>TransactWriteItems</code> is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds.</p>\n\n<p>You can optionally include a client token when you make a TransactWriteItems call to ensure that the request is idempotent. Making your transactions idempotent helps prevent application errors if the same operation is submitted multiple times due to a connection time-out or other connectivity issue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</strong> - A <code>TransactWriteItems</code> operation differs from a <code>BatchWriteItem</code> operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a <code>BatchWriteItem</code> operation, it is possible that only some of the actions in the batch succeed while the others do not.</p>\n\n<p><strong>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</strong></p>\n\n<p><strong>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</strong></p>\n\n<p>Many applications benefit from capturing changes to items stored in a DynamoDB table, at the point in time when such changes occur. DynamoDB supports streaming of item-level change data capture records in near-real-time. You can build applications that consume these streams and take action based on the contents. DynamoDB streams cannot be used to capture transactions in DynamoDB, therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A development team is building a game where players can buy items with virtual coins. For every virtual coin bought by a user, both the players table as well as the items table in DynamodDB need to be updated simultaneously using an all-or-nothing operation.\n\nAs a developer associate, how will you implement this functionality?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486966,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</p>",
                "<p>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</p>",
                "<p>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>",
                "<p>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>"
            ],
            "question": "<p>A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet.</p>\n\n<p>As a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</strong></p>\n\n<p>When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.</p>\n\n<p>Using Amazon VPC Endpoints to Access DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</strong> - You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here.</p>\n\n<p><strong>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other.</p>\n\n<p><strong>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet.\n\nAs a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486964,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Configure Timeout mechanism for each request made to the SES service</p>",
                "<p>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</p>",
                "<p>Raise a service request with Amazon to increase the throttling limit for the SES API</p>",
                "<p>Implement retry mechanism for all 4xx errors to avoid throttling error</p>"
            ],
            "question": "<p>A company uses Amazon Simple Email Service (SES) to cost-effectively send susbscription emails to the customers. Intermittently, the SES service throws the error: <code>Throttling – Maximum sending rate exceeded</code>.</p>\n\n<p>As a developer associate, which of the following would you recommend to fix this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</strong> - A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed.</p>\n\n<p>Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved.</p>\n\n<p>The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt.</p>\n\n<p>A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will self-tune and it will call Amazon SES at close to the maximum allowed rate.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Timeout mechanism for each request made to the SES service</strong> - Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying.</p>\n\n<p><strong>Raise a service request with Amazon to increase the throttling limit for the SES API</strong> - If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.</p>\n\n<p><strong>Implement retry mechanism for all 4xx errors to avoid throttling error</strong> - 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/\">https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/\">https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company uses Amazon Simple Email Service (SES) to cost-effectively send susbscription emails to the customers. Intermittently, the SES service throws the error: Throttling – Maximum sending rate exceeded.\n\nAs a developer associate, which of the following would you recommend to fix this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487048,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>When you send HTTP requests to an AWS service</p>",
                "<p>When you send anonymous requests to Amazon Simple Storage Service (Amazon S3)</p>",
                "<p>When you use the AWS Command Line Interface (AWS CLI) to run commands on an AWS resource</p>",
                "<p>When you use one of the AWS SDKs to make requests to AWS resources/services</p>"
            ],
            "question": "<p>Signing AWS API requests helps AWS identify an authentic user from a potential threat.</p>\n\n<p>As a developer associate, which of the following would you identify as the use-case where you need to sign the API requests?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>When you send HTTP requests to an AWS service</strong> - When you send HTTP requests to AWS, you sign the requests so that AWS can identify who sent them. You sign requests with your AWS access key, which consists of an access key ID and secret access key.</p>\n\n<p>When you write custom code to send HTTP requests to AWS, you need to include code to sign the requests. You might do this for the following reasons:</p>\n\n<ol>\n<li><p>You are working with a programming language for which there is no AWS SDK.</p></li>\n<li><p>You want complete control over how a request is sent to AWS.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you send anonymous requests to Amazon Simple Storage Service (Amazon S3)</strong> - Anonymous requests made to Amazon S3 resources need not be signed. Some API operations in AWS Security Token Service (AWS STS) are exempt from signing too.</p>\n\n<p><strong>When you use the AWS Command Line Interface (AWS CLI) to run commands on an AWS resource</strong></p>\n\n<p><strong>When you use one of the AWS SDKs to make requests to AWS resources/services</strong></p>\n\n<p>When you use the AWS Command Line Interface (AWS CLI) or one of the AWS SDKs to make requests to AWS, these tools automatically sign the requests for you with the access key that you specify when you configure the tools. When you use these tools, you don't need to learn how to sign requests yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html\">https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "Signing AWS API requests helps AWS identify an authentic user from a potential threat.\n\nAs a developer associate, which of the following would you identify as the use-case where you need to sign the API requests?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486962,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use CloudFront signed cookies feature to control access to the file</p>",
                "<p>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</p>",
                "<p>Use CloudFront signed URL feature to control access to the file</p>",
                "<p>Using CloudFront's Field-Level Encryption to help protect sensitive data</p>"
            ],
            "question": "<p>A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world.</p>\n\n<p>Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudFront signed URL feature to control access to the file</strong></p>\n\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.</p>\n\n<p>Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:</p>\n\n<ol>\n<li><p>In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.</p></li>\n<li><p>Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.</p></li>\n<li><p>A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.</p></li>\n<li><p>Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.</p></li>\n</ol>\n\n<p>This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user.</p>\n\n<ol>\n<li>CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront signed cookies feature to control access to the file</strong> - CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution.</p>\n\n<p>Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.</p>\n\n<p><strong>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file.</p>\n\n<p><strong>Using CloudFront's Field-Level Encryption to help protect sensitive data</strong> - CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/\">https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world.\n\nWhich of the following represents the most optimal solution to address this requirement without compromising the security of the content?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487040,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</p>",
                "<p>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</p>",
                "<p>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</p>",
                "<p>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</p>",
                "<p>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</p>"
            ],
            "question": "<p>A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions.</p>\n\n<p>Which of the following statements should the developer consider while defining the signers? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</strong> - Each signer that you use to create CloudFront signed URLs or signed cookies must have a public–private key pair. The signer uses its private key to sign the URL or cookies, and CloudFront uses the public key to verify the signature.</p>\n\n<p>When you create signed URLs or signed cookies, you use the private key from the signer’s key pair to sign a portion of the URL or the cookie. When someone requests a restricted file, CloudFront compares the signature in the URL or cookie with the unsigned URL or cookie, to verify that it hasn’t been tampered with. CloudFront also verifies that the URL or cookie is valid, meaning, for example, that the expiration date and time haven’t passed.</p>\n\n<p><strong>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</strong> - When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account.</p>\n\n<p>Whereas, with CloudFront key groups, you can associate a higher number of public keys with your CloudFront distribution, giving you more flexibility in how you use and manage the public keys. By default, you can associate up to four key groups with a single distribution, and you can have up to five public keys in a key group.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</strong> - When you use the AWS account root user to manage CloudFront key pairs, you can’t restrict what the root user can do or the conditions in which it can do them. You can’t apply IAM permissions policies to the root user, which is one reason why AWS best practices recommend against using the root user.</p>\n\n<p><strong>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</strong> - CloudFront key pairs can only be created using the root user account and hence is not a best practice to create CloudFront key pairs as signers.</p>\n\n<p><strong>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</strong> - With CloudFront key groups, you can manage public keys, key groups, and trusted signers using the CloudFront API. You can use the API to automate key creation and key rotation. When you use the AWS root user, you have to use the AWS Management Console to manage CloudFront key pairs, so you can’t automate the process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Security",
        "question_plain": "A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions.\n\nWhich of the following statements should the developer consider while defining the signers? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487066,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources</p>",
                "<p>Use Tag feature of CloudFormation to monitor the changes happening on specific resources</p>",
                "<p>Use Drift Detection feature of CloudFormation</p>",
                "<p>Use Change Sets feature of CloudFormation</p>"
            ],
            "question": "<p>An e-commerce company uses AWS CloudFormation to implement Infrastructure as Code for the entire organization. Maintaining resources as stacks with CloudFormation has greatly reduced the management effort needed to manage and maintain the resources. However, a few teams have been complaining of failing stack updates owing to out-of-band fixes running on the stack resources.</p>\n\n<p>Which of the following is the best solution that can help in keeping the CloudFormation stack and its resources in sync with each other?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Drift Detection feature of CloudFormation</strong></p>\n\n<p>Drift detection enables you to detect whether a stack's actual configuration differs, or has drifted, from its expected configuration. Use CloudFormation to detect drift on an entire stack, or individual resources within the stack. A resource is considered to have drifted if any of its actual property values differ from the expected property values. This includes if the property or resource has been deleted. A stack is considered to have drifted if one or more of its resources have drifted.</p>\n\n<p>To determine whether a resource has drifted, CloudFormation determines the expected resource property values, as defined in the stack template and any values specified as template parameters. CloudFormation then compares those expected values with the actual values of those resource properties as they currently exist in the stack. A resource is considered to have drifted if one or more of its properties have been deleted, or had their value changed.</p>\n\n<p>You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure configuration consistency and successful stack operations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources</strong> - Elastic Beanstalk environment provides full access to the resources created. So, it is possible to edit the resources and hence does not solve the issue mentioned for the given use case.</p>\n\n<p><strong>Use Tag feature of CloudFormation to monitor the changes happening on specific resources</strong> - Tags help you identify and categorize the resources created as part of CloudFormation template. This feature is not helpful for the given use case.</p>\n\n<p><strong>Use Change Sets feature of CloudFormation</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Change sets are not useful for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An e-commerce company uses AWS CloudFormation to implement Infrastructure as Code for the entire organization. Maintaining resources as stacks with CloudFormation has greatly reduced the management effort needed to manage and maintain the resources. However, a few teams have been complaining of failing stack updates owing to out-of-band fixes running on the stack resources.\n\nWhich of the following is the best solution that can help in keeping the CloudFormation stack and its resources in sync with each other?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486970,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</p>",
                "<p>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</p>",
                "<p>Use Amazon SNS to develop event-driven applications that can share information</p>",
                "<p>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</p>"
            ],
            "question": "<p>A company wants to automate its order fulfillment and inventory tracking workflow. Starting from order creation to updating inventory to shipment, the entire process has to be tracked, managed and updated automatically.</p>\n\n<p>Which of the following would you recommend as the most optimal solution for this requirement?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</strong></p>\n\n<p>AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.</p>\n\n<p>AWS Step Functions enables you to implement a business process as a series of steps that make up a workflow. The individual steps in the workflow can invoke a Lambda function or a container that has some business logic, update a database such as DynamoDB or publish a message to a queue once that step or the entire workflow completes execution.</p>\n\n<p>Benefits of Step Functions:</p>\n\n<p>Build and update apps quickly: AWS Step Functions lets you build visual workflows that enable the fast translation of business requirements into technical requirements. You can build applications in a matter of minutes, and when needs change, you can swap or reorganize components without customizing any code.</p>\n\n<p>Improve resiliency: AWS Step Functions manages state, checkpoints and restarts for you to make sure that your application executes in order and as expected. Built-in try/catch, retry and rollback capabilities deal with errors and exceptions automatically.</p>\n\n<p>Write less code: AWS Step Functions manages the logic of your application for you and implements basic primitives such as branching, parallel execution, and timeouts. This removes extra code that may be repeated in your microservices and functions.</p>\n\n<p>How Step Functions work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</strong> - You should consider AWS Step Functions when you need to coordinate service components in the development of highly scalable and auditable applications. You should consider using Amazon Simple Queue Service (Amazon SQS), when you need a reliable, highly scalable, hosted queue for sending, storing, and receiving messages between services. Step Functions keeps track of all tasks and events in an application. Amazon SQS requires you to implement your own application-level tracking, especially if your application uses multiple queues.</p>\n\n<p><strong>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, and your choice will depend on your specific needs. Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners.</p>\n\n<p><strong>Use Amazon SNS to develop event-driven applications that can share information</strong> - Amazon SNS is recommended when you want to build an application that reacts to high throughput or low latency messages published by other applications or microservices (as Amazon SNS provides nearly unlimited throughput), or for applications that need very high fan-out (thousands or millions of endpoints).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/faqs/\">https://aws.amazon.com/eventbridge/faqs/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company wants to automate its order fulfillment and inventory tracking workflow. Starting from order creation to updating inventory to shipment, the entire process has to be tracked, managed and updated automatically.\n\nWhich of the following would you recommend as the most optimal solution for this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487026,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>",
                "<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>",
                "<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>",
                "<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>"
            ],
            "question": "<p>While defining a business workflow as state machine on AWS Step Functions, a developer has configured several states.</p>\n\n<p>Which of the following would you identify as the state that represents a single unit of work performed by a state machine?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>\n\n<p>A Task state (\"Type\": \"Task\") represents a single unit of work performed by a state machine.</p>\n\n<p>All work in your state machine is done by tasks. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services.</p>\n\n<p>AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of programming languages, using the AWS Management Console or by uploading code to Lambda.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>\n\n<ul>\n<li>A Wait state (\"Type\": \"Wait\") delays the state machine from continuing for a specified time.</li>\n</ul>\n\n<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>\n\n<ul>\n<li><code>Resource</code> field is a required parameter for <code>Task</code> state. This definition is not of a <code>Task</code> but of type <code>Pass</code>.</li>\n</ul>\n\n<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>\n\n<ul>\n<li>A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. Because Fail states always exit the state machine, they have no Next field and don't require an End field.</li>\n</ul>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "While defining a business workflow as state machine on AWS Step Functions, a developer has configured several states.\n\nWhich of the following would you identify as the state that represents a single unit of work performed by a state machine?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487014,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Amazon EC2 Auto Scaling adds 3 instances to the group</p>",
                "<p>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</p>",
                "<p>Amazon EC2 Auto Scaling adds only 1 instance to the group</p>",
                "<p>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</p>"
            ],
            "question": "<p>An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a scaling policy that adds 3 instances.</p>\n\n<p>When executing this scaling policy, what is the expected outcome?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM.</p>\n\n<p>When a scaling policy is executed, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n\n<p><strong>Amazon EC2 Auto Scaling adds only 1 instance to the group</strong></p>\n\n<p>For the given use-case, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group</strong> - This is an incorrect statement. Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</strong> - This is an incorrect statement. Adding the instances initially and immediately downsizing them is impractical.</p>\n\n<p><strong>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a scaling policy that adds 3 instances.\n\nWhen executing this scaling policy, what is the expected outcome?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486984,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>To the non-failed instances</p>",
                "<p>To the failed instances</p>",
                "<p>To the new instances</p>",
                "<p>You cannot rollback a CodeDeploy deployment</p>"
            ],
            "question": "<p>You are a developer handling a deployment service that automates application deployments to Amazon EC2 instances. Most of the deployments consist of code, but sometimes web and configuration files. One of your deployments failed and was rolled back by AWS CodeDeploy to the last known good application revision.</p>\n\n<p>During rollback which of the following instances did AWS CodeDeploy deploy first to?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p><strong>To the failed instances</strong>: AWS CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment on the failed instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To the non-failed instances</strong> - Nothing happens to the non-failed instances if any.</p>\n\n<p><strong>To new instances</strong> - Nothing is deployed to the new instances.</p>\n\n<p><strong>You cannot rollback a CodeDeploy deployment</strong> - You can rollback a CodeDeploy deployment. This option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "You are a developer handling a deployment service that automates application deployments to Amazon EC2 instances. Most of the deployments consist of code, but sometimes web and configuration files. One of your deployments failed and was rolled back by AWS CodeDeploy to the last known good application revision.\n\nDuring rollback which of the following instances did AWS CodeDeploy deploy first to?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487080,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Stack B, then Stack C, then Stack A</p>",
                "<p>Stack A, then Stack B, then Stack C</p>",
                "<p>Stack A, Stack C then Stack B</p>",
                "<p>Stack C then Stack A then Stack B</p>"
            ],
            "question": "<p>As an AWS certified developer associate, you are working on an AWS CloudFormation template that will create resources for a company's cloud infrastructure. Your template is composed of three stacks which are Stack-A, Stack-B, and Stack-C. Stack-A will provision a VPC, a security group, and subnets for public web applications that will be referenced in Stack-B and Stack-C.</p>\n\n<p>After running the stacks you decide to delete them, in which order should you do it?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><strong>Stack B, then Stack C, then Stack A</strong></p>\n\n<p>All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete Stack A.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stack A, then Stack B, then Stack C</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack A, Stack C then Stack B</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack C then Stack A then Stack B</strong> - Stack C is fine but you should delete Stack B before Stack A because all of the imports must be removed before you can delete the exporting stack or modify the output value.</p>\n\n<p>For more information visit</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As an AWS certified developer associate, you are working on an AWS CloudFormation template that will create resources for a company's cloud infrastructure. Your template is composed of three stacks which are Stack-A, Stack-B, and Stack-C. Stack-A will provision a VPC, a security group, and subnets for public web applications that will be referenced in Stack-B and Stack-C.\n\nAfter running the stacks you decide to delete them, in which order should you do it?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487054,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Application Load Balancer + ECS</p>",
                "<p>Classic Load Balancer + Beanstalk</p>",
                "<p>Application Load Balancer + Beanstalk</p>",
                "<p>Classic Load Balancer + ECS</p>"
            ],
            "question": "<p>Your company has embraced cloud-native microservices architectures. New applications must be dockerized and stored in a registry service offered by AWS. The architecture should support dynamic port mapping and support multiple tasks from a single service on the same container instance. All services should run on the same EC2 instance.</p>\n\n<p>Which of the following options offers the best-fit solution for the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Application Load Balancer + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same container instance. Amazon ECS manages updates to your services by automatically registering and deregistering containers with your target group using the instance ID and port for each container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Classic Load Balancer + Beanstalk</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p><strong>Application Load Balancer + Beanstalk</strong> - You can create docker environments that support multiple containers per Amazon EC2 instance with a multi-container Docker platform for Elastic Beanstalk. However, ECS gives you finer control.</p>\n\n<p><strong>Classic Load Balancer + ECS</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task in the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "Your company has embraced cloud-native microservices architectures. New applications must be dockerized and stored in a registry service offered by AWS. The architecture should support dynamic port mapping and support multiple tasks from a single service on the same container instance. All services should run on the same EC2 instance.\n\nWhich of the following options offers the best-fit solution for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487018,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p><code>!Param</code></p>",
                "<p><code>!GetAtt</code></p>",
                "<p><code>!Ref</code></p>",
                "<p><code>!Join</code></p>"
            ],
            "question": "<p>A team lead has asked you to create an AWS CloudFormation template that creates EC2 instances and RDS databases. The template should be reusable by allowing the user to input a parameter value for an Amazon EC2 AMI ID.</p>\n\n<p>Which of the following intrinsic function should you choose to reference the parameter?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>!Ref</code></strong></p>\n\n<p>The intrinsic function <code>Ref</code> returns the value of the specified parameter or resource. When you specify a parameter's logical name, it returns the value of the parameter, when you specify a resource's logical name, it returns a value that you can typically use to refer to that resource such as a physical ID. Take a look at this YAML sample template:</p>\n\n<pre><code>MyEIP:\n  Type: \"AWS::EC2::EIP\"\n  Properties:\n    InstanceId: !Ref MyEC2Instance\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!GetAtt</code></strong> - This function returns the value of an attribute from a resource in the template. The YAML syntax is like so:</p>\n\n<p><code>!GetAtt logicalNameOfResource.attributeName</code></p>\n\n<p><strong><code>!Param</code></strong> - This is not a valid function name. This option has been added as a distractor.</p>\n\n<p><strong><code>!Join</code></strong> - This function appends a set of values into a single value, separated by the specified delimiter. The YAML syntax is like so:</p>\n\n<p><code>!Join [ delimiter, [ comma-delimited list of values ] ]</code></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A team lead has asked you to create an AWS CloudFormation template that creates EC2 instances and RDS databases. The template should be reusable by allowing the user to input a parameter value for an Amazon EC2 AMI ID.\n\nWhich of the following intrinsic function should you choose to reference the parameter?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486986,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>ASGAverageCPUUtilization</p>",
                "<p>ApproximateNumberOfMessagesVisible</p>",
                "<p>ASGAverageNetworkOut</p>",
                "<p>ALBRequestCountPerTarget</p>"
            ],
            "question": "<p>A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically.</p>\n\n<p>Which metric below is NOT part of Target Tracking Scaling Policy?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ApproximateNumberOfMessagesVisible</strong> - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.</p>\n\n<p>Incorrect options:</p>\n\n<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p>\n\n<p>It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value.</p>\n\n<p><strong>ASGAverageCPUUtilization</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.</p>\n\n<p><strong>ASGAverageNetworkOut</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.</p>\n\n<p><strong>ALBRequestCountPerTarget</strong> - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically.\n\nWhich metric below is NOT part of Target Tracking Scaling Policy?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487086,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</p>",
                "<p>Deploy a CloudFormation wrapper</p>",
                "<p>Include config files in .ebextensions/ at the root of your source code</p>",
                "<p>Use an AWS Lambda hook</p>"
            ],
            "question": "<p>You are a developer working on a web application written in Java and would like to use AWS Elastic Beanstalk for deployment because it would handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. In the past, you connected to your provisioned instances through SSH to issue configuration commands. Now, you would like a configuration mechanism that automatically applies settings for you.</p>\n\n<p>Which of the following options would help do this?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Include config files in .ebextensions/ at the root of your source code</strong></p>\n\n<p>The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy a CloudFormation wrapper</strong> - This is a made-up option. This has been added as a distractor.</p>\n\n<p><strong>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</strong> - SSM parameter is still not supported for Elastic Beanstalk. So this option is incorrect.</p>\n\n<p><strong>Use an AWS Lambda hook</strong> - Lambda functions are not the best-fit to trigger these configuration changes as it would involve significant development effort.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "You are a developer working on a web application written in Java and would like to use AWS Elastic Beanstalk for deployment because it would handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. In the past, you connected to your provisioned instances through SSH to issue configuration commands. Now, you would like a configuration mechanism that automatically applies settings for you.\n\nWhich of the following options would help do this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486958,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p>You configure monitoring for EC2 instances using a launch configuration or template. Monitoring is enabled whenever an instance is launched, either basic monitoring (5-minute granularity) or detailed monitoring (1-minute granularity).</p>\n\n<p><strong>AWS Management Console might have been used to create the launch configuration</strong> - By default, basic monitoring is enabled when you create a launch template or when you use the AWS Management Console to create a launch configuration. This could be the reason behind only the basic monitoring taking place.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/enable-as-instance-metrics.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/enable-as-instance-metrics.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CLI was used to create the launch configuration</strong> - Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI or an SDK.</p>\n\n<p><strong>SDK was used to create the launch configuration</strong> - Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI or an SDK.</p>\n\n<p><strong>The default configuration for Auto Scaling was not set</strong> - This is an invalid statement and given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/enable-as-instance-metrics.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/enable-as-instance-metrics.html</a></p>\n",
            "answers": [
                "<p>AWS Management Console might have been used to create the launch configuration</p>",
                "<p>AWS CLI was used to create the launch configuration</p>",
                "<p>SDK was used to create the launch configuration</p>",
                "<p>The default configuration for Auto Scaling was not set</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A development team has configured their Amazon EC2 instances for Auto Scaling. A Developer during routine checks has realized that only basic monitoring is active, as opposed to detailed monitoring.</p>\n\n<p>Which of the following represents the best root-cause behind the issue?</p>\n",
            "relatedLectureIds": ""
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A development team has configured their Amazon EC2 instances for Auto Scaling. A Developer during routine checks has realized that only basic monitoring is active, as opposed to detailed monitoring.\n\nWhich of the following represents the best root-cause behind the issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486972,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Stop the Spot Instance</p>",
                "<p>Hibernate the Spot Instance</p>",
                "<p>Terminate the Spot Instance</p>",
                "<p>Reboot the Spot Instance</p>"
            ],
            "question": "<p>A new recruit is trying to configure what an Amazon EC2 should do when it interrupts a Spot Instance.</p>\n\n<p>Which of the below CANNOT be configured as an interruption behavior?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated.</p>\n\n<p>You can specify that Amazon EC2 should do one of the following when it interrupts a Spot Instance:</p>\n\n<p>Stop the Spot Instance</p>\n\n<p>Hibernate the Spot Instance</p>\n\n<p>Terminate the Spot Instance</p>\n\n<p>The default is to terminate Spot Instances when they are interrupted.</p>\n\n<p><strong>Reboot the Spot Instance</strong> - This is an invalid option.</p>\n\n<p>Incorrect options:</p>\n\n<p>It is always possible that Spot Instances might be interrupted. Therefore, you must ensure that your application is prepared for a Spot Instance interruption.</p>\n\n<p><strong>Stop the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to stop the instance when an interruption occurs on Spot instances.</p>\n\n<p><strong>Hibernate the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to hibernate the instance when an interruption occurs on Spot instances.</p>\n\n<p><strong>Terminate the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to hibernate the instance when an interruption occurs on Spot instances. The default behavior is to terminate Spot Instances when they are interrupted.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A new recruit is trying to configure what an Amazon EC2 should do when it interrupts a Spot Instance.\n\nWhich of the below CANNOT be configured as an interruption behavior?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486990,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Your IAM permissions are incorrect</p>",
                "<p>The maximum file size is 5 GB</p>",
                "<p>You need to use multi-part upload for large files</p>",
                "<p>You need to place a service limit request increase with AWS</p>"
            ],
            "question": "<p>You are an administrator for a video-on-demand web application where content creators upload videos directly into S3. Recent support requests from customers state that uploading video files near 500GB size causes the website to break. After doing some investigation you find the following error: 'Your proposed upload exceeds the maximum allowed size'.</p>\n\n<p>What must you do to solve this error?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon recommends that you use multipart uploading for the following use-cases:</p>\n\n<p>If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.</p>\n\n<p>If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p><strong>You need to use multi-part upload for large files</strong>: In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The maximum file size is 5 GB</strong> - With a single PUT operation, you can upload objects up to 5 GB in size.</p>\n\n<p><strong>Your IAM permissions are incorrect</strong> - Permissions can be set to limit the size of an object put but for the most part it is good practice to use the multipart upload operation.</p>\n\n<p><strong>You need to place a service limit request increase with AWS</strong> - There is no need to do this, this option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are an administrator for a video-on-demand web application where content creators upload videos directly into S3. Recent support requests from customers state that uploading video files near 500GB size causes the website to break. After doing some investigation you find the following error: 'Your proposed upload exceeds the maximum allowed size'.\n\nWhat must you do to solve this error?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487036,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Regional Reserved Instances</p>",
                "<p>Both Regional Reserved Instances and Zonal Reserved Instances</p>",
                "<p>Neither Regional Reserved Instances nor Zonal Reserved Instances</p>",
                "<p>Zonal Reserved Instances</p>"
            ],
            "question": "<p>A media publishing company is using Amazon EC2 instances for running their business-critical applications. Their IT team is looking at reserving capacity apart from savings plans for the critical instances.</p>\n\n<p>As a Developer Associate, which of the following reserved instance types you would select to provide capacity reservations?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>When you purchase a Reserved Instance for a specific Availability Zone, it's referred to as a Zonal Reserved Instance. Zonal Reserved Instances provide capacity reservations as well as discounts.</p>\n\n<p><strong>Zonal Reserved Instances</strong> - A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.</p>\n\n<p>Regional and Zonal Reserved Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Regional Reserved Instances</strong> - When you purchase a Reserved Instance for a Region, it's referred to as a regional Reserved Instance. A regional Reserved Instance does not provide a capacity reservation.</p>\n\n<p><strong>Both Regional Reserved Instances and Zonal Reserved Instances</strong> - As discussed above, only Zonal Reserved Instances provide capacity reservation.</p>\n\n<p><strong>Neither Regional Reserved Instances nor Zonal Reserved Instances</strong> - As discussed above, Zonal Reserved Instances provide capacity reservation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A media publishing company is using Amazon EC2 instances for running their business-critical applications. Their IT team is looking at reserving capacity apart from savings plans for the critical instances.\n\nAs a Developer Associate, which of the following reserved instance types you would select to provide capacity reservations?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487068,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>X-Ray only works with AWS Lambda aliases</p>",
                "<p>Enable X-Ray sampling</p>",
                "<p>Fix the IAM Role</p>",
                "<p>Change the security group rules</p>"
            ],
            "question": "<p>Recently in your organization, the AWS X-Ray SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the X-Ray service in the AWS Management Console to get an overview of the information collected, they discover that no data is available.</p>\n\n<p>What is the most likely reason for this issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Fix the IAM Role</strong></p>\n\n<p>Create an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options.</p>\n\n<p>Here is an example of X-Ray Read-Only permissions via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\",\n                \"xray:BatchGetTraces\",\n                \"xray:GetServiceGraph\",\n                \"xray:GetTraceGraph\",\n                \"xray:GetTraceSummaries\",\n                \"xray:GetGroups\",\n                \"xray:GetGroup\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Another example of write permissions for using X-Ray via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:PutTraceSegments\",\n                \"xray:PutTelemetryRecords\",\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable X-Ray sampling</strong> - If permissions are not configured correctly sampling will not work, so this option is not correct.</p>\n\n<p><strong>X-Ray only works with AWS Lambda aliases</strong> - This is not true, aliases are pointers to specific Lambda function versions. To use the X-Ray SDK on Lambda, bundle it with your function code each time you create a new version.</p>\n\n<p><strong>Change the security group rules</strong> - You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html\">https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Recently in your organization, the AWS X-Ray SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the X-Ray service in the AWS Management Console to get an overview of the information collected, they discover that no data is available.\n\nWhat is the most likely reason for this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487078,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Using the CLI, create a dummy EC2 and delete it using another CLI call</p>",
                "<p>Use the AWS CLI --test option</p>",
                "<p>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</p>",
                "<p>Use the AWS CLI --dry-run option</p>"
            ],
            "question": "<p>You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances.</p>\n\n<p>Which of the following options would you execute?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS CLI --dry-run option</strong>: The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS CLI --test option</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</strong> - EC2 metadata service is used to retrieve dynamic information such as instance-id, local-hostname, public-hostname. This cannot be used to check whether you have the required permissions for the action.</p>\n\n<p><strong>Using the CLI, create a dummy EC2 and delete it using another CLI call</strong> - That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dry-run option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances.\n\nWhich of the following options would you execute?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486982,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</p>",
                "<p>All instances are charged at one hour of Reserved Instance usage</p>",
                "<p>All instances are charged at one hour of On-Demand Instance usage</p>",
                "<p>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</p>"
            ],
            "question": "<p>A business has purchased one m4.xlarge Reserved Instance but it has used three m4.xlarge instances concurrently for an hour.</p>\n\n<p>As a Developer, explain how the instances are charged?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>All Reserved Instances provide you with a discount compared to On-Demand pricing.</p>\n\n<p><strong>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</strong></p>\n\n<p>A Reserved Instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. You can run multiple instances concurrently, but can only receive the benefit of the Reserved Instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the On-Demand rate.</p>\n\n<p>Please review this note on the EC2 Reserved Instance types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>All instances are charged at one hour of Reserved Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>All instances are charged at one hour of On-Demand Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</strong> - This is incorrect. If multiple eligible instances are running concurrently, the Reserved Instance billing benefit is applied to all the instances at the same time up to a maximum of 3600 seconds in a clock-hour; thereafter, On-Demand rates apply.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A business has purchased one m4.xlarge Reserved Instance but it has used three m4.xlarge instances concurrently for an hour.\n\nAs a Developer, explain how the instances are charged?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487002,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>10.6 TiB</p>",
                "<p>5.3 TiB</p>",
                "<p>16 TiB</p>",
                "<p>2.7 TiB</p>"
            ],
            "question": "<p>A business has their test environment built on Amazon EC2 configured on General purpose SSD volume.</p>\n\n<p>At which gp2 volume size will their test environment hit the max IOPS?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate I/O credits faster.</p>\n\n<p><strong>5.3 TiB</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size.</p>\n\n<p>Maximum IOPS vs Volume Size for General Purpose SSD (gp2) volumes:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>10.6 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>16 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>2.7 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A business has their test environment built on Amazon EC2 configured on General purpose SSD volume.\n\nAt which gp2 volume size will their test environment hit the max IOPS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487022,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>200 GiB size volume with 2000 IOPS</p>",
                "<p>200 GiB size volume with 10000 IOPS</p>",
                "<p>200 GiB size volume with 15000 IOPS</p>",
                "<p>200 GiB size volume with 5000 IOPS</p>"
            ],
            "question": "<p>A pharmaceutical company runs their database workloads on Provisioned IOPS SSD (io1) volumes.</p>\n\n<p>As a Developer Associate, which of the following options would you identify as an <strong>INVALID</strong> configuration for io1 EBS volume types?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>200 GiB size volume with 15000 IOPS</strong> - This is an invalid configuration. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 200 GiB volume size, max IOPS possible is 200*50 = 10000 IOPS.</p>\n\n<p>Overview of Provisioned IOPS SSD (io1) volumes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>Provisioned IOPS SSD (io1) volumes allow you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. An io1 volume can range in size from 4 GiB to 16 TiB. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1. For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS.</p>\n\n<p><strong>200 GiB size volume with 2000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 10000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 5000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A pharmaceutical company runs their database workloads on Provisioned IOPS SSD (io1) volumes.\n\nAs a Developer Associate, which of the following options would you identify as an INVALID configuration for io1 EBS volume types?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487044,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
                "<p>The second statement in this policy provides the security group (mentioned in first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</p>",
                "<p>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
                "<p>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</p>"
            ],
            "question": "<p>An Accounting firm extensively uses Amazon EBS volumes for persistent storage of application data of Amazon EC2 instances. The volumes are encrypted to protect the critical data of the clients. As part of managing the security credentials, the project manager has come across a policy snippet that looks like the following:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow for use of this Key\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:GenerateDataKeyWithoutPlaintext\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Allow for EC2 Use\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                \"kms:ViaService\": \"ec2.us-west-2.amazonaws.com\"\n            }\n        }\n    ]\n}\n</code></pre>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</strong> - To create and use an encrypted Amazon Elastic Block Store (EBS) volume, you need permissions to use Amazon EBS. The key policy associated with the CMK would need to include these. The above policy is an example of one such policy.</p>\n\n<p>In this CMK policy, the first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary. These two APIs are necessary to encrypt the EBS volume while it’s attached to an Amazon Elastic Compute Cloud (EC2) instance.</p>\n\n<p>The second statement in this policy provides the specified IAM principal the ability to create, list, and revoke grants for Amazon EC2. Grants are used to delegate a subset of permissions to AWS services, or other principals, so that they can use your keys on your behalf. In this case, the condition policy explicitly ensures that only Amazon EC2 can use the grants. Amazon EC2 will use them to re-attach an encrypted EBS volume back to an instance if the volume gets detached due to a planned or unplanned outage. These events will be recorded within AWS CloudTrail when, and if, they do occur for your\nauditing.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</strong></p>\n\n<p><strong>The second statement in this policy provides the security group (mentioned in the first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p><strong>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf\">https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "An Accounting firm extensively uses Amazon EBS volumes for persistent storage of application data of Amazon EC2 instances. The volumes are encrypted to protect the critical data of the clients. As part of managing the security credentials, the project manager has come across a policy snippet that looks like the following:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow for use of this Key\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:GenerateDataKeyWithoutPlaintext\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Allow for EC2 Use\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                \"kms:ViaService\": \"ec2.us-west-2.amazonaws.com\"\n            }\n        }\n    ]\n}",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487056,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>RDS Oracle</p>",
                "<p>RDS Maria DB</p>",
                "<p>RDS MySQL</p>",
                "<p>RDS PostGreSQL</p>",
                "<p>RDS Sequel Server</p>"
            ],
            "question": "<p>The Development team at a media company is working on securing their databases.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL.</p>\n\n<p><strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS Maria DB</strong></p>\n\n<p><strong>RDS Sequel Server</strong></p>\n\n<p>These three options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Security",
        "question_plain": "The Development team at a media company is working on securing their databases.\n\nWhich of the following AWS database engines can be configured with IAM Database Authentication? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486998,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>",
                "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
                "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
                "<p>Define an <code>appspec.yml</code> file in the root directory</p>"
            ],
            "question": "<p>A developer in your company was just promoted to Team Lead and will be in charge of code deployment on EC2 instances via AWS CodeCommit and AWS CodeDeploy. Per the new requirements, the deployment process should be able to change permissions for deployed files as well as verify the deployment success.</p>\n\n<p>Which of the following actions should the new Developer take?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong>: An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>The AppSpec file is used to:</p>\n\n<p>Map the source files in your application revision to their destinations on the instance.</p>\n\n<p>Specify custom permissions for deployed files.</p>\n\n<p>Specify scripts to be run on each instance at various stages of the deployment process.</p>\n\n<p>During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file. The status of each script is logged in the CodeDeploy agent log file on the instance.</p>\n\n<p>If a script runs successfully, it returns an exit code of 0 (zero). If the CodeDeploy agent installed on the operating system doesn't match what's listed in the AppSpec file, the deployment fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - This file is for AWS CodeDeploy and must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "A developer in your company was just promoted to Team Lead and will be in charge of code deployment on EC2 instances via AWS CodeCommit and AWS CodeDeploy. Per the new requirements, the deployment process should be able to change permissions for deployed files as well as verify the deployment success.\n\nWhich of the following actions should the new Developer take?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487052,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>BeanStalk</p>",
                "<p>Cognito Identity Pools</p>",
                "<p>Cognito Sync</p>",
                "<p>Cognito User Pools</p>"
            ],
            "question": "<p>A university has created a student portal that is accessible through a smartphone app and web application. The smartphone app is available in both Android and IOS and the web application works on most major browsers. Students will be able to do group study online and create forum questions. All changes made via smartphone devices should be available even when offline and should synchronize with other devices.</p>\n\n<p>Which of the following AWS services will meet these requirements?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Cognito Sync</strong></p>\n\n<p>Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cognito Identity Pools</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p><strong>Cognito User Pools</strong> - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p><strong>Beanstalk</strong> - With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A university has created a student portal that is accessible through a smartphone app and web application. The smartphone app is available in both Android and IOS and the web application works on most major browsers. Students will be able to do group study online and create forum questions. All changes made via smartphone devices should be available even when offline and should synchronize with other devices.\n\nWhich of the following AWS services will meet these requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487050,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>API Gateway exposing Lambda Functionality</p>",
                "<p>Fargate with Lambda at the front</p>",
                "<p>Public-facing Application Load Balancer with ECS on Amazon EC2</p>",
                "<p>Route 53 with EC2 as backend</p>"
            ],
            "question": "<p>As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture.</p>\n\n<p>Which of the following solutions will you choose to move the company to the serverless architecture paradigm?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway exposing Lambda Functionality</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless.</p>\n\n<p><strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.</p>\n\n<p><strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture.\n\nWhich of the following solutions will you choose to move the company to the serverless architecture paradigm?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487072,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>AWS CodePipeline</p>",
                "<p>AWS CodeBuild</p>",
                "<p>AWS Elastic Beanstalk</p>",
                "<p>AWS CodeDeploy</p>"
            ],
            "question": "<p>A developer needs to automate software package deployment to both Amazon EC2 instances and virtual servers running on-premises, as part of continuous integration and delivery that the business has adopted.</p>\n\n<p>Which AWS service should he use to accomplish this task?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Continuous integration is a DevOps software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run.</p>\n\n<p>Continuous delivery is a software development practice where code changes are automatically prepared for a release to production. A pillar of modern application development, continuous delivery expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage.</p>\n\n<p><strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed \"deployment\" service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. This is the right choice for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates. Whereas CodeDeploy is a deployment service, CodePipeline is a continuous delivery service. For our current scenario, CodeDeploy is the correct choice.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codedeploy/\">https://aws.amazon.com/codedeploy/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "A developer needs to automate software package deployment to both Amazon EC2 instances and virtual servers running on-premises, as part of continuous integration and delivery that the business has adopted.\n\nWhich AWS service should he use to accomplish this task?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486994,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>AWS CodePipeline</p>",
                "<p>Amazon Versioned S3 Bucket</p>",
                "<p>AWS CodeCommit</p>",
                "<p>AWS CodeBuild</p>"
            ],
            "question": "<p>A company needs a version control system for their fast development lifecycle with incremental changes, version control, and support to existing Git tools.</p>\n\n<p>Which AWS service will meet these requirements?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CodeCommit</strong> - AWS CodeCommit is a fully-managed Source Control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching and merging. AWS CodeCommit keeps your repositories close to your build, staging, and production environments in the AWS cloud. You can transfer incremental changes instead of the entire application.\nAWS CodeCommit supports all Git commands and works with your existing Git tools. You can keep using your preferred development environment plugins, continuous integration/continuous delivery systems, and graphical clients with CodeCommit.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Versioned S3 Bucket</strong> - AWS CodeCommit is designed for collaborative software development. It manages batches of changes across multiple files, offers parallel branching, and includes version differencing (\"diffing\"). In comparison, Amazon S3 versioning supports recovering past versions of individual files but doesn’t support tracking batched changes that span multiple files or other features needed for collaborative software development.</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A company needs a version control system for their fast development lifecycle with incremental changes, version control, and support to existing Git tools.\n\nWhich AWS service will meet these requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487034,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>10</p>",
                "<p>60</p>",
                "<p>30</p>",
                "<p>20</p>"
            ],
            "question": "<p>As a senior architect, you are responsible for the development, support, maintenance, and implementation of all database applications written using NoSQL technology. A new project demands a throughput requirement of 10 strongly consistent reads per second of 6KB in size each.</p>\n\n<p>How many read capacity units will you need when configuring your DynamoDB table?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>20</strong></p>\n\n<p>One read capacity unit represents one strongly consistent read per second for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 6KB / 4 KB = 1.5 or 2 read capacity units.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second</p>\n\n<p>So, in the above case, 2 x 10 = 20 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>60</strong></p>\n\n<p><strong>30</strong></p>\n\n<p><strong>10</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a senior architect, you are responsible for the development, support, maintenance, and implementation of all database applications written using NoSQL technology. A new project demands a throughput requirement of 10 strongly consistent reads per second of 6KB in size each.\n\nHow many read capacity units will you need when configuring your DynamoDB table?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487024,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Developers need IAM permissions on API execution component of API Gateway</p>",
                "<p>Enable Lambda authorizer to access API</p>",
                "<p>Use Stage Variables for development state of API</p>",
                "<p>Redeploy the API to an existing stage or to a new stage</p>"
            ],
            "question": "<p>As a Senior Developer, you are tasked with creating several API Gateway powered APIs along with your team of developers. The developers are working on the API in the development environment, but they find the changes made to the APIs are not reflected when the API is called.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Redeploy the API to an existing stage or to a new stage</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. Every time you update an API, you must redeploy the API to an existing stage or to a new stage. Updating an API includes modifying routes, methods, integrations, authorizers, and anything else other than stage settings.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Developers need IAM permissions on API execution component of API Gateway</strong> - Access control access to Amazon API Gateway APIs is done with IAM permissions. To call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. In the current scenario, developers do not need permissions on \"execution components\" but on \"management components\" of API Gateway that help them to create, deploy, and manage an API. Hence, this statement is an incorrect option.</p>\n\n<p><strong>Enable Lambda authorizer to access API</strong> - A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. So, this feature too helps in access control, but in the current scenario its the developers and not the users who are facing the issue. So, this statement is an incorrect option.</p>\n\n<p><strong>Use Stage Variables for development state of API</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. Stage variables are not connected to the scenario described in the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a Senior Developer, you are tasked with creating several API Gateway powered APIs along with your team of developers. The developers are working on the API in the development environment, but they find the changes made to the APIs are not reflected when the API is called.\n\nAs a Developer Associate, which of the following solutions would you recommend for this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487046,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</p>",
                "<p>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</p>",
                "<p>Amazon Redshift with Amazon S3</p>",
                "<p>Amazon ElastiCache with Amazon S3 as backup</p>"
            ],
            "question": "<p>A developer working with EC2 Windows instance has installed Kinesis Agent for Windows to stream JSON-formatted log files to Amazon Simple Storage Service (S3) via Amazon Kinesis Data Firehose. The developer wants to understand the sink type capabilities of Kinesis Firehose.</p>\n\n<p>Which of the following sink types is NOT supported by Kinesis Firehose.</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified.</p>\n\n<p><strong>Amazon ElastiCache with Amazon S3 as backup</strong> - Amazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached. ElastiCache is NOT a supported destination for Amazon Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</strong> - Amazon ES is a supported destination type for Kinesis Firehose. Streaming data is delivered to your Amazon ES cluster, and can optionally be backed up to your S3 bucket concurrently.</p>\n\n<p>Data Flow for ES:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</strong> - For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p><strong>Amazon Redshift with Amazon S3</strong> - For Amazon Redshift destinations, streaming data is delivered to your S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for Redshift:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer working with EC2 Windows instance has installed Kinesis Agent for Windows to stream JSON-formatted log files to Amazon Simple Storage Service (S3) via Amazon Kinesis Data Firehose. The developer wants to understand the sink type capabilities of Kinesis Firehose.\n\nWhich of the following sink types is NOT supported by Kinesis Firehose.",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487038,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Batch writes</p>",
                "<p>Conditional writes</p>",
                "<p>Atomic Counters</p>",
                "<p>Use Scan operation</p>"
            ],
            "question": "<p>A startup has been experimenting with DynamoDB in its new test environment. The development team has discovered that some of the write operations have been overwriting existing items that have the specified primary key. This has messed up their data, leading to data discrepancies.</p>\n\n<p>Which DynamoDB write option should be selected to prevent this kind of overwriting?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</p>\n\n<p>For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the current scenario of overwriting changes.</p>\n\n<p><strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the current scenario.</p>\n\n<p><strong>Use Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This is given as a distractor and not related to DynamoDB item updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A startup has been experimenting with DynamoDB in its new test environment. The development team has discovered that some of the write operations have been overwriting existing items that have the specified primary key. This has messed up their data, leading to data discrepancies.\n\nWhich DynamoDB write option should be selected to prevent this kind of overwriting?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487032,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>dynamodb:AddItem, dynamodb:GetItem</p>",
                "<p>dynamodb:UpdateItem, dynamodb:GetItem</p>",
                "<p>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</p>",
                "<p>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem</strong> - With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation.</p>\n\n<p>You can use AWS Identity and Access Management (IAM) to restrict the actions that transactional operations can perform in Amazon DynamoDB. Permissions for Put, Update, Delete, and Get actions are governed by the permissions used for the underlying PutItem, UpdateItem, DeleteItem, and GetItem operations. For the ConditionCheck action, you can use the <code>dynamodb:ConditionCheck</code> permission in IAM policies.</p>\n\n<p><code>UpdateItem</code> action of DynamoDB APIs, edits an existing item's attributes or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values. You can also perform a conditional update on an existing item (insert a new attribute name-value pair if it doesn't exist, or replace an existing name-value pair if it has certain expected attribute values).</p>\n\n<p>There is no need to inlcude the <code>dynamodb:PutItem</code> action for the given use-case.</p>\n\n<p>So, the IAM policy must include permissions to get and update the item in the DynamoDB table.</p>\n\n<p>Actions defined by DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>dynamodb:AddItem, dynamodb:GetItem</strong></p>\n\n<p><strong>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</strong></p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n",
            "question": "<p>A development team is working on an AWS Lambda function that accesses DynamoDB. The Lambda function must do an upsert, that is, it must retrieve an item and update some of its attributes or create the item if it does not exist.</p>\n\n<p>Which of the following represents the solution with MINIMUM IAM permissions that can be used for the Lambda function to achieve this functionality?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A development team is working on an AWS Lambda function that accesses DynamoDB. The Lambda function must do an upsert, that is, it must retrieve an item and update some of its attributes or create the item if it does not exist.\n\nWhich of the following represents the solution with MINIMUM IAM permissions that can be used for the Lambda function to achieve this functionality?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487076,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>VPN logs</p>",
                "<p>Subnet logs</p>",
                "<p>VPC Flow Logs</p>",
                "<p>BGP logs</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p>\n\n<p>You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.</p>\n\n<p>Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow.</p>\n\n<p>To create a flow log, you specify:</p>\n\n<ol>\n<li><p>The resource for which to create the flow log</p></li>\n<li><p>The type of traffic to capture (accepted traffic, rejected traffic, or all traffic)</p></li>\n<li><p>The destinations to which you want to publish the flow log data</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPN logs</strong></p>\n\n<p><strong>Subnet logs</strong></p>\n\n<p><strong>BGP logs</strong></p>\n\n<p>These three options are incorrect and have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n",
            "question": "<p>A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its on-premises data center to Amazon EC2 instances in the company’s account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC.</p>\n\n<p>Which logs can be used to verify whether the traffic is reaching subnet B?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its on-premises data center to Amazon EC2 instances in the company’s account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC.\n\nWhich logs can be used to verify whether the traffic is reaching subnet B?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486980,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>String</p>",
                "<p>DependentParameter</p>",
                "<p>CommaDelimitedList</p>",
                "<p>AWS::EC2::KeyPair::KeyName</p>"
            ],
            "question": "<p>Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWS-specific parameter types to catch invalid values.</p>\n\n<p>When specifying parameters which of the following is not a valid Parameter type?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.</p>\n\n<p>CloudFormation currently supports the following parameter types:</p>\n\n<pre><code>String – A literal string\nNumber – An integer or float\nList&lt;Number&gt; – An array of integers or floats\nCommaDelimitedList – An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id – A security group ID\nAWS::EC2::Subnet::Id – A subnet ID\nAWS::EC2::VPC::Id – A VPC ID\nList&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs\nList&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs\nList&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs\n</code></pre>\n\n<p><strong>DependentParameter</strong></p>\n\n<p>In CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>String</strong></p>\n\n<p><strong>CommaDelimitedList</strong></p>\n\n<p><strong>AWS::EC2::KeyPair::KeyName</strong></p>\n\n<p>As mentioned in the explanation above, these are valid parameter types.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/\">https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWS-specific parameter types to catch invalid values.\n\nWhen specifying parameters which of the following is not a valid Parameter type?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487004,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>It is not possible to reuse SSH key pairs across AWS Regions</p>",
                "<p>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</p>",
                "<p>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</p>",
                "<p>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</p>"
            ],
            "question": "<p>A company runs its flagship application on a fleet of Amazon EC2 instances. After misplacing a couple of private keys from the SSH key pairs, they have decided to re-use their SSH key pairs for the different instances across AWS Regions.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</strong></p>\n\n<p>Here is the correct way of reusing SSH keys in your AWS Regions:</p>\n\n<ol>\n<li><p>Generate a public SSH key (.pub) file from the private SSH key (.pem) file.</p></li>\n<li><p>Set the AWS Region you wish to import to.</p></li>\n<li><p>Import the public SSH key into the new Region.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>It is not possible to reuse SSH key pairs across AWS Regions</strong> - As explained above, it is possible to reuse with manual import.</p>\n\n<p><strong>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</strong> - AWS Trusted Advisor is an application that draws upon best practices learned from AWS' aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It does not store key pair credentials.</p>\n\n<p><strong>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</strong> - Storing private key to Amazon S3 is possible. But, this will not make the key accessible for all AWS Regions, as is the need in the current use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A company runs its flagship application on a fleet of Amazon EC2 instances. After misplacing a couple of private keys from the SSH key pairs, they have decided to re-use their SSH key pairs for the different instances across AWS Regions.\n\nAs a Developer Associate, which of the following would you recommend to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487042,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>The instance's subnet is not associated with any route table</p>",
                "<p>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</p>",
                "<p>The instance's subnet is associated with multiple route tables with conflicting configurations</p>",
                "<p>The route table in the instance’s subnet should have a route to an Internet Gateway</p>",
                "<p>The subnet has been configured to be Public and has no access to the internet</p>"
            ],
            "question": "<p>While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.</p>\n\n<p>Which conditions should be met for Internet connectivity to be established? (Select two)</p>\n",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity</p>\n\n<p><strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.</p>\n\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time.</p>\n\n<p><strong>The subnet has been configured to be Public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.\n\nWhich conditions should be met for Internet connectivity to be established? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487030,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use KMS direct encryption and store as file</p>",
                "<p>Use Envelope Encryption and reference the data as file within the code</p>",
                "<p>Use Envelope Encryption and store as environment variable</p>",
                "<p>Use KMS Encryption and store as environment variable</p>"
            ],
            "question": "<p>You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime.</p>\n\n<p>Which of the following methods is suitable to address the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Envelope Encryption and reference the data as file within the code</strong></p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS direct encryption and store as file</strong> - You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use Envelope Encryption and store as an environment variable</strong> - Environment variables must not exceed 4 KB, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use KMS Encryption and store as an environment variable</strong> - You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime.\n\nWhich of the following methods is suitable to address the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487082,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>You have created a continuous delivery service model with automated steps using AWS CodePipeline. Your pipeline uses your code, maintained in a CodeCommit repository, AWS CodeBuild, and AWS Elastic Beanstalk to automatically deploy your code every time there is a code change. However, the deployment to Elastic Beanstalk is taking a very long time due to resolving dependencies on all of your 100 target EC2 instances.</p>\n\n<p>Which of the following actions should you take to improve performance with limited code changes?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Bundle the dependencies in the source code during the build stage of CodeBuild\"</p>\n\n<p>AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.</p>\n\n<p>A typical application build process includes phases like preparing the environment, updating the configuration, downloading dependencies, running unit tests, and finally, packaging the built artifact.</p>\n\n<p>Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies.</p>\n\n<p>This will allow the code bundle to be deployed to Elastic Beanstalk to have both the dependencies and the code, hence speeding up the deployment time to Elastic Beanstalk</p>\n\n<p>Incorrect options:</p>\n\n<p>\"Bundle the dependencies in the source code in CodeCommit\" - This is not the best practice and could make the CodeCommit repository huge.</p>\n\n<p>\"Store the dependencies in S3, to be used while deploying to Beanstalk\" - This option acts as a distractor. S3 can be used as a storage location for your source code, logs, and other artifacts that are created when you use Elastic Beanstalk. Dependencies are used during the process of building code, not while deploying to Beanstalk.</p>\n\n<p>\"Create a custom platform for Elastic Beanstalk\" - This is a more advanced feature that requires code changes, so does not fit the use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n",
            "answers": [
                "<p>Store the dependencies in S3, to be used while deploying to Beanstalk</p>",
                "<p>Bundle the dependencies in the source code in CodeCommit</p>",
                "<p>Bundle the dependencies in the source code during the build stage of CodeBuild</p>",
                "<p>Create a custom platform for Elastic Beanstalk</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "You have created a continuous delivery service model with automated steps using AWS CodePipeline. Your pipeline uses your code, maintained in a CodeCommit repository, AWS CodeBuild, and AWS Elastic Beanstalk to automatically deploy your code every time there is a code change. However, the deployment to Elastic Beanstalk is taking a very long time due to resolving dependencies on all of your 100 target EC2 instances.\n\nWhich of the following actions should you take to improve performance with limited code changes?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487062,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Standard AWS IAM roles and policies</p>",
                "<p>AWS Security Token Service (STS)</p>",
                "<p>Lambda Authorizer</p>",
                "<p>Cognito User Pools</p>"
            ],
            "question": "<p>A developer is looking at establishing access control for an API that connects to a Lambda function downstream.</p>\n\n<p>Which of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><strong>AWS Security Token Service (STS)</strong> - AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.</p>\n\n<p>API Gateway supports the following mechanisms for authentication and authorization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard AWS IAM roles and policies</strong> - Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</p>\n\n<p><strong>Lambda Authorizer</strong> - Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</p>\n\n<p><strong>Cognito User Pools</strong> - Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html\">https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A developer is looking at establishing access control for an API that connects to a Lambda function downstream.\n\nWhich of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487058,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>ValidateService</p>",
                "<p>AfterInstall</p>",
                "<p>ApplicationStart</p>",
                "<p>AllowTraffic</p>"
            ],
            "question": "<p>A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success.</p>\n\n<p>Which of the following hook events should be used to verify the success of the deployment?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n\n<p><strong>ValidateService</strong>: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AfterInstall</strong> - You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions</p>\n\n<p><strong>ApplicationStart</strong> - You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop</p>\n\n<p><strong>AllowTraffic</strong> - During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success.\n\nWhich of the following hook events should be used to verify the success of the deployment?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487064,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</p>",
                "<p>A DDoS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</p>",
                "<p>Object Encryption has been enabled and each object is stored twice as part of this configuration</p>",
                "<p>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</p>"
            ],
            "question": "<p>A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer.</p>\n\n<p>Which of the following reasons explains this behavior?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</strong> - When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q57-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</strong> - This is an incorrect statement. A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it.</p>\n\n<p><strong>A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</strong> - This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket.</p>\n\n<p><strong>Object Encryption has been enabled and each object is stored twice as part of this configuration</strong> - Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenario</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer.\n\nWhich of the following reasons explains this behavior?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487020,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</p>",
                "<p>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</p>",
                "<p>Use AWS Glue to copy your table to Amazon S3 and download locally</p>",
                "<p>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</p>"
            ],
            "question": "<p>A diagnostic lab stores its data on DynamoDB. The lab wants to backup a particular DynamoDB table data on Amazon S3, so it can download the S3 backup locally for some operational use.</p>\n\n<p>Which of the following options is NOT feasible?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</strong> - This option is not feasible for the given use-case. DynamoDB has two built-in backup methods (On-demand, Point-in-time recovery) that write to Amazon S3, but you will not have access to the S3 buckets that are used for these backups.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</strong> - This is the easiest method. This method is used when you want to make a one-time backup using the lowest amount of AWS resources possible. Data Pipeline uses Amazon EMR to create the backup, and the scripting is done for you. You don't have to learn Apache Hive or Apache Spark to accomplish this task.</p>\n\n<p><strong>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</strong> - Use Hive to export data to an S3 bucket. Or, use the open-source emr-dynamodb-connector to manage your own custom backup method in Spark or Hive. These methods are the best practice to use if you're an active Amazon EMR user and are comfortable with Hive or Spark. These methods offer more control than the Data Pipeline method.</p>\n\n<p><strong>Use AWS Glue to copy your table to Amazon S3 and download locally</strong> - Use AWS Glue to copy your table to Amazon S3. This is the best practice to use if you want automated, continuous backups that you can also use in another service, such as Amazon Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A diagnostic lab stores its data on DynamoDB. The lab wants to backup a particular DynamoDB table data on Amazon S3, so it can download the S3 backup locally for some operational use.\n\nWhich of the following options is NOT feasible?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486968,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Increase the VisibilityTimeout</p>",
                "<p>Use DeleteMessage</p>",
                "<p>Reduce the VisibilityTimeout</p>",
                "<p>Implement a Dead-Letter Queue</p>"
            ],
            "question": "<p>An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors.  These messages need to be isolated for further processing and troubleshooting.</p>\n\n<p>Which of the following options will help achieve this?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead-Letter Queue</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nAmazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case.</p>\n\n<p><strong>Use DeleteMessage</strong> - Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors.  These messages need to be isolated for further processing and troubleshooting.\n\nWhich of the following options will help achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487016,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use CloudWatch Events</p>",
                "<p>Use AWS Lambda integration</p>",
                "<p>Enable S3 and CloudWatch Logs integration</p>",
                "<p>Use AWS CloudTrail and deliver logs to S3</p>"
            ],
            "question": "<p>As a Team Lead, you are expected to generate a report of the code builds for every week to report internally and to the client. This report consists of the number of code builds performed for a week, the percentage success and failure, and overall time spent on these builds by the team members. You also need to retrieve the CodeBuild logs for failed builds and analyze them in Athena.</p>\n\n<p>Which of the following options will help achieve this?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable S3 and CloudWatch Logs integration</strong> - AWS CodeBuild monitors functions on your behalf and reports metrics through Amazon CloudWatch. These metrics include the number of total builds, failed builds, successful builds, and the duration of builds. You can monitor your builds at two levels: Project level, AWS account level. You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudWatch Events</strong> - You can integrate CloudWatch Events with CodeBuild. However, we are looking at storing and running queries on logs, so Cloudwatch logs with S3 integration makes sense for this context.o</p>\n\n<p><strong>Use AWS Lambda integration</strong> - Lambda is a good choice to use boto3 library to read logs programmatically. But, CloudWatch and S3 integration is already built-in and is an optimized way of managing the given use-case.</p>\n\n<p><strong>Use AWS CloudTrail and deliver logs to S3</strong> - AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild. This is an important feature for monitoring a service but isn't a good fit for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a Team Lead, you are expected to generate a report of the code builds for every week to report internally and to the client. This report consists of the number of code builds performed for a week, the percentage success and failure, and overall time spent on these builds by the team members. You also need to retrieve the CodeBuild logs for failed builds and analyze them in Athena.\n\nWhich of the following options will help achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487070,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use Bucket policy to block the unintended access</p>",
                "<p>Share pre-signed URLs with resources that need access</p>",
                "<p>Use Routing policies to re-route unintended access</p>",
                "<p>It is not possible to implement time constraints on Amazon S3 Bucket access</p>"
            ],
            "question": "<p>After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a time-bound constraint.</p>\n\n<p>Which of the following options will address the given use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Share pre-signed URLs with resources that need access</strong> - All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Bucket policy to block the unintended access</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide time-based access, as is the case in the current use case.</p>\n\n<p><strong>Use Routing policies to re-route unintended access</strong> - There is no such facility directly available with Amazon S3.</p>\n\n<p><strong>It is not possible to implement time constraints on Amazon S3 Bucket access</strong> - This is an incorrect statement. As explained above, it is possible to give time-bound access permissions on S3 buckets and objects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a time-bound constraint.\n\nWhich of the following options will address the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486988,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>The required IAM permissions are missing</p>",
                "<p>EBS volumes are region locked</p>",
                "<p>EBS volumes are AZ locked</p>",
                "<p>The EBS volume is encrypted</p>"
            ],
            "question": "<p>A developer with access to the AWS Management Console terminated an instance in the us-east-1a availability zone. The attached EBS volume remained and is now available for attachment to other instances. Your colleague launches a new Linux EC2 instance in the us-east-1e availability zone and is attempting to attach the EBS volume. Your colleague informs you that it is not possible and need your help.</p>\n\n<p>Which of the following explanations would you provide to them?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>EBS volumes are AZ locked</strong></p>\n\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.</p>\n\n<p>When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.</p>\n\n<p>![EBS Volume Overview]https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q62-i1.jpg)\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region.</p>\n\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone.</p>\n\n<p><strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer with access to the AWS Management Console terminated an instance in the us-east-1a availability zone. The attached EBS volume remained and is now available for attachment to other instances. Your colleague launches a new Linux EC2 instance in the us-east-1e availability zone and is attempting to attach the EBS volume. Your colleague informs you that it is not possible and need your help.\n\nWhich of the following explanations would you provide to them?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487084,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>It represents a Lambda function definition</p>",
                "<p>It represents an intrinsic function</p>",
                "<p>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</p>",
                "<p>Presence of <code>Transform</code> section indicates it is a CloudFormation Parameter</p>"
            ],
            "question": "<p>As a Developer, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains <code>Transform: 'AWS::Serverless-2016-10-31'</code>.</p>\n\n<p>What does the <code>Transform</code> section in the document represent?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template.</p>\n\n<p><strong>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, the presence of the <code>Transform</code> section indicates, the document is a SAM template.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to <code>Transform</code> section.</p>\n\n<p><strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, similar to below code:</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a Developer, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains Transform: 'AWS::Serverless-2016-10-31'.\n\nWhat does the Transform section in the document represent?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39486960,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>The ASG will terminate the EC2 Instance</p>",
                "<p>The ASG will detach the EC2 instance from the group, and leave it running</p>",
                "<p>The ASG will keep the instance running and re-start the application</p>",
                "<p>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</p>"
            ],
            "question": "<p>You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy.</p>\n\n<p>Which of the following actions will take place?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The ASG will terminate the EC2 Instance</strong></p>\n\n<p>To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it</p>\n\n<p><strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application</p>\n\n<p><strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy.\n\nWhich of the following actions will take place?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 39487074,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>no limit</p>",
                "<p>10000</p>",
                "<p>100000</p>",
                "<p>10000000</p>"
            ],
            "question": "<p>A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system.</p>\n\n<p>Which of the following represents the maximum number of messages that can be stored in an SQS queue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>\"no limit\": There are no message limits for storing in SQS, but 'in-flight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Incorrect options:</p>\n\n<p>\"10000\"</p>\n\n<p>\"100000\"</p>\n\n<p>\"10000000\"</p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system.\n\nWhich of the following represents the maximum number of messages that can be stored in an SQS queue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790090,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use DelaySeconds parameter</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement application-side delay</strong> - You can customize your application to delay sending messages but it is not a robust solution. You can run into a scenario where your application crashes before sending a message, then that message would be lost.</p>\n\n<p><strong>Use visibility timeout parameter</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Enable LongPolling</strong> - Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. You cannot use LongPolling to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
            "answers": [
                "<p>Implement application-side delay</p>",
                "<p>Use DelaySeconds parameter</p>",
                "<p>Use visibility timeout parameter</p>",
                "<p>Enable LongPolling</p>"
            ],
            "question": "<p>A senior cloud engineer designs and deploys online fraud detection solutions for credit card companies processing millions of transactions daily. The Elastic Beanstalk application sends files to Amazon S3 and then sends a message to an Amazon SQS queue containing the path of the uploaded file in S3. The engineer wants to postpone the delivery of any new messages to the queue for at least 10 seconds.</p>\n\n<p>Which SQS feature should the engineer leverage?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A senior cloud engineer designs and deploys online fraud detection solutions for credit card companies processing millions of transactions daily. The Elastic Beanstalk application sends files to Amazon S3 and then sends a message to an Amazon SQS queue containing the path of the uploaded file in S3. The engineer wants to postpone the delivery of any new messages to the queue for at least 10 seconds.\n\nWhich SQS feature should the engineer leverage?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790166,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A communication platform serves millions of customers and deploys features in a production environment on AWS via CodeDeploy. You are reviewing scripts for the deployment process located in the AppSec file.</p>\n\n<p>Which of the following options lists the correct order of lifecycle events?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>DownloadBundle =&gt; BeforeInstall =&gt; ApplicationStart =&gt; ValidateService</strong></p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line.</p>\n\n<p>Please review the correct order of lifecycle events:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>BeforeInstall =&gt; ApplicationStart =&gt; DownloadBundle =&gt; ValidateService</strong></p>\n\n<p><strong>ValidateService =&gt; BeforeInstall =&gt;DownloadBundle =&gt; ApplicationStart</strong></p>\n\n<p><strong>BeforeInstall =&gt; ValidateService =&gt;DownloadBundle =&gt; ApplicationStart</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n",
            "answers": [
                "<p>DownloadBundle =&gt; BeforeInstall =&gt; ApplicationStart =&gt; ValidateService</p>",
                "<p>BeforeInstall =&gt; ApplicationStart =&gt; DownloadBundle =&gt; ValidateService</p>",
                "<p>ValidateService =&gt; BeforeInstall =&gt;DownloadBundle =&gt; ApplicationStart</p>",
                "<p>BeforeInstall =&gt; ValidateService =&gt;DownloadBundle =&gt; ApplicationStart</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "A communication platform serves millions of customers and deploys features in a production environment on AWS via CodeDeploy. You are reviewing scripts for the deployment process located in the AppSec file.\n\nWhich of the following options lists the correct order of lifecycle events?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790172,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are assigned as the new project lead for a web application that processes orders for customers. You want to integrate event-driven processing anytime data is modified or deleted and use a serverless approach using AWS Lambda for processing stream events.</p>\n\n<p>Which of the following databases should you choose from?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified.</p>\n\n<p>DynamoDB Streams Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS</strong> - By itself, RDS cannot be used to stream events like DynamoDB, so this option is ruled out. However, you can use Amazon Kinesis for streaming data from RDS.</p>\n\n<p>Please refer to this excellent blog for more details on using Kinesis for streaming data from RDS:\n<a href=\"https://aws.amazon.com/blogs/database/streaming-changes-in-a-database-with-amazon-kinesis/\">https://aws.amazon.com/blogs/database/streaming-changes-in-a-database-with-amazon-kinesis/</a></p>\n\n<p><strong>ElastiCache</strong> - ElastiCache works as an in-memory data store and cache, it cannot be used to stream data like DynamoDB.</p>\n\n<p><strong>Kinesis</strong> - Kinesis is not a database, so this option is ruled out.</p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.</p>\n\n<p>How Kinesis Data Streams Work\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
            "answers": [
                "<p>DynamoDB</p>",
                "<p>RDS</p>",
                "<p>ElastiCache</p>",
                "<p>Kinesis</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are assigned as the new project lead for a web application that processes orders for customers. You want to integrate event-driven processing anytime data is modified or deleted and use a serverless approach using AWS Lambda for processing stream events.\n\nWhich of the following databases should you choose from?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790148,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An IT company is using AWS CloudFormation to manage its IT infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest to provide this information to another stack?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use 'Export' field in the Output section of the stack's template</strong></p>\n\n<p>To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.</p>\n\n<p>To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use 'Expose' field in the Output section of the stack's template</strong> - 'Expose' is a made-up option, and only given as a distractor.</p>\n\n<p><strong>Use Fn::ImportValue</strong> - To import the values exported by another stack, we use the Fn::ImportValue function in the template for the other stacks. This function is not useful for the current scenario.</p>\n\n<p><strong>Use Fn::Transform</strong> - The intrinsic function Fn::Transform specifies a macro to perform custom processing on part of a stack template. Macros enable you to perform custom processing on templates, from simple actions like find-and-replace operations to extensive transformations of entire templates. This function is not useful for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n",
            "answers": [
                "<p>Use 'Expose' field in the Output section of the stack's template</p>",
                "<p>Use 'Export' field in the Output section of the stack's template</p>",
                "<p>Use Fn::ImportValue</p>",
                "<p>Use Fn::Transform</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "An IT company is using AWS CloudFormation to manage its IT infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack.\n\nAs a Developer Associate, which of the following options would you suggest to provide this information to another stack?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790116,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company manages hundreds of EC2 instances running on Linux OS. The instances are configured in several Availability Zones in the eu-west-3 region. Your manager has requested to collect system memory metrics on all EC2 instances using a script.</p>\n\n<p>Which of the following solutions will help you collect this data?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Use a cron job on the instances that pushes the EC2 RAM statistics as a Custom metric into CloudWatch\"</p>\n\n<p>The Amazon CloudWatch Monitoring Scripts for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances demonstrate how to produce and consume Amazon CloudWatch custom metrics. These Perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a Linux instance. You can set a cron schedule for metrics reported to CloudWatch and report memory utilization to CloudWatch every x minutes.</p>\n\n<p>Incorrect options:</p>\n\n<p>\"Extract RAM statistics using the instance metadata\" - Instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, hostname, events, and security groups. The instance metadata can only provide the ID of the RAM disk specified at launch time. So this option is incorrect.</p>\n\n<p>\"Extract RAM statistics from the standard CloudWatch metrics for EC2 instances\" - Amazon EC2 sends metrics to Amazon CloudWatch. By default, each data point covers the 5 minutes that follow the start time of activity for the instance. If you've enabled detailed monitoring, each data point covers the next minute of activity from the start time. The standard CloudWatch metrics don't have any metrics for memory utilization details.</p>\n\n<p>\"Extract RAM statistics using X-Ray\" - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>X-Ray cannot be used to extract RAM statistics for EC2 instances.</p>\n\n<p>For more information visit https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</p>\n",
            "answers": [
                "<p>Use a cron job on the instances that pushes the EC2 RAM statistics as a Custom metric into CloudWatch</p>",
                "<p>Extract RAM statistics using the instance metadata</p>",
                "<p>Extract RAM statistics from the standard CloudWatch metrics for EC2 instances</p>",
                "<p>Extract RAM statistics using X-Ray</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your company manages hundreds of EC2 instances running on Linux OS. The instances are configured in several Availability Zones in the eu-west-3 region. Your manager has requested to collect system memory metrics on all EC2 instances using a script.\n\nWhich of the following solutions will help you collect this data?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790192,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An organization uses Alexa as its intelligent assistant to improve productivity throughout the organization. A group of developers manages custom Alexa Skills written in Node.Js to control conference-room equipment settings and start meetings using voice activation. The manager has requested developers that all functions code should be monitored for error rates with the possibility of creating alarms on top of them.</p>\n\n<p>Which of the following options should be chosen? (select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. You can correlate your metrics and logs to better understand the health and performance of your resources. You can also create alarms based on metric value thresholds you specify, or that can watch for anomalous metric behavior based on machine learning algorithms.</p>\n\n<p>How CloudWatch works:\n<img src=\"https://d1.awsstatic.com/product-marketing/cloudwatch/product-page-diagram_Cloudwatch_v4.55c15d1cc086395cbd5ad279a2f1fc37e8452e77.png\">\nvia - <a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><strong>CloudWatch Metrics</strong></p>\n\n<p>Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real-time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. Metric data is kept for 15 months, enabling you to view both up-to-the-minute data and historical data.</p>\n\n<p>CloudWatch retains metric data as follows:</p>\n\n<p>Data points with a period of less than 60 seconds are available for 3 hours. These data points are high-resolution custom metrics.\nData points with a period of 60 seconds (1 minute) are available for 15 days\nData points with a period of 300 seconds (5 minute) are available for 63 days\nData points with a period of 3600 seconds (1 hour) are available for 455 days (15 months)</p>\n\n<p><strong>CloudWatch Alarms</strong></p>\n\n<p>You can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.</p>\n\n<p>CloudWatch alarms send notifications or automatically make changes to the resources you are monitoring based on rules that you define. Alarms work together with CloudWatch Metrics.</p>\n\n<p>A metric alarm has the following possible states:</p>\n\n<p>OK – The metric or expression is within the defined threshold.</p>\n\n<p>ALARM – The metric or expression is outside of the defined threshold.</p>\n\n<p>INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>X-Ray cannot be used to capture metrics and set up alarms as per the given use-case, so this option is incorrect.</p>\n\n<p><strong>CloudTrail</strong> - CloudWatch is a monitoring service whereas CloudTrail is more of an audit service where you can find API calls made on services and by whom.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><strong>Systems Manager</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager cannot be used to capture metrics and set up alarms as per the given use-case, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n",
            "answers": [
                "<p>CloudWatch Metrics</p>",
                "<p>X-Ray</p>",
                "<p>CloudWatch Alarms</p>",
                "<p>CloudTrail</p>",
                "<p>SSM</p>"
            ]
        },
        "correct_response": [
            "a",
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An organization uses Alexa as its intelligent assistant to improve productivity throughout the organization. A group of developers manages custom Alexa Skills written in Node.Js to control conference-room equipment settings and start meetings using voice activation. The manager has requested developers that all functions code should be monitored for error rates with the possibility of creating alarms on top of them.\n\nWhich of the following options should be chosen? (select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790162,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are working on a project that has over 100 dependencies. Every time your AWS CodeBuild runs a build step it has to resolve Java dependencies from external Ivy repositories which take a long time. Your manager wants to speed this process up in AWS CodeBuild.</p>\n\n<p>Which of the following will help you do this with minimal effort?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Cache dependencies on S3</strong></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your build servers.</p>\n\n<p>Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies in S3.</p>\n\n<p>Best Practices for Caching Dependencies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Reduce the number of dependencies</strong> - This is ideal but sometimes you may not have control over this as your application needs those dependencies, so this option is ruled out.</p>\n\n<p><strong>Ship all the dependencies as part of the source code</strong> - This is not a good practice as doing this will increase your build time. If your dependencies are not changing then its best to cache them.</p>\n\n<p><strong>Use Instance Store type of EC2 instances to facilitate internal dependency cache</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>Instance Store cannot be used to facilitate the internal dependency cache for the code build process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n",
            "answers": [
                "<p>Use Instance Store type of EC2 instances to facilitate internal dependency cache</p>",
                "<p>Reduce the number of dependencies</p>",
                "<p>Ship all the dependencies as part of the source code</p>",
                "<p>Cache dependencies on S3</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "You are working on a project that has over 100 dependencies. Every time your AWS CodeBuild runs a build step it has to resolve Java dependencies from external Ivy repositories which take a long time. Your manager wants to speed this process up in AWS CodeBuild.\n\nWhich of the following will help you do this with minimal effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790142,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A developer is migrating an on-premises application to AWS Cloud. The application currently processes user uploads and uploads them to a local directory on the server. All such file uploads must be saved and then made available to all instances in an Auto Scaling group.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend for this use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 and make code changes in the application so all uploads are put on S3</strong></p>\n\n<p>Amazon S3 is an object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.</p>\n\n<p>Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Using this web service, you can easily build applications that make use of Internet storage.</p>\n\n<p>You can use S3 PutObject API from the application to upload the objects in a single bucket, which is then accessible from all instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances</strong> - Using EBS to share data between instances is not possible because EBS volume is tied to an instance by definition. Creating a snapshot would only manage to move the stale data into the new instances.</p>\n\n<p><strong>Use Instance Store type of EC2 instances and share the files via file synchronization software</strong></p>\n\n<p><strong>Use Amazon EBS as the storage volume and share the files via file synchronization software</strong></p>\n\n<p>Technically you could use file synchronization software on EC2 instances with EBS or Instance Store type, but that involves a lot of development effort and still would not be as production-ready as just using S3. So both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
            "answers": [
                "<p>Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances</p>",
                "<p>Use Amazon S3 and make code changes in the application so all uploads are put on S3</p>",
                "<p>Use Instance Store type of EC2 instances and share the files via file synchronization software</p>",
                "<p>Use Amazon EBS as the storage volume and share the files via file synchronization software</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer is migrating an on-premises application to AWS Cloud. The application currently processes user uploads and uploads them to a local directory on the server. All such file uploads must be saved and then made available to all instances in an Auto Scaling group.\n\nAs a Developer Associate, which of the following options would you recommend for this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790126,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An IT company has a web application running on Amazon EC2 instances that needs read-only access to an Amazon DynamoDB table.</p>\n\n<p>As a Developer Associate, what is the best-practice solution you would recommend to accomplish this task?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile</strong></p>\n\n<p>As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role.</p>\n\n<p>So for the given use-case, you should create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new IAM user with access keys. Attach an inline policy to the IAM user with read-only access to DynamoDB. Place the keys in the code. For security, redeploy the code whenever the keys rotate</strong></p>\n\n<p><strong>Create an IAM user with Administrator access and configure AWS credentials for this user on the given EC2 instance</strong></p>\n\n<p><strong>Run application code with AWS account root user credentials to ensure full access to all AWS services</strong></p>\n\n<p>As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application. The security implications are even higher when you use an IAM user with admin privileges or use the AWS account root user. So all three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n",
            "answers": [
                "<p>Create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile</p>",
                "<p>Create a new IAM user with access keys. Attach an inline policy to the IAM user with read-only access to DynamoDB. Place the keys in the code. For security, redeploy the code whenever the keys rotate</p>",
                "<p>Create an IAM user with Administrator access and configure AWS credentials for this user on the given EC2 instance</p>",
                "<p>Run application code with AWS account root user credentials to ensure full access to all AWS services</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "An IT company has a web application running on Amazon EC2 instances that needs read-only access to an Amazon DynamoDB table.\n\nAs a Developer Associate, what is the best-practice solution you would recommend to accomplish this task?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790128,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A cybersecurity company is publishing critical log data to a log group in Amazon CloudWatch Logs, which was created 3 months ago. The company must encrypt the log data using an AWS KMS customer master key (CMK), so any future data can be encrypted to meet the company’s security guidelines.</p>\n\n<p>How can the company address this use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS CLI <code>associate-kms-key</code> command and specify the KMS key ARN</strong></p>\n\n<p>Log group data is always encrypted in CloudWatch Logs. You can optionally use AWS AWS Key Management Service for this encryption. If you do, the encryption is done using an AWS KMS (AWS KMS) customer master key (CMK). Encryption using AWS KMS is enabled at the log group level, by associating a CMK with a log group, either when you create the log group or after it exists.</p>\n\n<p>After you associate a CMK with a log group, all newly ingested data for the log group is encrypted using the CMK. This data is stored in an encrypted format throughout its retention period. CloudWatch Logs decrypts this data whenever it is requested. CloudWatch Logs must have permissions for the CMK whenever encrypted data is requested.</p>\n\n<p>To associate the CMK with an existing log group, you can use the <code>associate-kms-key</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the encrypt feature on the log group via the CloudWatch Logs console</strong> - CloudWatch Logs console does not have an option to enable encryption for a log group.</p>\n\n<p><strong>Use the AWS CLI <code>describe-log-groups</code> command and specify the KMS key ARN</strong> - You can use the <code>describe-log-groups</code> command to find whether a log group already has a CMK associated with it.</p>\n\n<p><strong>Use the AWS CLI <code>create-log-group</code> command and specify the KMS key ARN</strong> - You can use the <code>create-log-group</code> command to associate the CMK with a log group when you create it.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html</a></p>\n",
            "answers": [
                "<p>Enable the encrypt feature on the log group via the CloudWatch Logs console</p>",
                "<p>Use the AWS CLI <code>describe-log-groups</code> command and specify the KMS key ARN</p>",
                "<p>Use the AWS CLI <code>create-log-group</code> command and specify the KMS key ARN</p>",
                "<p>Use the AWS CLI <code>associate-kms-key</code> command and specify the KMS key ARN</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A cybersecurity company is publishing critical log data to a log group in Amazon CloudWatch Logs, which was created 3 months ago. The company must encrypt the log data using an AWS KMS customer master key (CMK), so any future data can be encrypted to meet the company’s security guidelines.\n\nHow can the company address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790132,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A development team uses the AWS SDK for Java to maintain an application that stores data in AWS DynamoDB. The application makes use of <code>Scan</code> operations to return several items from a 25 GB table. There is no possibility of creating indexes to retrieve these items predictably. Developers are trying to get these specific rows from DynamoDB as fast as possible.</p>\n\n<p>Which of the following options can be used to improve the performance of the Scan operation?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use parallel scans</strong></p>\n\n<p>By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel.</p>\n\n<p>To make use of a parallel Scan feature, you will need to run multiple worker threads or processes in parallel. Each worker will be able to scan a separate partition of a table concurrently with the other workers.</p>\n\n<p>How DynamoDB parallel Scan works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a ProjectionExpression</strong> - A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated</p>\n\n<p><strong>Use a FilterExpression</strong> - If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded.</p>\n\n<p>A filter expression is applied after a Scan finishes, but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present.</p>\n\n<p><strong>Use a Query</strong> - This could work if we were able to create an index, but the question says: \"There is no possibility of creating indexes to retrieve these items predictably\". As such, we cannot use a Query.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p>\n",
            "answers": [
                "<p>Use a Query</p>",
                "<p>Use a ProjectionExpression</p>",
                "<p>Use a FilterExpression</p>",
                "<p>Use parallel scans</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A development team uses the AWS SDK for Java to maintain an application that stores data in AWS DynamoDB. The application makes use of Scan operations to return several items from a 25 GB table. There is no possibility of creating indexes to retrieve these items predictably. Developers are trying to get these specific rows from DynamoDB as fast as possible.\n\nWhich of the following options can be used to improve the performance of the Scan operation?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790112,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A media company is building an application that needs to store video files in Amazon S3. Management requires that the files be encrypted before they are sent to Amazon S3 for storage. The encryption keys need to be managed by an in-house security team but the key itself is stored on AWS.</p>\n\n<p>Which solution should the company use to meet these requirements?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use client-side encryption with an AWS KMS managed customer master key (CMK)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>As the company wants to make sure that the files are encrypted before they are sent to Amazon S3, therefore you should use client-side encryption.</p>\n\n<p>To enable client-side encryption, you have the following options:</p>\n\n<p>Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p>\n\n<p>Use a master key you store within your application.</p>\n\n<p>As the use-case mentions that the encryption keys need to be managed by an in-house security team but the key itself should be stored on AWS, therefore you must use the customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use client-side encryption with Amazon S3 managed key</strong> - As mentioned earlier in the explanation, to meet the requirements of the given use-case, the company must use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p>\n\n<p>Also, there is no such thing as \"client-side encryption with Amazon S3 managed key\" as explained above.</p>\n\n<p><strong>Use server-side encryption with a customer-provided encryption key (SSE-C)</strong></p>\n\n<p><strong>Use server-side encryption with a client-side master key</strong></p>\n\n<p>Both these options are incorrect as the use-case mandates client-side encryption.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n",
            "answers": [
                "<p>Use server-side encryption with customer-provided encryption key (SSE-C)</p>",
                "<p>Use client-side encryption with an AWS KMS managed customer master key (CMK)</p>",
                "<p>Use server-side encryption with a client-side master key</p>",
                "<p>Use client-side encryption with Amazon S3 managed key</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A media company is building an application that needs to store video files in Amazon S3. Management requires that the files be encrypted before they are sent to Amazon S3 for storage. The encryption keys need to be managed by an in-house security team but the key itself is stored on AWS.\n\nWhich solution should the company use to meet these requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790136,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are a manager for a tech company that has just hired a team of developers to work on the company's AWS infrastructure. All the developers are reporting to you that when using the AWS CLI to execute commands it fails with the following exception: You are not authorized to perform this operation. Encoded authorization failure message: 6h34GtpmGjJJUm946eDVBfzWQJk6z5GePbbGDs9Z2T8xZj9EZtEduSnTbmrR7pMqpJrVYJCew2m8YBZQf4HRWEtrpncANrZMsnzk.</p>\n\n<p>Which of the following actions will help developers decode the message?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS STS decode-authorization-message</strong></p>\n\n<p>Use decode-authorization-message to decode additional information about the authorization status of a request from an encoded message returned in response to an AWS request. If a user is not authorized to perform an action that was requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the operation should not see. To decode an authorization status message, a user must be granted permissions via an IAM policy to request the DecodeAuthorizationMessage (sts:DecodeAuthorizationMessage) action.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS IAM decode-authorization-message</strong> - The IAM service does not have this command, as it's a made-up option.</p>\n\n<p><strong>Use KMS decode-authorization-message</strong> - The KMS service does not have this command, as it's a made-up option.</p>\n\n<p><strong>AWS Cognito Decoder</strong> - The Cognito service does not have this command, as it's a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html\">https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html</a></p>\n",
            "answers": [
                "<p>AWS IAM decode-authorization-message</p>",
                "<p>AWS STS decode-authorization-message</p>",
                "<p>Use KMS decode-authorization-message</p>",
                "<p>AWS Cognito Decoder</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "You are a manager for a tech company that has just hired a team of developers to work on the company's AWS infrastructure. All the developers are reporting to you that when using the AWS CLI to execute commands it fails with the following exception: You are not authorized to perform this operation. Encoded authorization failure message: 6h34GtpmGjJJUm946eDVBfzWQJk6z5GePbbGDs9Z2T8xZj9EZtEduSnTbmrR7pMqpJrVYJCew2m8YBZQf4HRWEtrpncANrZMsnzk.\n\nWhich of the following actions will help developers decode the message?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790200,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>After reviewing your monthly AWS bill you notice that the cost of using Amazon SQS has gone up substantially after creating new queues; however, you know that your queue clients do not have a lot of traffic and are receiving empty responses.</p>\n\n<p>Which of the following actions should you take?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use LongPolling</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the differences between Short Polling vs Long Polling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q14-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - Because there is no guarantee that a consumer received a message, the consumer must delete it. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout. Visibility timeout will not help with cost reduction.</p>\n\n<p><strong>Use a FIFO queue</strong> - FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced. FIFO queues will not help with cost reduction. In fact, they are costlier than standard queues.</p>\n\n<p><strong>Decrease DelaySeconds</strong> - This is similar to VisibilityTimeout. The difference is that a message is hidden when it is first added to a queue for DelaySeconds, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. DelaySeconds will not help with cost reduction.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n",
            "answers": [
                "<p>Use a FIFO queue</p>",
                "<p>Increase the VisibilityTimeout</p>",
                "<p>Use LongPolling</p>",
                "<p>Decrease DelaySeconds</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "After reviewing your monthly AWS bill you notice that the cost of using Amazon SQS has gone up substantially after creating new queues; however, you know that your queue clients do not have a lot of traffic and are receiving empty responses.\n\nWhich of the following actions should you take?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790150,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>The development team at a company wants to insert vendor records into an Amazon DynamoDB table as soon as the vendor uploads a new file into an Amazon S3 bucket.</p>\n\n<p>As a Developer Associate, which set of steps would you recommend to achieve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an S3 event to invoke a Lambda function that inserts records into DynamoDB</strong></p>\n\n<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.</p>\n\n<p>Amazon S3 APIs such as PUT, POST, and COPY can create an object. Using these event types, you can enable notification when an object is created using a specific API, or you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object.</p>\n\n<p>For the given use-case, you would create an S3 event notification that triggers a Lambda function whenever we have a PUT object operation in the S3 bucket. The Lambda function in turn would execute custom code to inserts records into DynamoDB.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB</strong> - This is not efficient because there may not be any unprocessed file in the S3 bucket when the cron triggers the Lambda on schedule. So this is not the correct option.</p>\n\n<p><strong>Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB</strong> - The CloudWatch event cannot directly insert records into DynamoDB as it's not a supported target type. The CloudWatch event needs to use something like a Lambda function to insert the records into DynamoDB.</p>\n\n<p><strong>Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB</strong> - This is not efficient because there may not be any unprocessed file in the S3 bucket when the Lambda function polls the S3 bucket at a given time interval. So this is not the correct option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
            "answers": [
                "<p>Create an S3 event to invoke a Lambda function that inserts records into DynamoDB</p>",
                "<p>Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB</p>",
                "<p>Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB</p>",
                "<p>Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a company wants to insert vendor records into an Amazon DynamoDB table as soon as the vendor uploads a new file into an Amazon S3 bucket.\n\nAs a Developer Associate, which set of steps would you recommend to achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790152,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An IT company uses a blue/green deployment policy to provision new Amazon EC2 instances in an Auto Scaling group behind a new Application Load Balancer for each new application version. The current set up requires the users to log in after every new deployment.</p>\n\n<p>As a Developer Associate, what advice would you give to the company for resolving this issue?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to maintain user sessions</strong></p>\n\n<p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p>To address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached via ElastiCache.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/caching-session-management-diagram-v2.c6856e6de83c4222dbc4853d9ff873f5542a86d8.PNG\">\nvia - <a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use rolling updates instead of a blue/green deployment</strong> - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances.</p>\n\n<p>This means that some of the users can experience session disruptions when the instances maintaining the sessions were detached as part of the given batch. So this option is incorrect.</p>\n\n<p><strong>Enable sticky sessions in the Application Load Balancer</strong> - As the Application Load Balancer itself is replaced on each new deployment, so maintaining sticky sessions via the Application Load Balancer will not work.</p>\n\n<p><strong>Use multicast to replicate session information</strong> - This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method</a></p>\n",
            "answers": [
                "<p>Use ElastiCache to maintain user sessions</p>",
                "<p>Use rolling updates instead of a blue/green deployment</p>",
                "<p>Enable sticky sessions in the Application Load Balancer</p>",
                "<p>Use multicast to replicate session information</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "An IT company uses a blue/green deployment policy to provision new Amazon EC2 instances in an Auto Scaling group behind a new Application Load Balancer for each new application version. The current set up requires the users to log in after every new deployment.\n\nAs a Developer Associate, what advice would you give to the company for resolving this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790114,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>You have an Amazon Kinesis Data Stream with 10 shards, and from the metrics, you are well below the throughput utilization of 10 MB per second to send data. You send 3 MB per second of data and yet you are receiving ProvisionedThroughputExceededException errors frequently.</p>\n\n<p>What is the likely cause of this?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The partition key that you have selected isn't distributed enough</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.</p>\n\n<p>A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>\n\n<p>The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>For the given use-case, as the partition key is not distributed enough, all the data is getting skewed at a few specific shards and not leveraging the entire cluster of shards.</p>\n\n<p>You can also use metrics to determine which are your \"hot\" or \"cold\" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Metrics are slow to update</strong> - Metrics are a CloudWatch concept. This option has been added as a distractor.</p>\n\n<p><strong>You have too many shards</strong> - Too many shards is not the issue as you would see a LimitExceededException in that case.</p>\n\n<p><strong>The data retention period is too long</strong> - Your streaming data is retained for up to 365 days. The data retention period is not an issue causing this error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html</a></p>\n",
            "answers": [
                "<p>Metrics are slow to update</p>",
                "<p>The partition key that you have selected isn't distributed enough</p>",
                "<p>You have too many shards</p>",
                "<p>The data retention period is too long</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have an Amazon Kinesis Data Stream with 10 shards, and from the metrics, you are well below the throughput utilization of 10 MB per second to send data. You send 3 MB per second of data and yet you are receiving ProvisionedThroughputExceededException errors frequently.\n\nWhat is the likely cause of this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790102,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An e-commerce company has implemented AWS CodeDeploy as part of its AWS cloud CI/CD strategy. The company has configured automatic rollbacks while deploying a new version of its flagship application to Amazon EC2.</p>\n\n<p>What occurs if the deployment of the new version fails?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>A new deployment of the last known working version of the application is deployed with a new deployment ID</strong></p>\n\n<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment. These rolled-back deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment.</p>\n\n<p>To roll back an application to a previous revision, you just need to deploy that revision. AWS CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment, so there is no difference between redeploy and rollback. However, you need to make sure that the previous revisions are available for rollback.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The last known working deployment is automatically restored using the snapshot stored in Amazon S3</strong> - CodeDeploy deployment does not have a snapshot stored on S3, so this option is incorrect.</p>\n\n<p><strong>AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production</strong> - The use-case does not talk about using CodePipeline, so this option just acts as a distractor.</p>\n\n<p><strong>CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment</strong> - The use-case does not talk about the blue/green deployment, so this option has just been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html</a></p>\n",
            "answers": [
                "<p>CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment</p>",
                "<p>AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production</p>",
                "<p>The last known working deployment is automatically restored using the snapshot stored in Amazon S3</p>",
                "<p>A new deployment of the last known working version of the application is deployed with a new deployment ID</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "An e-commerce company has implemented AWS CodeDeploy as part of its AWS cloud CI/CD strategy. The company has configured automatic rollbacks while deploying a new version of its flagship application to Amazon EC2.\n\nWhat occurs if the deployment of the new version fails?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790180,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A developer has pushed a Lambda function that pushes data into an RDS MySQL database with the following Python code:</p>\n\n<pre><code>def handler(event, context):\n    mysql = mysqlclient.connect()\n    data = event['data']\n    mysql.execute(f\"INSERT INTO foo (bar) VALUES (${data});\")\n    mysql.close()\n    return\n</code></pre>\n\n<p>On the first execution, the Lambda function takes 2 seconds to execute. On the second execution and all the subsequent ones, the Lambda function takes 1.9 seconds to execute.</p>\n\n<p>What can be done to improve the execution time of the Lambda function?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Move the database connection out of the handler</strong></p>\n\n<p>Here at every Lambda function execution, the database connection handler will be created, and then closed. These connections steps are expensive in terms of time, and thus should be moved out of the <code>handler</code> function so that they are kept in the function execution context, and re-used across function calls. This is what the function should look like in the end:</p>\n\n<pre><code>mysql = mysqlclient.connect()\n\ndef handler(event, context):\n    data = event['data']\n    mysql.execute(f\"INSERT INTO foo (bar) VALUES (${data});\")\n    return\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upgrade the MySQL instance type</strong> - The bottleneck here is the MySQL connection object, not the MySQL instance itself.</p>\n\n<p><strong>Change the runtime to Node.js</strong> - Re-writing the function in another runtime won't improve the performance.</p>\n\n<p><strong>Increase the Lambda function RAM</strong> - While this may help speed-up the Lambda function, as increasing the RAM also increases the CPU allocated to your function, it only makes sense if RAM or CPU is a critical factor in the Lambda function performance. Here, the connection object is at fault.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html\">https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html</a></p>\n",
            "answers": [
                "<p>Upgrade the MySQL instance type</p>",
                "<p>Change the runtime to Node.js</p>",
                "<p>Increase the Lambda function RAM</p>",
                "<p>Move the database connection out of the handler</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A developer has pushed a Lambda function that pushes data into an RDS MySQL database with the following Python code:\n\ndef handler(event, context):\n    mysql = mysqlclient.connect()\n    data = event['data']\n    mysql.execute(f\"INSERT INTO foo (bar) VALUES (${data});\")\n    mysql.close()\n    return\n\n\nOn the first execution, the Lambda function takes 2 seconds to execute. On the second execution and all the subsequent ones, the Lambda function takes 1.9 seconds to execute.\n\nWhat can be done to improve the execution time of the Lambda function?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790106,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A firm uses AWS DynamoDB to store information about people’s favorite sports teams and allow the information to be searchable from their home page. There is a daily requirement that all 10 million records in the table should be deleted then re-loaded at 2:00 AM each night.</p>\n\n<p>Which option is an efficient way to delete with minimal costs?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete then re-create the table</strong></p>\n\n<p>The DeleteTable operation deletes a table and all of its items. After a <code>DeleteTable</code> request, the specified table is in the <code>DELETING</code> state until DynamoDB completes the deletion.</p>\n\n<p><strong>Scan and call DeleteItem</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Scan and call BatchDeleteItem</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Call PurgeTable</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html\">https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html</a></p>\n",
            "answers": [
                "<p>Scan and call BatchDeleteItem</p>",
                "<p>Scan and call DeleteItem</p>",
                "<p>Delete then re-create the table</p>",
                "<p>Call PurgeTable</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A firm uses AWS DynamoDB to store information about people’s favorite sports teams and allow the information to be searchable from their home page. There is a daily requirement that all 10 million records in the table should be deleted then re-loaded at 2:00 AM each night.\n\nWhich option is an efficient way to delete with minimal costs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790194,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have a popular web application that accesses data stored in an Amazon Simple Storage Service (S3) bucket. Developers use the SDK to maintain the application and add new features. Security compliance requests that all new objects uploaded to S3 be encrypted using SSE-S3 at the time of upload. Which of the following headers must the developers add to their request?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'AES256'</strong></p>\n\n<p>Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n\n<p>SSE-S3 Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-S3'</strong> - SSE-S3 (Amazon S3-Managed Keys) is an option available but it's not a valid header value.</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-KMS'</strong> - SSE-KMS (AWS KMS-Managed Keys) is an option available but it's not a valid header value. A valid value would be 'aws:kms'</p>\n\n<p><strong>'x-amz-server-side-encryption': 'aws:kms'</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p>\n\n<p>This is a valid header value and you can use if you need more control over your keys like create, rotating, disabling them using AWS KMS. Otherwise, if you wish to let AWS S3 manage your keys just stick with SSE-S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
            "answers": [
                "<p>'x-amz-server-side-encryption': 'AES256'</p>",
                "<p>'x-amz-server-side-encryption': 'SSE-S3'</p>",
                "<p>'x-amz-server-side-encryption': 'SSE-KMS'</p>",
                "<p>'x-amz-server-side-encryption': 'aws:kms'</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "You have a popular web application that accesses data stored in an Amazon Simple Storage Service (S3) bucket. Developers use the SDK to maintain the application and add new features. Security compliance requests that all new objects uploaded to S3 be encrypted using SSE-S3 at the time of upload. Which of the following headers must the developers add to their request?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790122,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your development team uses the AWS SDK for Java on a web application that uploads files to several Amazon Simple Storage Service (S3) buckets using the SSE-KMS encryption mechanism. Developers are reporting that they are receiving permission errors when trying to push their objects over HTTP. Which of the following headers should they include in their request?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'aws:kms'</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p>\n\n<p>If the request does not include the x-amz-server-side-encryption header, then the request is denied.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-S3'</strong> - This is an invalid header value. The correct value is 'x-amz-server-side-encryption': 'AES256'. This refers to Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3).</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-KMS'</strong> - Invalid header value. SSE-KMS is an encryption option.</p>\n\n<p><strong>'x-amz-server-side-encryption': 'AES256'</strong> - This is the correct header value if you are using SSE-S3 server-side encryption.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n",
            "answers": [
                "<p>'x-amz-server-side-encryption': 'SSE-KMS'</p>",
                "<p>'x-amz-server-side-encryption': 'SSE-S3'</p>",
                "<p>'x-amz-server-side-encryption': 'aws:kms'</p>",
                "<p>'x-amz-server-side-encryption': 'AES256'</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "Your development team uses the AWS SDK for Java on a web application that uploads files to several Amazon Simple Storage Service (S3) buckets using the SSE-KMS encryption mechanism. Developers are reporting that they are receiving permission errors when trying to push their objects over HTTP. Which of the following headers should they include in their request?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790096,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A data analytics company with its IT infrastructure on the AWS Cloud wants to build and deploy its flagship application as soon as there are any changes to the source code.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest to trigger the deployment? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository</strong></p>\n\n<p><strong>Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.</p>\n\n<p>How CodePipeline Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p>Using change detection methods that you specify, you can make your pipeline start when a change is made to a repository. You can also make your pipeline start on a schedule.</p>\n\n<p>When you use the console to create a pipeline that has a CodeCommit source repository or S3 source bucket, CodePipeline creates an Amazon CloudWatch Events rule that starts your pipeline when the source changes. This is the recommended change detection method.</p>\n\n<p>If you use the AWS CLI to create the pipeline, the change detection method defaults to starting the pipeline by periodically checking the source (CodeCommit, Amazon S3, and GitHub source providers only). AWS recommends that you disable periodic checks and create the rule manually.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated</strong></p>\n\n<p><strong>Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code</strong></p>\n\n<p>Both EFS and EBS are not supported as valid source providers for CodePipeline to check for any changes to the source code, hence these two options are incorrect.</p>\n\n<p><strong>Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes</strong> - As mentioned in the explanation above, although you could have the change detection method start the pipeline by periodically checking the S3 bucket, but this method is inefficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html</a></p>\n",
            "answers": [
                "<p>Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated</p>",
                "<p>Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository</p>",
                "<p>Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated</p>",
                "<p>Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes</p>",
                "<p>Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code</p>"
            ]
        },
        "correct_response": [
            "b",
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A data analytics company with its IT infrastructure on the AWS Cloud wants to build and deploy its flagship application as soon as there are any changes to the source code.\n\nAs a Developer Associate, which of the following options would you suggest to trigger the deployment? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790198,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward.</p>\n\n<p>Which of the following actions should you take?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an LSI</strong></p>\n\n<p>LSI stands for Local Secondary Index. Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Call Scan</strong> - Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue Scan requests again.</p>\n\n<p><strong>Create a GSI</strong> - GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.</p>\n\n<p><strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n",
            "answers": [
                "<p>Create a GSI</p>",
                "<p>Call Scan</p>",
                "<p>Create a LSI</p>",
                "<p>Migrate away from DynamoDB</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward.\n\nWhich of the following actions should you take?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790130,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>As part of internal regulations, you must ensure that all communications to Amazon S3 are encrypted.</p>\n\n<p>For which of the following encryption mechanisms will a request get rejected if the connection is not using HTTPS?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>Server-side encryption is about protecting data at rest. Server-side encryption encrypts only the object data, not object metadata. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys.</p>\n\n<p>When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches and then decrypts the object before returning the object data to you.</p>\n\n<p>Amazon S3 will reject any requests made over HTTP when using SSE-C. For security considerations, AWS recommends that you consider any key you send erroneously using HTTP to be compromised.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - It is not mandatory to use HTTPS.</p>\n\n<p><strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before sending it to Amazon S3. It is not mandatory to use HTTPS for this.</p>\n\n<p><strong>SSE-S3</strong> - It is not mandatory to use HTTPS. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p>\n",
            "answers": [
                "<p>SSE-KMS</p>",
                "<p>SSE-C</p>",
                "<p>Client Side Encryption</p>",
                "<p>SSE-S3</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "As part of internal regulations, you must ensure that all communications to Amazon S3 are encrypted.\n\nFor which of the following encryption mechanisms will a request get rejected if the connection is not using HTTPS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790212,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have moved your on-premise infrastructure to AWS and are in the process of configuring an AWS Elastic Beanstalk deployment environment for production, development, and testing. You have configured your production environment to use a rolling deployment to prevent your application from becoming unavailable to users. For the development and testing environment, you would like to deploy quickly and are not concerned about downtime.</p>\n\n<p>Which of the following deployment policies meet your needs?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>All at once</strong></p>\n\n<p>This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Rolling</strong> - With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.</p>\n\n<p><strong>Rolling with additional batches</strong> - With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.</p>\n\n<p><strong>Immutable</strong> - A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
            "answers": [
                "<p>Immutable</p>",
                "<p>Rolling</p>",
                "<p>Rolling with additional batches</p>",
                "<p>All at once</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "You have moved your on-premise infrastructure to AWS and are in the process of configuring an AWS Elastic Beanstalk deployment environment for production, development, and testing. You have configured your production environment to use a rolling deployment to prevent your application from becoming unavailable to users. For the development and testing environment, you would like to deploy quickly and are not concerned about downtime.\n\nWhich of the following deployment policies meet your needs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790208,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have uploaded a zip file to AWS Lambda that contains code files written in Node.Js. When your function is executed you receive the following output, 'Error: Memory Size: 10,240 MB Max Memory Used'.</p>\n\n<p>Which of the following explains the problem?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Your Lambda function ran out of RAM</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>The maximum amount of memory available to the Lambda function at runtime is 10,240 MB. Your Lambda function was deployed with 10,240 MB of RAM, but it seems your code requested or used more than that, so the Lambda function failed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your zip file is corrupt</strong> - A memory size error states that Lambda was able to extract so the file is not corrupt</p>\n\n<p><strong>The uncompressed zip file exceeds AWS Lambda limits</strong> - This is not correct as your function was able to execute.</p>\n\n<p><strong>You have uploaded a zip file larger than 50 MB to AWS Lambda</strong> -  This is not correct as your lambda function was able to execute</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
            "answers": [
                "<p>The uncompressed zip file exceeds AWS Lambda limits</p>",
                "<p>You have uploaded a zip file larger than 50 MB to AWS Lambda</p>",
                "<p>Your Lambda function ran out of RAM</p>",
                "<p>Your zip file is corrupt</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have uploaded a zip file to AWS Lambda that contains code files written in Node.Js. When your function is executed you receive the following output, 'Error: Memory Size: 10,240 MB Max Memory Used'.\n\nWhich of the following explains the problem?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790196,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An organization recently began using AWS CodeCommit for its source control service. A compliance security team visiting the organization was auditing the software development process and noticed developers making many git push commands within their development machines. The compliance team requires that encryption be used for this activity.</p>\n\n<p>How can the organization ensure source code is encrypted in transit and at rest?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Repositories are automatically encrypted at rest</strong></p>\n\n<p>Data in AWS CodeCommit repositories is encrypted in transit and at rest. When data is pushed into an AWS CodeCommit repository (for example, by calling git push), AWS CodeCommit encrypts the received data as it is stored in the repository.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable KMS encryption</strong> - You don't have to. The first time you create an AWS CodeCommit repository in a new region in your AWS account, CodeCommit creates an AWS-managed key in that same region in AWS Key Management Service (AWS KMS) that is used only by CodeCommit.</p>\n\n<p><strong>Use AWS Lambda as a hook to encrypt the pushed code</strong> - This is not needed as CodeCommit handles it for you.</p>\n\n<p><strong>Use a git command line hook to encrypt the code client-side</strong> - This is not needed as CodeCommit handles it for you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</a></p>\n\n<p>For more information visit https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</p>\n",
            "answers": [
                "<p>Use a git command line hook to encrypt the code client side</p>",
                "<p>Enable KMS encryption</p>",
                "<p>Use AWS Lambda as a hook to encrypt the pushed code</p>",
                "<p>Repositories are automatically encrypted at rest</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "An organization recently began using AWS CodeCommit for its source control service. A compliance security team visiting the organization was auditing the software development process and noticed developers making many git push commands within their development machines. The compliance team requires that encryption be used for this activity.\n\nHow can the organization ensure source code is encrypted in transit and at rest?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790184,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A voting system hosted on-premise was recently migrated to AWS to lower cost, gain scalability, and to better serve thousands of concurrent users. When one of the AWS resource state changes, it generates an event and will need to trigger AWS Lambda. The AWS resource whose state changes and AWS Lambda does not have direct integration.</p>\n\n<p>Which of the following methods can be used to trigger AWS Lambda?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch Events Rules with AWS Lambda</strong></p>\n\n<p>You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>Schedule Expressions for CloudWatch Events Rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q29-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda Custom Sources</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Open a support ticket with AWS</strong> - You can, although the AWS support team will not add a custom configuration for you, they will step you through creating event rule with Lambda.</p>\n\n<p><strong>Cron jobs to trigger AWS Lambda to check the state of your service</strong> - You would need an additional server for your cron job instead you should consider using a cron expression with CloudWatch.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n",
            "answers": [
                "<p>Open a support ticket with AWS</p>",
                "<p>AWS Lambda Custom Sources</p>",
                "<p>CloudWatch Events Rules with AWS Lambda</p>",
                "<p>Cron jobs to trigger AWS Lambda to check the state of your service</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A voting system hosted on-premise was recently migrated to AWS to lower cost, gain scalability, and to better serve thousands of concurrent users. When one of the AWS resource state changes, it generates an event and will need to trigger AWS Lambda. The AWS resource whose state changes and AWS Lambda does not have direct integration.\n\nWhich of the following methods can be used to trigger AWS Lambda?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790160,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have a web application hosted on EC2 that makes GET and PUT requests for objects stored in Amazon Simple Storage Service (S3) using the SDK for PHP. As the security team completed the final review of your application for vulnerabilities, they noticed that your application uses hardcoded IAM access key and secret access key to gain access to AWS services. They recommend you leverage a more secure setup, which should use temporary credentials if possible.</p>\n\n<p>Which of the following options can be used to address the given use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an IAM Instance Role</strong></p>\n\n<p>An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. The AWS SDK will use the EC2 metadata service to obtain temporary credentials thanks to the IAM instance role. This is the most secure and common setup when deploying any kind of applications onto an EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use environment variables</strong> - This is another option if you configure AWS CLI on the EC2 instance. When configuring the AWS CLI you will set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. This practice may not be bad for one instance but once you start running more EC2 instances this is not a good practice because you may have to change credentials on each instance whereas an IAM Role gets temporary permissions.</p>\n\n<p><strong>Hardcode the credentials in the application code</strong> - It will work for sure, but it's not a good practice from a security point of view.</p>\n\n<p><strong>Use the SSM parameter store</strong> - With parameter store you can store data such as passwords. The problem is that you need the SDK to access parameter store and without credentials, you cannot use the SDK. Use parameter store for other uses such as database connection strings or other secret codes when you have already authenticated to AWS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p>\n",
            "answers": [
                "<p>Use the SSM parameter store</p>",
                "<p>Use environment variables</p>",
                "<p>Hardcode the credentials in the application code</p>",
                "<p>Use an IAM Instance Role</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "You have a web application hosted on EC2 that makes GET and PUT requests for objects stored in Amazon Simple Storage Service (S3) using the SDK for PHP. As the security team completed the final review of your application for vulnerabilities, they noticed that your application uses hardcoded IAM access key and secret access key to gain access to AWS services. They recommend you leverage a more secure setup, which should use temporary credentials if possible.\n\nWhich of the following options can be used to address the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790154,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A developer is configuring an Application Load Balancer (ALB) to direct traffic to the application's EC2 instances and Lambda functions.</p>\n\n<p>Which of the following characteristics of the ALB can be identified as correct? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>An ALB has three possible target types: Instance, IP and Lambda</strong></p>\n\n<p>When you create a target group, you specify its target type, which determines the type of target you specify when registering targets with this target group. After you create a target group, you cannot change its target type. The following are the possible target types:</p>\n\n<ol>\n<li><code>Instance</code> - The targets are specified by instance ID</li>\n<li><code>IP</code> - The targets are IP addresses</li>\n<li><code>Lambda</code> - The target is a Lambda function</li>\n</ol>\n\n<p><strong>You can not specify publicly routable IP addresses to an ALB</strong></p>\n\n<p>When the target type is IP, you can specify IP addresses from specific CIDR blocks only. You can't specify publicly routable IP addresses.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces</strong> - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance.</p>\n\n<p><strong>If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address</strong> - If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port.</p>\n\n<p><strong>An ALB has three possible target types: Hostname, IP and Lambda</strong> - This is incorrect, as described in the correct explanation above.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html</a></p>\n",
            "answers": [
                "<p>An ALB has three possible target types: Instance, IP and Lambda</p>",
                "<p>If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces</p>",
                "<p>If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address</p>",
                "<p>An ALB has three possible target types: Hostname, IP and Lambda</p>",
                "<p>You can not specify publicly routable IP addresses to an ALB</p>"
            ]
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer is configuring an Application Load Balancer (ALB) to direct traffic to the application's EC2 instances and Lambda functions.\n\nWhich of the following characteristics of the ALB can be identified as correct? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790188,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A company has several Linux-based EC2 instances that generate various log files which need to be analyzed for security and compliance purposes. The company wants to use Kinesis Data Streams (KDS) to analyze this log data.</p>\n\n<p>Which of the following is the most optimal way of sending log data from the EC2 instances to KDS?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Install and configure Kinesis Agent on each of the instances</strong></p>\n\n<p>Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. The agent continuously monitors a set of files and sends new data to your stream. The agent handles file rotation, checkpointing, and retry upon failures. It delivers all of your data in a reliable, timely, and simple manner. It also emits Amazon CloudWatch metrics to help you better monitor and troubleshoot the streaming process.</p>\n\n<p>You can install the agent on Linux-based server environments such as web servers, log servers, and database servers. After installing the agent, configure it by specifying the files to monitor and the stream for the data. After the agent is configured, it durably collects data from the files and reliably sends it to the stream.</p>\n\n<p>The agent can also pre-process the records parsed from monitored files before sending them to your stream. You can enable this feature by adding the dataProcessingOptions configuration setting to your file flow. One or more processing options can be added and they will be performed in the specified order.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run cron job on each of the instances to collect log data and send it to Kinesis Data Streams</strong> - This solution is possible, though not an optimal one. This solution requires writing custom code and tracking file/log changes, retry failures and so on. Kinesis Agent is built to handle all these requirements and integrates with Data Streams.</p>\n\n<p><strong>Install AWS SDK on each of the instances and configure it to send the necessary files to Kinesis Data Streams</strong> - Kinesis Data Streams APIs that are available in the AWS SDKs, helps you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records. You will need to write custom code to handle new data in the log files and send it over to your stream. Kinesis Agent does it easily, as it is designed to continuously monitor a set of files and send new data to your stream.</p>\n\n<p><strong>Use Kinesis Producer Library (KPL) to collect and ingest data from each EC2 instance</strong> - The KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. This is not optimal compared to Kinesis Agent which is designed to continuously monitor a set of files and send new data to your stream.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\">https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html</a></p>\n",
            "answers": [
                "<p>Use Kinesis Producer Library (KPL) to collect and ingest data from each EC2 instance</p>",
                "<p>Run cron job on each of the instances to collect log data and send it to Kinesis Data Streams</p>",
                "<p>Install and configure Kinesis Agent on each of the instances</p>",
                "<p>Install AWS SDK on each of the instances and configure it to send the necessary files to Kinesis Data Streams</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "A company has several Linux-based EC2 instances that generate various log files which need to be analyzed for security and compliance purposes. The company wants to use Kinesis Data Streams (KDS) to analyze this log data.\n\nWhich of the following is the most optimal way of sending log data from the EC2 instances to KDS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790118,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An Amazon Simple Queue Service (SQS) has to be configured between two AWS accounts for shared access to the queue. AWS account A has the SQS queue in its account and AWS account B has to be given access to this queue.</p>\n\n<p>Which of the following options need to be combined to allow this cross-account access? (Select three)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>The account A administrator creates an IAM role and attaches a permissions policy</strong></p>\n\n<p><strong>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role</strong></p>\n\n<p><strong>The account B administrator delegates the permission to assume the role to any users in account B</strong></p>\n\n<p>To grant cross-account permissions, you need to attach an identity-based permissions policy to an IAM role. For example, the AWS account A administrator can create a role to grant cross-account permissions to AWS account B as follows:</p>\n\n<ol>\n<li><p>The account A administrator creates an IAM role and attaches a permissions policy—that grants permissions on resources in account A—to the role.</p></li>\n<li><p>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.</p></li>\n<li><p>The account B administrator delegates the permission to assume the role to any users in account B. This allows users in account B to create or access queues in account A.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal</strong> - As mentioned above, the account A administrator needs to create an IAM role and then attach a permissions policy. So, this option is incorrect.</p>\n\n<p><strong>The account A administrator delegates the permission to assume the role to any users in account A</strong> - This is irrelevant, as users in account B need to be given access.</p>\n\n<p><strong>The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role</strong> - AWS service principal is given as principal in the trust policy when you need to grant the permission to assume the role to an AWS service. The given use case talks about giving permission to another account. So, service principal is not an option here.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html</a></p>\n",
            "answers": [
                "<p>The account A administrator creates an IAM role and attaches a permissions policy</p>",
                "<p>The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal</p>",
                "<p>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role</p>",
                "<p>The account A administrator delegates the permission to assume the role to any users in account A</p>",
                "<p>The account B administrator delegates the permission to assume the role to any users in account B</p>",
                "<p>The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role</p>"
            ]
        },
        "correct_response": [
            "a",
            "c",
            "e"
        ],
        "section": "Security",
        "question_plain": "An Amazon Simple Queue Service (SQS) has to be configured between two AWS accounts for shared access to the queue. AWS account A has the SQS queue in its account and AWS account B has to be given access to this queue.\n\nWhich of the following options need to be combined to allow this cross-account access? (Select three)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790202,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A user has an IAM policy as well as an Amazon SQS policy that apply to his account. The IAM policy grants his account permission for the <code>ReceiveMessage</code> action on <code>example_queue</code>, whereas the Amazon SQS policy gives his account permission for the <code>SendMessage</code> action on the same queue.</p>\n\n<p>Considering the permissions above, which of the following options are correct? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>The user can send a <code>ReceiveMessage</code> request to <code>example_queue</code>, the IAM policy allows this action</strong></p>\n\n<p>The user has both an IAM policy and an Amazon SQS policy that apply to his account. The IAM policy grants his account permission for the <code>ReceiveMessage</code> action on <code>example_queue</code>, whereas the Amazon SQS policy gives his account permission for the <code>SendMessage</code> action on the same queue.</p>\n\n<p>How IAM policy and SQS policy work in tandem:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n\n<p><strong>If you add a policy that denies the user access to all actions for the queue, the policy will override the other two policies and the user will not have access to <code>example_queue</code></strong></p>\n\n<p>To remove the user's full access to the queue, the easiest thing to do is to add a policy that denies him access to all actions for the queue. This policy overrides the other two because an explicit deny always overrides an allow.</p>\n\n<p>You can also add an additional statement to the Amazon SQS policy that denies the user any type of access to the queue. It has the same effect as adding an IAM policy that denies the user access to the queue.</p>\n\n<p>How IAM policy and SQS policy work in tandem:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the IAM policy will deny this action</strong> - If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the Amazon SQS policy allows the action. The IAM policy has no explicit deny on this action, so it plays no part.</p>\n\n<p><strong>Either of IAM policies or Amazon SQS policies should be used to grant permissions. Both cannot be used together</strong> - There are two ways to give your users permissions to your Amazon SQS resources: using the Amazon SQS policy system and using the IAM policy system. You can use one or the other, or both. For the most part, you can achieve the same result with either one.</p>\n\n<p><strong>Adding only an IAM policy to deny the user of all actions on the queue is not enough. The SQS policy should also explicitly deny all action</strong> - The user can be denied access using any one of the policies. Explicit deny in any policy will override all other allow actions defined using either of the policies.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n",
            "answers": [
                "<p>If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the IAM policy will deny this action</p>",
                "<p>Adding only an IAM policy to deny the user of all actions on the queue is not enough. The SQS policy should also explicitly deny all action</p>",
                "<p>The user can send a <code>ReceiveMessage</code> request to <code>example_queue</code>, the IAM policy allows this action</p>",
                "<p>Either of IAM policies or Amazon SQS policies should be used to grant permissions. Both cannot be used together</p>",
                "<p>If you add a policy that denies the user access to all actions for the queue, the policy will override the other two policies and the user will not have access to <code>example_queue</code></p>"
            ]
        },
        "correct_response": [
            "c",
            "e"
        ],
        "section": "Refactoring",
        "question_plain": "A user has an IAM policy as well as an Amazon SQS policy that apply to his account. The IAM policy grants his account permission for the ReceiveMessage action on example_queue, whereas the Amazon SQS policy gives his account permission for the SendMessage action on the same queue.\n\nConsidering the permissions above, which of the following options are correct? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790146,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>An order management system uses a cron job to poll for any new orders. Every time a new order is created, the cron job sends this order data as a message to the message queues to facilitate downstream order processing in a reliable way. To reduce costs and improve performance, the company wants to move this functionality to AWS cloud.</p>\n\n<p>Which of the following is the most optimal solution to meet this requirement?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications when an order is created. Configure different Amazon Simple Queue Service (SQS) queues to receive these messages for downstream processing</strong></p>\n\n<p>Amazon SNS works closely with Amazon Simple Queue Service (Amazon SQS). These services provide different benefits for developers. Amazon SNS allows applications to send time-critical messages to multiple subscribers through a “push” mechanism, eliminating the need to periodically check or “poll” for updates. Amazon SQS is a message queue service used by distributed applications to exchange messages through a polling model, and can be used to decouple sending and receiving components—without requiring each component to be concurrently available.</p>\n\n<p>Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also stored in an Amazon SQS queue for other applications to process at a later time.</p>\n\n<p>When you subscribe an Amazon SQS queue to an Amazon SNS topic, you can publish a message to the topic and Amazon SNS sends an Amazon SQS message to the subscribed queue. The Amazon SQS message contains the subject and message that were published to the topic along with metadata about the message in a JSON document.</p>\n\n<p>SNS-SQS fanout is the right solution for this use case.</p>\n\n<p>Sample SNS-SQS Fanout message:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure different Amazon Simple Queue Service (SQS) queues to poll for new orders</strong> - Amazon SQS cannot be used as a polling service, as messages need to be pushed to the queue, which are then handled by the queue consumers.</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications and use AWS Lambda functions to process the information received from SNS</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. When a message is published to an SNS topic that has a Lambda function subscribed to it, the Lambda function is invoked with the payload of the published message. For the given scenario, we need a service that can store the message data pushed by SNS, for further processing. AWS Lambda does not have capacity to store the message data. In case a Lambda function is unable to process a specific message, it will be left unprocessed. Hence this option is not correct.</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications to Kinesis Data Firehose delivery streams for processing the data for downstream applications</strong> - You can subscribe Amazon Kinesis Data Firehose delivery streams to SNS topics, which allows you to send notifications to additional storage and analytics endpoints. However, Kinesis is built for real-time processing of big data. Whereas, SQS is meant for decoupling dependent systems with easy methods to transmit data/messages. SQS also is a cheaper option when compared to Firehose. Therefore this option is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-firehose-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-firehose-as-subscriber.html</a></p>\n",
            "answers": [
                "<p>Configure different Amazon Simple Queue Service (SQS) queues to poll for new orders</p>",
                "<p>Use Amazon Simple Notification Service (SNS) to push notifications and use AWS Lambda functions to process the information received from SNS</p>",
                "<p>Use Amazon Simple Notification Service (SNS) to push notifications to Kinesis Data Firehose delivery streams for processing the data for downstream applications</p>",
                "<p>Use Amazon Simple Notification Service (SNS) to push notifications when an order is created. Configure different Amazon Simple Queue Service (SQS) queues to receive these messages for downstream processing</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "An order management system uses a cron job to poll for any new orders. Every time a new order is created, the cron job sends this order data as a message to the message queues to facilitate downstream order processing in a reliable way. To reduce costs and improve performance, the company wants to move this functionality to AWS cloud.\n\nWhich of the following is the most optimal solution to meet this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790210,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>You have configured a Network ACL and a Security Group for the load balancer and Amazon EC2 instances to allow inbound traffic on port 80. However, users are still unable to connect to your website after launch.</p>\n\n<p>Which additional configuration is required to make the website accessible to all users over the internet?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 1024 - 65535</strong></p>\n\n<p>A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p>\n\n<p>When you create a custom Network ACL and associate it with a subnet, by default, this custom Network ACL denies all inbound and outbound traffic until you add rules. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p>\n\n<p>The client that initiates the request chooses the ephemeral port range. The range varies depending on the client's operating system. Requests originating from Elastic Load Balancing use ports 1024-65535. List of ephemeral port ranges:</p>\n\n<ol>\n<li><p>Many Linux kernels (including the Amazon Linux kernel) use ports 32768-61000.</p></li>\n<li><p>Requests originating from Elastic Load Balancing use ports 1024-65535.</p></li>\n<li><p>Windows operating systems through Windows Server 2003 use ports 1025-5000.</p></li>\n<li><p>Windows Server 2008 and later versions use ports 49152-65535.</p></li>\n<li><p>A NAT gateway uses ports 1024-65535.</p></li>\n</ol>\n\n<p>AWS Lambda functions use ports 1024-65535.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 1025 - 5000</strong> - As discussed above, Windows operating systems through Windows Server 2003 use ports 1025-5000. ELB uses the port range 1024-65535.</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 32768 - 61000</strong> - As discussed above, Linux kernels (including the Amazon Linux kernel) use ports 1025-5000. ELB uses the port range 1024-65535.</p>\n\n<p><strong>Add a rule to the Security Group allowing outbound traffic on port 80</strong> - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n",
            "answers": [
                "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 1025 - 5000</p>",
                "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 1024 - 65535</p>",
                "<p>Add a rule to the Security Group allowing outbound traffic on port 80</p>",
                "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 32768 - 61000</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have configured a Network ACL and a Security Group for the load balancer and Amazon EC2 instances to allow inbound traffic on port 80. However, users are still unable to connect to your website after launch.\n\nWhich additional configuration is required to make the website accessible to all users over the internet?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790098,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A development team had enabled and configured CloudTrail for all the Amazon S3 buckets used in a project. The project manager owns all the S3 buckets used in the project. However, the manager noticed that he did not receive any object-level API access logs when the data was read by another AWS account.</p>\n\n<p>What could be the reason for this behavior/error?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The bucket owner also needs to be object owner to get the object access logs</strong></p>\n\n<p>If the bucket owner is also the object owner, the bucket owner gets the object access logs. Otherwise, the bucket owner must get permissions, through the object ACL, for the same object API to get the same object-access API logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudTrail always delivers object-level API access logs to the requester and not to object owner</strong> - CloudTrail always delivers object-level API access logs to the requester. In addition, CloudTrail also delivers the same logs to the bucket owner only if the bucket owner has permissions for the same API actions on that object.</p>\n\n<p><strong>CloudTrail needs to be configured on both the AWS accounts for receiving the access logs in cross-account access</strong></p>\n\n<p><strong>The meta-data of the bucket is in an invalid state and needs to be corrected by the bucket owner from AWS console to fix the issue</strong></p>\n\n<p>These two options are incorrect and are given only as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html#cloudtrail-object-level-crossaccount\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html#cloudtrail-object-level-crossaccount</a></p>\n",
            "answers": [
                "<p>The bucket owner also needs to be object owner to get the object access logs</p>",
                "<p>CloudTrail needs to be configured on both the AWS accounts for receiving the access logs in cross-account access</p>",
                "<p>CloudTrail always delivers object-level API access logs to the requester and not to object owner</p>",
                "<p>The meta-data of the bucket is in an invalid state and needs to be corrected by the bucket owner from AWS console to fix the issue</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A development team had enabled and configured CloudTrail for all the Amazon S3 buckets used in a project. The project manager owns all the S3 buckets used in the project. However, the manager noticed that he did not receive any object-level API access logs when the data was read by another AWS account.\n\nWhat could be the reason for this behavior/error?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790088,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A company wants to add geospatial capabilities to the cache layer, along with query capabilities and an ability to horizontally scale. The company uses Amazon RDS as the database tier.</p>\n\n<p>Which solution is optimal for this use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage the capabilities offered by ElastiCache for Redis with cluster mode enabled</strong></p>\n\n<p>You can use Amazon ElastiCache to accelerate your high volume application workloads by caching your data in-memory providing sub-millisecond data retrieval performance. When used in conjunction with any database including Amazon RDS or Amazon DynamoDB, ElastiCache can alleviate the pressure associated with heavy request loads, increase overall application performance and reduce costs associated with scaling for throughput on other databases.</p>\n\n<p>Amazon ElastiCache makes it easy to deploy and manage a highly available and scalable in-memory data store in the cloud. Among the open source in-memory engines available for use with ElastiCache is Redis, which added powerful geospatial capabilities in its newer versions.</p>\n\n<p>You can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster Mode comes with the primary benefit of horizontal scaling up and down of your Redis cluster, with almost zero impact on the performance of the cluster.</p>\n\n<p>Enabling Cluster Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.</p>\n\n<p>Cluster Mode also allows for more flexibility when designing new workloads with unknown storage requirements or heavy write activity. In a read-heavy workload, one can scale a single shard by adding read replicas, up to five, but a write-heavy workload can benefit from additional write endpoints when cluster mode is enabled.</p>\n\n<p>Geospatial on Amazon ElastiCache for Redis:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the capabilities offered by ElastiCache for Redis with cluster mode disabled</strong> - For a production workload, you should consider using a configuration that includes replication to enhance the protection of your data. Also, only vertical scaling is possible when cluster mode is disabled. The use case mentions horizontal scaling as a requirement, hence disabling cluster mode is not an option.</p>\n\n<p><strong>Use CloudFront caching to cater to demands of increasing workloads</strong> - One of the purposes of using CloudFront is to reduce the number of requests that your origin server must respond to directly. With CloudFront caching, more objects are served from CloudFront edge locations, which are closer to your users. This reduces the load on your origin server and reduces latency. However, the use case mentions that in-memory caching is needed for enhancing the performance of the application. So, this option is incorrect.</p>\n\n<p><strong>Migrate to Amazon DynamoDB to utilize the automatically integrated DynamoDB Accelerator (DAX) along with query capability features</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. Database migration is a more elaborate effort compared to implementing and optimizing the caching layer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/\">https://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/</a></p>\n",
            "answers": [
                "<p>Leverage the capabilities offered by ElastiCache for Redis with cluster mode disabled</p>",
                "<p>Use CloudFront caching to cater to demands of increasing workloads</p>",
                "<p>Leverage the capabilities offered by ElastiCache for Redis with cluster mode enabled</p>",
                "<p>Migrate to Amazon DynamoDB to utilize the automatically integrated DynamoDB Accelerator (DAX) along with query capability features</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "A company wants to add geospatial capabilities to the cache layer, along with query capabilities and an ability to horizontally scale. The company uses Amazon RDS as the database tier.\n\nWhich solution is optimal for this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790186,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A development team is considering Amazon ElastiCache for Redis as its in-memory caching solution for its relational database.</p>\n\n<p>Which of the following options are correct while configuring ElastiCache? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>All the nodes in a Redis cluster must reside in the same region</strong></p>\n\n<p>All the nodes in a Redis cluster (cluster mode enabled or cluster mode disabled) must reside in the same region.</p>\n\n<p><strong>While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary</strong></p>\n\n<p>While using Redis with cluster mode enabled, there are some limitations:</p>\n\n<ol>\n<li><p>You cannot manually promote any of the replica nodes to primary.</p></li>\n<li><p>Multi-AZ is required.</p></li>\n<li><p>You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously</strong> - When you add a read replica to a cluster, all of the data from the primary is copied to the new node. From that point on, whenever data is written to the primary, the changes are asynchronously propagated to all the read replicas, for both the Redis offerings (cluster mode enabled or cluster mode disabled).</p>\n\n<p><strong>If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled</strong> - If you have no replicas and a node fails, you experience loss of all data in that node's shard, when using Redis with cluster mode enabled. If you have no replicas and the node fails, you experience total data loss in Redis with cluster mode disabled.</p>\n\n<p><strong>You can scale write capacity for Redis by adding replica nodes</strong> - This increases only the read capacity of the Redis cluster, write capacity is not enhanced by read replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html</a></p>\n",
            "answers": [
                "<p>All the nodes in a Redis cluster must reside in the same region</p>",
                "<p>You can scale write capacity for Redis by adding replica nodes</p>",
                "<p>While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary</p>",
                "<p>While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously</p>",
                "<p>If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled</p>"
            ]
        },
        "correct_response": [
            "a",
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A development team is considering Amazon ElastiCache for Redis as its in-memory caching solution for its relational database.\n\nWhich of the following options are correct while configuring ElastiCache? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790176,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A website serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from an application load balancer. The user base is spread across the world and latency should be minimized for a better user experience.</p>\n\n<p>Which technology/service can help access the static and dynamic content while keeping the data latency low?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users</strong></p>\n\n<p>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins.</p>\n\n<p>Steps to configure CLoudFront for multiple origins:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q40-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-distribution-serve-content/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-distribution-serve-content/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly</strong> - AWS Lambda@Edge is a general-purpose serverless compute feature that supports a wide range of computing needs and customizations. Lambda@Edge is best suited for computationally intensive operations. This is not relevant for the given use case.</p>\n\n<p><strong>Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs</strong> - AWS Global Accelerator is a networking service that improves the performance of your users’ traffic by up to 60% using Amazon Web Services’ global network infrastructure.</p>\n\n<p>With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.</p>\n\n<p>CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.</p>\n\n<p>Global Accelerator is not relevant for the given use-case.</p>\n\n<p><strong>Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing</strong> - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an Origin Group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. Origin Groups are for origin failure scenarios and not for request routing.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
            "answers": [
                "<p>Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly</p>",
                "<p>Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users</p>",
                "<p>Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs</p>",
                "<p>Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "A website serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from an application load balancer. The user base is spread across the world and latency should be minimized for a better user experience.\n\nWhich technology/service can help access the static and dynamic content while keeping the data latency low?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790138,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are storing your video files in a separate S3 bucket than your main static website in an S3 bucket. When accessing the video URLs directly the users can view the videos on the browser, but they can't play the videos while visiting the main website.</p>\n\n<p>What is the root cause of this problem?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CORS</strong></p>\n\n<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p>\n\n<p>For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the bucket policy</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that grants permissions. With this policy, you can do things such as allow one IP address to access the video file in the S3 bucket. In this scenario, we know that's not the case because it works using the direct URL but it doesn't work when you click on a link to access the video.</p>\n\n<p><strong>Amend the IAM policy</strong> - You attach IAM policies to IAM users, groups, or roles, which are then subject to the permissions you've defined. This scenario refers to public users of a website and they need not have an IAM user account.</p>\n\n<p><strong>Disable Server-Side Encryption</strong> - Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it, if the video file is encrypted at rest then there is nothing you need to do because AWS handles encrypt and decrypt. Disabling encryption is not an issue because you can access the video directly using an URL but not from the main website.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n",
            "answers": [
                "<p>Enable CORS</p>",
                "<p>Change the bucket policy</p>",
                "<p>Amend the IAM policy</p>",
                "<p>Disable Server-Side Encryption</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "You are storing your video files in a separate S3 bucket than your main static website in an S3 bucket. When accessing the video URLs directly the users can view the videos on the browser, but they can't play the videos while visiting the main website.\n\nWhat is the root cause of this problem?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790206,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A company developed an app-based service for citizens to book transportation rides in the local community. The platform is running on AWS EC2 instances and uses Amazon Relational Database Service (RDS) for storing transportation data. A new feature has been requested where receipts would be emailed to customers with PDF attachments retrieved from Amazon Simple Storage Service (S3).</p>\n\n<p>Which of the following options will provide EC2 instances with the right permissions to upload files to Amazon S3 and generate S3 Signed URL?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Role for EC2</strong></p>\n\n<p>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>\n\n<p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 User Data</strong> - You can specify user data when you launch an instance and you would not want to hard code the AWS credentials in the user data.</p>\n\n<p><strong>Run <code>aws configure</code> on the EC2 instance</strong> - When you first configure the CLI you have to run this command, afterward you should not need to if you want to obtain credentials to authenticate to other AWS services. An IAM role will receive temporary credentials for you so you can focus on using the CLI to get access to other AWS services if you have the permissions.</p>\n\n<p><strong>CloudFormation</strong> - AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
            "answers": [
                "<p>CloudFormation</p>",
                "<p>EC2 User Data</p>",
                "<p>Run <code>aws configure</code> on the EC2 instance</p>",
                "<p>Create an IAM Role for EC2</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A company developed an app-based service for citizens to book transportation rides in the local community. The platform is running on AWS EC2 instances and uses Amazon Relational Database Service (RDS) for storing transportation data. A new feature has been requested where receipts would be emailed to customers with PDF attachments retrieved from Amazon Simple Storage Service (S3).\n\nWhich of the following options will provide EC2 instances with the right permissions to upload files to Amazon S3 and generate S3 Signed URL?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790094,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB.</p>\n\n<p>Is it possible to send these messages to SQS?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Yes, the max message size is 256KB</strong></p>\n\n<p>The minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q43-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Yes, the max message size is 512KB</strong> - The max size is 256KB</p>\n\n<p><strong>No, the max message size is 128KB</strong> - The max size is 256KB</p>\n\n<p><strong>No, the max message size is 64KB</strong> - The max size is 256KB</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n",
            "answers": [
                "<p>Yes, the max message size is 512KB</p>",
                "<p>Yes, the max message size is 256KB</p>",
                "<p>No, the max message size is 128KB</p>",
                "<p>No, the max message size is 64KB</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB.\n\nIs it possible to send these messages to SQS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790168,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code <code>console.log(JSON.stringify(this.event))</code> in hopes of getting more details about the request to your Alexa skill.</p>\n\n<p>You would like the logs stored in an Amazon Simple Storage Service (S3) bucket named <code>MyAlexaLog</code>. How do you achieve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudWatch integration feature with S3</strong></p>\n\n<p>You can export log data from your CloudWatch log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p>\n\n<p>Exporting CloudWatch Log Data to Amazon S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudWatch integration feature with Kinesis</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.</p>\n\n<p><strong>Use CloudWatch integration feature with Lambda</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.</p>\n\n<p><strong>Use CloudWatch integration feature with Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. Glue is not the right fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n",
            "answers": [
                "<p>Use CloudWatch integration feature with Kinesis</p>",
                "<p>Use CloudWatch integration feature with S3</p>",
                "<p>Use CloudWatch integration feature with Lambda</p>",
                "<p>Use CloudWatch integration feature with Glue</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code console.log(JSON.stringify(this.event)) in hopes of getting more details about the request to your Alexa skill.\n\nYou would like the logs stored in an Amazon Simple Storage Service (S3) bucket named MyAlexaLog. How do you achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790100,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have a Java-based application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment.</p>\n\n<p>Which of the following options will allow you to deploy in this manner? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>In-place Deployment</strong></p>\n\n<p>The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.</p>\n\n<p><strong>Blue/green Deployment</strong></p>\n\n<p>With a blue/green deployment, you provision a new set of instances on which CodeDeploy installs the latest version of your application. CodeDeploy then re-routes load balancer traffic from an existing set of instances running the previous version of your application to the new set of instances running the latest version. After traffic is re-routed to the new instances, the existing instances can be terminated.</p>\n\n<p>CodeDeploy Deployment Types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cattle Deployment</strong> - This is a good option if you have cattle in a farm</p>\n\n<p><strong>Warm Standby Deployment</strong> - This is not a valid CodeDeploy deployment option. The term \"Warm Standby\" is used to describe a Disaster Recovery scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</p>\n\n<p><strong>Pilot Light Deployment</strong> - This is not a valid CodeDeploy deployment option. \"Pilot Light\" is a Disaster Recovery approach where you simply replicate part of your IT structure for a limited set of core services so that the AWS cloud environment seamlessly takes over in the event of a disaster.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>\n",
            "answers": [
                "<p>Cattle Deployment</p>",
                "<p>Warm Standby Deployment</p>",
                "<p>In-place Deployment</p>",
                "<p>Blue/green Deployment</p>",
                "<p>Pilot Light Deployment</p>"
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Deployment",
        "question_plain": "You have a Java-based application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment.\n\nWhich of the following options will allow you to deploy in this manner? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790190,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>SNS + Kinesis</p>",
                "<p>SNS + SQS</p>",
                "<p>SNS + Lambda</p>",
                "<p>SQS + SES</p>"
            ],
            "question": "<p>DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be re-processed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources.</p>\n\n<p>Which of the following solutions can be used to address this use-case?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SNS + SQS</strong></p>\n\n<p>Amazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.</p>\n\n<p>How SNS Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png\">\nvia - <a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.</p>\n\n<p>SNS and SQS can be used to create a fanout messaging scenario in which messages are \"pushed\" to multiple subscribers, which eliminates the need to periodically check or poll for updates and enables parallel asynchronous processing of the message by the subscribers. SQS can allow for later re-processing and dead letter queues. This is called the fan-out pattern.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SNS + Kinesis</strong> - You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real-time. Kinesis Data Streams stores records from 24 hours (by default) to 8760 hours (365 days). However, you need to manually provision shards in case the load increases or you need to use CloudWatch alarms to set up auto scaling for the shards. Since Kinesis does not support transparent scaling so this option is not the right fit for the given use case.</p>\n\n<p><strong>SNS + Lambda</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to other SNS topics, or send the message to other AWS services. However, your EC2 instances cannot \"poll\" from Lambda functions and as such, this would not work.</p>\n\n<p><strong>SQS + SES</strong> - This will not work as the messages need to be processed twice (once for sending the notification and later for order fulfillment) and SQS only allows for one consuming application.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/\">https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/</a></p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be re-processed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources.\n\nWhich of the following solutions can be used to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790214,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are planning to build a fleet of EBS-optimized EC2 instances to handle the load of your new application. Due to security compliance, your organization wants any secret strings used in the application to be encrypted to prevent exposing values as clear text.</p>\n\n<p>The solution requires that decryption events be audited and API calls to be simple. How can this be achieved? (select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Store the secret as SecureString in SSM Parameter Store</strong></p>\n\n<p>With AWS Systems Manager Parameter Store, you can create SecureString parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters. Also, if you are using customer-managed CMKs, you can use IAM policies and key policies to manage to encrypt and decrypt permissions. To retrieve the decrypted value you only need to do one API call.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n\n<p><strong>Audit using CloudTrail</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>CloudTrail will allow you to see all API calls made to SSM and KMS.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Encrypt first with KMS then store in SSM Parameter store</strong> - This could work but will require two API calls to get the decrypted value instead of one. So this is not the right option.</p>\n\n<p><strong>Store the secret as PlainText in SSM Parameter Store</strong> - Plaintext parameters are not secure and shouldn't be used to store secrets.</p>\n\n<p><strong>Audit using SSM Audit Trail</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n",
            "answers": [
                "<p>Encrypt first with KMS then store in SSM Parameter store</p>",
                "<p>Store the secret as SecureString in SSM Parameter Store</p>",
                "<p>Store the secret as PlainText in SSM Parameter Store</p>",
                "<p>Audit using CloudTrail</p>",
                "<p>Audit using SSM Audit Trail</p>"
            ]
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Security",
        "question_plain": "You are planning to build a fleet of EBS-optimized EC2 instances to handle the load of your new application. Due to security compliance, your organization wants any secret strings used in the application to be encrypted to prevent exposing values as clear text.\n\nThe solution requires that decryption events be audited and API calls to be simple. How can this be achieved? (select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790216,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing.</p>\n\n<p>Which of the following methods do you recommend?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment</strong></p>\n\n<p>AWS Elastic Beanstalk makes it easy to create new environments for your application. You can create and manage separate environments for development, testing, and production use, and you can deploy any version of your application to any environment. Environments can be long-running or temporary. When you terminate an environment, you can save its configuration to recreate it later.</p>\n\n<p>It is common practice to have many environments for the same application. You can deploy multiple environments when you need to run multiple versions of an application. So for the given use-case, you can set up 'dev' and 'load test' environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.html</a></p>\n\n<p><strong>You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment</strong> - Incorrect, use the Create New Environment wizard in the AWS Management Console for BeanStalk to guide you on this.</p>\n\n<p><strong>Use only one Beanstalk environment and perform configuration changes using an Ansible script</strong> - Ansible is an open-source deployment tool that integrates with AWS. It allows us to deploy the infrastructure. Elastic Beanstalk provisions the servers that you need for hosting the application and it also handles multiple environments, so Beanstalk is a better option.</p>\n\n<p><strong>Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB</strong> - This is not a good design if you need to load test because you will have two versions on the same instances and may not be able to access resources in the system due to the load testing.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html</a></p>\n",
            "answers": [
                "<p>Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB</p>",
                "<p>You cannot have multiple development environments in Elastic Beanstalk, just one development and one production environment</p>",
                "<p>Use only one Beanstalk environment and perform configuration changes using an Ansible script</p>",
                "<p>Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing.\n\nWhich of the following methods do you recommend?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790144,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues.</p>\n\n<p>Which of the following steps can you take to meet your requirements without making changes to the existing code?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable SQS KMS encryption</strong></p>\n\n<p>Server-side encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service (AWS KMS).</p>\n\n<p>AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that encrypt your message data are also encrypted and stored with the data they protect.</p>\n\n<p>You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the SSL endpoint</strong> - The given use-case needs encryption at rest. When using SSL, the data is encrypted during transit, but the data needs to be encrypted at rest as well, so this option is incorrect.</p>\n\n<p><strong>Use Client-side encryption</strong> - For additional security, you can build your application to encrypt messages before they are placed in a message queue but will require a code change, so this option is incorrect.</p>\n\n<p><em>*Use Secrets Manager *</em> - AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Secrets Manager cannot be used for encrypting data at rest.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html</a></p>\n",
            "answers": [
                "<p>Use Secrets Manager</p>",
                "<p>Use the SSL endpoint</p>",
                "<p>Use Client side encryption</p>",
                "<p>Enable SQS KMS encryption</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues.\n\nWhich of the following steps can you take to meet your requirements without making changes to the existing code?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790158,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are designing a high-performance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the user’s source IP address and source port without the use of X-Forwarded-For.</p>\n\n<p>Which of the following options will meet your needs?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Network Load Balancer</strong></p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Incoming connections remain unmodified, so application software need not support X-Forwarded-For.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer</strong> - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action.</p>\n\n<p>One of many benefits of the Application Load Balancer is its support for path-based routing. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. For needs relating to network traffic go with Network Load Balancer.</p>\n\n<p><strong>Elastic Load Balancer</strong> - Elastic Load Balancing is the service itself that offers different types of load balancers.</p>\n\n<p><strong>Classic Load Balancer</strong> - It is a basic load balancer that distributes traffic. If your account was created before 2013-12-04, your account supports EC2-Classic instances and you will benefit in using this type of load balancer. The classic load balancer can be used regardless of when your account was created and whether you use EC2-Classic or whether your instances are in a VPC but just remember its the basic load balancer AWS offers and not advanced as the others.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n",
            "answers": [
                "<p>Application Load Balancer</p>",
                "<p>Network Load Balancer</p>",
                "<p>Elastic Load Balancer</p>",
                "<p>Classic Load Balancer</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are designing a high-performance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the user’s source IP address and source port without the use of X-Forwarded-For.\n\nWhich of the following options will meet your needs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790204,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing end-to-end connections using HTTPS so that content is protected.</p>\n\n<p>Which of the following options is available for HTTPS in AWS CloudFront?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Between clients and CloudFront as well as between CloudFront and backend</strong></p>\n\n<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers.</p>\n\n<p>Requiring HTTPS for Communication Between Viewers and CloudFront:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q51-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p>\n\n<p>You also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.</p>\n\n<p>Requiring HTTPS for Communication Between CloudFront and Your Custom Origin:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q51-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Between clients and CloudFront only</strong> - This is incorrect as you can choose to require HTTPS between CloudFront and your origin.</p>\n\n<p><strong>Between CloudFront and backend only</strong> - This is incorrect as you can choose to require HTTPS between viewers and CloudFront.</p>\n\n<p><strong>Neither between clients and CloudFront nor between CloudFront and backend</strong> - This is incorrect as you can choose HTTPS settings both for communication between viewers and CloudFront as well as between CloudFront and your origin.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secure-connections-supported-viewer-protocols-ciphers.html#secure-connections-supported-ciphers-cloudfront-to-origin\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secure-connections-supported-viewer-protocols-ciphers.html#secure-connections-supported-ciphers-cloudfront-to-origin</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html</a></p>\n",
            "answers": [
                "<p>Between clients and CloudFront only</p>",
                "<p>Between clients and CloudFront as well as between CloudFront and backend</p>",
                "<p>Between CloudFront and backend only</p>",
                "<p>Neither between clients and CloudFront nor between CloudFront and backend</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing end-to-end connections using HTTPS so that content is protected.\n\nWhich of the following options is available for HTTPS in AWS CloudFront?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790134,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Use Git credentials generated from IAM</p>",
                "<p>Use IAM Multi-Factor authentication</p>",
                "<p>Use authentication offered by GitHub secure tokens</p>",
                "<p>Use IAM user secret access key and access key ID</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Git credentials generated from IAM</strong> - CodeCommit repositories are Git-based and support the basic functionalities of Git such as Git credentials. AWS recommends that you use an IAM user when working with CodeCommit. You can access CodeCommit with other identity types, but the other identity types are subject to limitations.</p>\n\n<p>The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections. You can also use these same credentials with any third-party tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password.</p>\n\n<p>An IAM user is an identity within your Amazon Web Services account that has specific custom permissions. For example, an IAM user can have permissions to create and manage Git credentials for accessing CodeCommit repositories. This is the recommended user type for working with CodeCommit. You can use an IAM user name and password to sign in to secure AWS webpages like the AWS Management Console, AWS Discussion Forums, or the AWS Support Center.</p>\n\n<p>Authentication and access control for AWS CodeCommit:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use IAM Multi-Factor authentication</strong> - AWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor—what they know), as well as for an authentication code from their AWS MFA device (the second factor—what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources.</p>\n\n<p><strong>Use IAM user secret access key and access key ID</strong> - Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best practice, AWS suggests using temporary security credentials (IAM roles) instead of access keys.</p>\n\n<p><strong>Use authentication offered by GitHub secure tokens</strong> - Personal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line. This option is specific to GitHub only and hence not useful for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html</a></p>\n",
            "question": "<p>A company would like to migrate the existing application code from a GitHub repository to AWS CodeCommit.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following would you recommend for migrating the cloned repository to CodeCommit over HTTPS?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A company would like to migrate the existing application code from a GitHub repository to AWS CodeCommit.\n\nAs an AWS Certified Developer Associate, which of the following would you recommend for migrating the cloned repository to CodeCommit over HTTPS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790110,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A development team is storing sensitive customer data in S3 that will require encryption at rest. The encryption keys must be rotated at least annually.</p>\n\n<p>What is the easiest way to implement a solution for this requirement?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS KMS with automatic key rotation</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. You have three mutually exclusive options, depending on how you choose to manage the encryption keys: Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS), Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer managed CMK that you have already created. If you don't specify a customer managed CMK, Amazon S3 automatically creates an AWS managed CMK in your AWS account the first time that you add an object encrypted with SSE-KMS to a bucket. By default, Amazon S3 uses this CMK for SSE-KMS.</p>\n\n<p>You can choose to have AWS KMS automatically rotate CMKs every year, provided that those keys were generated within AWS KMS HSMs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Encrypt the data before sending it to Amazon S3</strong> - The act of encrypting data before sending it to Amazon S3 is called Client-Side encryption. You will have to handle the key generation, maintenance and rotation process. Hence, this is not the right choice here.</p>\n\n<p><strong>Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function</strong> - When you import a custom key, you are responsible for maintaining a copy of your imported keys in your key management infrastructure so that you can re-import them at any time. Also, automatic key rotation is not supported for imported keys. Using Lambda functions to rotate keys is a possible solution, but not an optimal one for the current use case.</p>\n\n<p><strong>Use SSE-C with automatic key rotation on an annual basis</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The keys are not stored anywhere in Amazon S3. There is no automatic key rotation facility for this option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
            "answers": [
                "<p>Encrypt the data before sending it to Amazon S3</p>",
                "<p>Use AWS KMS with automatic key rotation</p>",
                "<p>Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function</p>",
                "<p>Use SSE-C with automatic key rotation on an annual basis</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "A development team is storing sensitive customer data in S3 that will require encryption at rest. The encryption keys must be rotated at least annually.\n\nWhat is the easiest way to implement a solution for this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790174,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A firm maintains a highly available application that receives HTTPS traffic from mobile devices and web browsers. The main Developer would like to set up the Load Balancer routing to route traffic from web servers to smart.com/api and from mobile devices to smart.com/mobile. A developer advises that the previous recommendation is not needed and that requests should be sent to api.smart.com and mobile.smart.com instead.</p>\n\n<p>Which of the following routing options were discussed in the given use-case? (select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Path based</strong></p>\n\n<p>You can create a listener with rules to forward requests based on the URL path. This is known as path-based routing. If you are running microservices, you can route traffic to multiple back-end services using path-based routing. For example, you can route general requests to one target group and request to render images to another target group.</p>\n\n<p>This path-based routing allows you to route requests to, for example, /api to one set of servers (also known as target groups) and /mobile to another set. Segmenting your traffic in this way gives you the ability to control the processing environment for each category of requests. Perhaps /api requests are best processed on Compute Optimized instances, while /mobile requests are best handled by Memory Optimized instances.</p>\n\n<p><strong>Host based</strong></p>\n\n<p>You can create Application Load Balancer rules that route incoming traffic based on the domain name specified in the Host header. Requests to api.example.com can be sent to one target group, requests to mobile.example.com to another, and all others (by way of a default rule) can be sent to a third. You can also create rules that combine host-based routing and path-based routing. This would allow you to route requests to api.example.com/production and api.example.com/sandbox to distinct target groups.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#rule-condition-types\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#rule-condition-types</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Client IP</strong> - This option has been added as a distractor. Routing is not based on the client's IP address.</p>\n\n<p><strong>Web browser version</strong> - Routing has nothing to do with the client's web browser, if it was then there is something sneaky going on.</p>\n\n<p><strong>Cookie value</strong> - Application Load Balancers support load balancer-generated cookies only and you cannot modify them. When routing sticky sessions to route requests to the same target then cookies are needed to be supported by the client's browser.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p>\n",
            "answers": [
                "<p>Path based</p>",
                "<p>Cookie value</p>",
                "<p>Client IP</p>",
                "<p>Web browser version</p>",
                "<p>Host based</p>"
            ]
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A firm maintains a highly available application that receives HTTPS traffic from mobile devices and web browsers. The main Developer would like to set up the Load Balancer routing to route traffic from web servers to smart.com/api and from mobile devices to smart.com/mobile. A developer advises that the previous recommendation is not needed and that requests should be sent to api.smart.com and mobile.smart.com instead.\n\nWhich of the following routing options were discussed in the given use-case? (select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790182,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have a popular three-tier web application that is used by users throughout the globe receiving thousands of incoming requests daily. You have AWS Route 53 policies to automatically distribute weighted traffic to the API resources located at URL api.global.com.</p>\n\n<p>What is an alternative way of distributing traffic to a web application?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ELB</strong></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. Route 53 failover policy is similar to an ELB in that when using failover routing, it lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudFront</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds.</p>\n\n<p><strong>S3</strong> - Amazon S3 provides you a way to store and retrieve any amount of data. It cannot be used to distribute traffic.</p>\n\n<p><strong>Auto Scaling</strong> - AWS Auto Scaling will automatically scale resources as needed to align to your selected scaling strategy, so you maintain performance and pay only for the resources you need. It cannot be used to distribute traffic.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n",
            "answers": [
                "<p>S3</p>",
                "<p>CloudFront</p>",
                "<p>ELB</p>",
                "<p>Auto Scaling</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have a popular three-tier web application that is used by users throughout the globe receiving thousands of incoming requests daily. You have AWS Route 53 policies to automatically distribute weighted traffic to the API resources located at URL api.global.com.\n\nWhat is an alternative way of distributing traffic to a web application?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790104,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You have been hired at a company that needs an experienced developer to help with a continuous integration/continuous delivery (CI/CD) workflow on AWS. You configure the company’s workflow to run an AWS CodePipeline pipeline whenever the application’s source code changes in a repository hosted in AWS Code Commit and compiles source code with AWS Code Build. You are configuring ProjectArtifacts in your build stage.</p>\n\n<p>Which of the following should you do?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket</strong></p>\n\n<p>If you choose ProjectArtifacts and your value type is S3 then the build project stores build output in Amazon Simple Storage Service (Amazon S3). For that, you will need to give AWS CodeBuild permissions to upload.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CodeBuild to store output artifacts on EC2 servers</strong> - EC2 servers are not a valid output location, so this option is ruled out.</p>\n\n<p><strong>Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket</strong> - AWS CodeCommit is the repository that holds source code and has no control over compiling the source code, so this option is incorrect.</p>\n\n<p><strong>Contact AWS Support to allow AWS CodePipeline to manage build outputs</strong> - You can set AWS CodePipeline to manage its build output locations instead of AWS CodeBuild. There is no need to contact AWS Support.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html#create-project-cli\">https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html#create-project-cli</a></p>\n",
            "answers": [
                "<p>Configure AWS CodeBuild to store output artifacts on EC2 servers</p>",
                "<p>Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket</p>",
                "<p>Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket</p>",
                "<p>Contact AWS Support to allow AWS CodePipeline to manage build outputs</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "You have been hired at a company that needs an experienced developer to help with a continuous integration/continuous delivery (CI/CD) workflow on AWS. You configure the company’s workflow to run an AWS CodePipeline pipeline whenever the application’s source code changes in a repository hosted in AWS Code Commit and compiles source code with AWS Code Build. You are configuring ProjectArtifacts in your build stage.\n\nWhich of the following should you do?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790124,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company manages MySQL databases on EC2 instances to have full control. Applications on other EC2 instances managed by an ASG make requests to these databases to get information that displays data on dashboards viewed on mobile phones, tablets, and web browsers.</p>\n\n<p>Your manager would like to scale your Auto Scaling group based on the number of requests per minute. How can you achieve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>You create a CloudWatch custom metric and build an alarm to scale your ASG</strong></p>\n\n<p>Here we need to scale on the metric \"number of requests per minute\", which is a custom metric we need to create, as it's not readily available in CloudWatch.</p>\n\n<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Attach an Elastic Load Balancer</strong> - This is not what you need for auto-scaling. An Elastic Load Balancer distributes workloads across multiple compute resources and checks your instances' health status to name a few, but it does not automatically increase and decrease the number of instances based on the application requirement.</p>\n\n<p><strong>Attach additional Elastic File Storage</strong> - This is a file storage service designed for performance. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. This cannot be used to facilitate auto-scaling.</p>\n\n<p>How EFS Works:\n<img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\">\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p><strong>You enable detailed monitoring and use that to scale your ASG</strong> - The detailed monitoring metrics won't provide information about database /application-level requests per minute, so this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n",
            "answers": [
                "<p>You create a CloudWatch custom metric and build an alarm to scale your ASG</p>",
                "<p>Attach an Elastic Load Balancer</p>",
                "<p>Attach additional Elastic File Storage</p>",
                "<p>You enable detailed monitoring and use that to scale your ASG</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company manages MySQL databases on EC2 instances to have full control. Applications on other EC2 instances managed by an ASG make requests to these databases to get information that displays data on dashboards viewed on mobile phones, tablets, and web browsers.\n\nYour manager would like to scale your Auto Scaling group based on the number of requests per minute. How can you achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790178,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your mobile application needs to perform API calls to DynamoDB. You do not want to store AWS secret and access keys onto the mobile devices and need all the calls to DynamoDB made with a different identity per mobile device.</p>\n\n<p>Which of the following services allows you to achieve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Cognito Identity Pools\"</p>\n\n<p>Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. Identity pools provide AWS credentials to grant your users access to other AWS services.</p>\n\n<p>Cognito Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Cognito User Pools\" - AWS Cognito User Pools is there to authenticate users for your applications which looks similar to Cognito Identity Pools. The difference is that Identity Pools allows a way to authorize your users to use the various AWS services and User Pools is not about authorizing to AWS services but to provide add sign-up and sign-in functionality to web and mobile applications.</p>\n\n<p>\"Cognito Sync\" - You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status.</p>\n\n<p>\"IAM\" - This is not a good solution because it would require you to have an IAM user for each mobile device which is not a good practice or manageable way of handling deployment.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
            "answers": [
                "<p>Cognito Identity Pools</p>",
                "<p>Cognito User Pools</p>",
                "<p>Cognito Sync</p>",
                "<p>IAM</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your mobile application needs to perform API calls to DynamoDB. You do not want to store AWS secret and access keys onto the mobile devices and need all the calls to DynamoDB made with a different identity per mobile device.\n\nWhich of the following services allows you to achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790164,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You are revising options that would be best for monitoring a few EC2 instances you currently manage. Amazon CloudWatch has metrics available to monitor your EC2 instances for CPU load, I/O, and network I/O. Your budget does not allow for spending on monitoring so using the default monitoring available is your preferred choice.</p>\n\n<p>With default monitoring, at what interval will these metrics be collected?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>5 minutes</strong></p>\n\n<p>By default, your instance is enabled for basic monitoring and Amazon EC2 sends metric data to CloudWatch in 5-minute periods. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance.</p>\n\n<p>The following describes the data interval and charge for basic and detailed monitoring for instances.</p>\n\n<p>Basic monitoring\nData is available automatically in 5-minute periods at no charge.</p>\n\n<p>Detailed monitoring\nData is available in 1-minute periods for an additional charge.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>10 minutes</strong></p>\n\n<p><strong>2 minutes</strong></p>\n\n<p><strong>1 minute</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html</a></p>\n",
            "answers": [
                "<p>1 minute</p>",
                "<p>10 minutes</p>",
                "<p>2 minutes</p>",
                "<p>5 minutes</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You are revising options that would be best for monitoring a few EC2 instances you currently manage. Amazon CloudWatch has metrics available to monitor your EC2 instances for CPU load, I/O, and network I/O. Your budget does not allow for spending on monitoring so using the default monitoring available is your preferred choice.\n\nWith default monitoring, at what interval will these metrics be collected?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790092,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your organization is looking into making a change to all virtual machines in the cloud. They would like to take advantage of running a bootstrap script for their Windows Server 2012 Base AMI virtual machines when they first boot.</p>\n\n<p>Which of the following options will allow the organization to achieve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>EC2 User Data</strong></p>\n\n<p>When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. A use case for user data would be if you run web servers for various small businesses, they can all use the same AMI and retrieve their content from the Amazon S3 bucket you specify in the user data at launch.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Mount EFS network drives</strong> - Amazon EFS is a file storage service for use with Amazon EC2 that can provide storage for up to thousands of Amazon EC2 instances. EFS does not provide instructions but instead a storage service.</p>\n\n<p><strong>EC2 MetaData</strong> - Meta Data is information about your running instance, for example, you can access information such as the local IP address of your instance.</p>\n\n<p><strong>Custom AMI</strong> - With a Custom AMI, you can pre-install software and configure your Operating System to have all it needs before launching. If you were using User Data, you would have software or other tasks that you can specify when the instance launches but may take longer to boot so a Custom AMI can be recommended.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n",
            "answers": [
                "<p>Custom AMI</p>",
                "<p>Mount EFS network drives</p>",
                "<p>EC2 Meta Data</p>",
                "<p>EC2 User Data</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your organization is looking into making a change to all virtual machines in the cloud. They would like to take advantage of running a bootstrap script for their Windows Server 2012 Base AMI virtual machines when they first boot.\n\nWhich of the following options will allow the organization to achieve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790140,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company is in the process of building a DevOps culture and is moving all of its on-premise resources to the cloud using serverless architectures and automated deployments. You have created a CloudFormation template in YAML that uses an AWS Lambda function to pull HTML files from GitHub and place them into an Amazon Simple Storage Service (S3) bucket that you specify.</p>\n\n<p>Which of the following AWS CLI commands can you use to upload AWS Lambda functions and AWS CloudFormation templates to AWS?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>cloudformation package</code> and <code>cloudformation deploy</code></strong></p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>The <code>cloudformation package</code> command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command will upload local artifacts, such as your source code for your AWS Lambda function.</p>\n\n<p>The <code>cloudformation deploy</code> command deploys the specified AWS CloudFormation template by creating and then executing a changeset.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>cloudformation package</code> and <code>cloudformation upload</code></strong> - The <code>cloudformation upload</code> command does not exist.</p>\n\n<p><strong><code>cloudformation zip</code> and <code>cloudformation upload</code></strong> - Both commands do not exist, this is a made-up option.</p>\n\n<p><strong><code>cloudformation zip</code> and <code>cloudformation deploy</code></strong> - The <code>cloudformation zip</code> command does not exist, this is a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p>\n",
            "answers": [
                "<p><code>cloudformation zip</code> and <code>cloudformation deploy</code></p>",
                "<p><code>cloudformation package</code> and <code>cloudformation upload</code></p>",
                "<p><code>cloudformation zip</code> and <code>cloudformation upload</code></p>",
                "<p><code>cloudformation package</code> and <code>cloudformation deploy</code></p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Your company is in the process of building a DevOps culture and is moving all of its on-premise resources to the cloud using serverless architectures and automated deployments. You have created a CloudFormation template in YAML that uses an AWS Lambda function to pull HTML files from GitHub and place them into an Amazon Simple Storage Service (S3) bucket that you specify.\n\nWhich of the following AWS CLI commands can you use to upload AWS Lambda functions and AWS CloudFormation templates to AWS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790108,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your AWS CodeDeploy deployment to T2 instances succeed. The new application revision makes API calls to Amazon S3 however the application is not working as expected due to authorization exceptions and you were assigned to troubleshoot the issue.</p>\n\n<p>Which of the following should you do?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Fix the IAM permissions for the EC2 instance role</strong></p>\n\n<p>You should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. In this case, make sure your role has access to the S3 bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fix the IAM permissions for the CodeDeploy service role</strong> - The fact that CodeDeploy deployed the application to EC2 instances tells us that there was no issue between those two. The actual issue is between the EC2 instances and S3.</p>\n\n<p><strong>Make the S3 bucket public</strong> - This is not a good practice, you should strive to provide least privilege access. You may have files in here that should not be allowed public access and you are opening the door to security breaches.</p>\n\n<p><strong>Enable CodeDeploy Proxy</strong> - This is not correct as we don't need to look into CodeDeploy settings but rather between EC2 and S3 permissions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p>\n",
            "answers": [
                "<p>Fix the IAM permissions for the CodeDeploy service role</p>",
                "<p>Fix the IAM permissions for the EC2 instance role</p>",
                "<p>Make the S3 bucket public</p>",
                "<p>Enable CodeDeploy Proxy</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "Your AWS CodeDeploy deployment to T2 instances succeed. The new application revision makes API calls to Amazon S3 however the application is not working as expected due to authorization exceptions and you were assigned to troubleshoot the issue.\n\nWhich of the following should you do?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790156,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A financial services company with over 10,000 employees has hired you as the new Senior Developer. Initially caching was enabled to reduce the number of calls made to all API endpoints and improve the latency of requests to the company’s API Gateway.</p>\n\n<p>For testing purposes, you would like to invalidate caching for the API clients to get the most recent responses. Which of the following should you do?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Using the Header Cache-Control: max-age=0</strong></p>\n\n<p>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q63-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Request parameter: ?bypass_cache=1</strong> - Method parameters take query string but this is not one of them.</p>\n\n<p><strong>Using the Header Bypass-Cache=1</strong> - This is a made-up option.</p>\n\n<p><strong>Using the request parameter ?cache-control-max-age=0</strong> - To invalidate cache it requires a header and not a request parameter.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching</a></p>\n",
            "answers": [
                "<p>Using the request parameter ?cache-control-max-age=0</p>",
                "<p>Use the Request parameter: ?bypass_cache=1</p>",
                "<p>Using the Header Bypass-Cache=1</p>",
                "<p>Using the Header Cache-Control: max-age=0</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A financial services company with over 10,000 employees has hired you as the new Senior Developer. Initially caching was enabled to reduce the number of calls made to all API endpoints and improve the latency of requests to the company’s API Gateway.\n\nFor testing purposes, you would like to invalidate caching for the API clients to get the most recent responses. Which of the following should you do?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790120,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>As a Full-stack Web Developer, you are involved with every aspect of a company’s platform from development with PHP and JavaScript to the configuration of NoSQL databases with Amazon DynamoDB. You are not concerned about your response receiving stale data from your database and need to perform 16 eventually consistent reads per second of 12 KB in size each.</p>\n\n<p>How many read capacity units (RCUs) do you need?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q64-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q64-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>24</strong></p>\n\n<p>One read capacity unit represents <strong>two</strong> eventually consistent reads per second, for an item up to <strong>4 KB</strong> in size.\nSo that means that for an item of 12KB in size, we need 3 RCU (12 KB / 4 KB) for <strong>two</strong> eventually consistent reads per second. As we need 16 eventually consistent reads per second, we need 3 * (16 / 2) = 24 RCU.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>12</strong></p>\n\n<p><strong>192</strong></p>\n\n<p><strong>48</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html</a></p>\n",
            "answers": [
                "<p>12</p>",
                "<p>192</p>",
                "<p>24</p>",
                "<p>48</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a Full-stack Web Developer, you are involved with every aspect of a company’s platform from development with PHP and JavaScript to the configuration of NoSQL databases with Amazon DynamoDB. You are not concerned about your response receiving stale data from your database and need to perform 16 eventually consistent reads per second of 12 KB in size each.\n\nHow many read capacity units (RCUs) do you need?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 45790170,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>Your company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New security compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n\n<p>Which of the following options should you use?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
            "answers": [
                "<p>SSE-S3</p>",
                "<p>SSE-C</p>",
                "<p>Client Side Encryption</p>",
                "<p>SSE-KMS</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "Your company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New security compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.\n\nWhich of the following options should you use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334280,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong></p>\n\n<p>As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role.</p>\n\n<p>So for the given use-case, you should create an IAM role with read-only permissions for the S3 bucket and apply it to the EC2 instance profile.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</strong></p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</strong></p>\n\n<p>As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application or even configure these credentials in the user data of the EC2 instance. So both these options are incorrect.</p>\n\n<p><strong>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong> - As the application is running on EC2 instances, therefore you need to set up an EC2 service role, not an S3 service role.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</p>",
                "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</p>",
                "<p>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>",
                "<p>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>"
            ],
            "question": "<p>A media company wants to migrate a video editing service to Amazon EC2 while following security best practices. The videos are sourced and read from a non-public S3 bucket.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A media company wants to migrate a video editing service to Amazon EC2 while following security best practices. The videos are sourced and read from a non-public S3 bucket.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334306,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p>You can develop and test your serverless application locally, and then you can deploy your application by using the sam deploy command. The sam deploy command zips your application artifacts, uploads them to Amazon Simple Storage Service (Amazon S3), and deploys your application to the AWS Cloud. AWS SAM uses AWS CloudFormation as the underlying deployment mechanism.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p>\n",
            "question": "<p>A media analytics company has built a streaming application on Lambda using Serverless Application Model (SAM).</p>\n\n<p>As a Developer Associate, which of the following would you identify as the correct order of execution to successfully deploy the application?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</p>",
                "<p>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</p>",
                "<p>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</p>",
                "<p>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A media analytics company has built a streaming application on Lambda using Serverless Application Model (SAM).\n\nAs a Developer Associate, which of the following would you identify as the correct order of execution to successfully deploy the application?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334258,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><strong>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</strong></p>\n\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. For the given use-case, you can set up a rule that detects pipeline changes and invokes an AWS Lambda function.</p>\n\n<p>Amazon CloudWatch Events With CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i2.jpg\">\n<a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</strong> - As mentioned in the explanation above, you need to use a CloudWatch event and not CloudWatch alarm for this use-case.</p>\n\n<p><strong>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</strong> - You cannot create a trigger with CodePipeline as the event source via the Lambda Console.</p>\n\n<p><strong>Use the CodePipeline console to set up a trigger for the Lambda function</strong> - CodePipeline console cannot be used to configure a trigger for a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
            "question": "<p>An IT company leverages CodePipeline to automate its release pipelines. The development team wants to write a Lambda function that will send notifications for state changes within the pipeline.</p>\n\n<p>As a Developer Associate, which steps would you suggest to associate the Lambda function with the event source?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</p>",
                "<p>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</p>",
                "<p>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</p>",
                "<p>Use the CodePipeline console to set up a trigger for the Lambda function</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An IT company leverages CodePipeline to automate its release pipelines. The development team wants to write a Lambda function that will send notifications for state changes within the pipeline.\n\nAs a Developer Associate, which steps would you suggest to associate the Lambda function with the event source?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334276,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</strong></p>\n\n<p>As each message takes more than 15 minutes to process, therefore the development team cannot use Lambda for message processing. To build a scalable solution, the EC2 instances must be provisioning via an Auto Scaling group to handle variations in the message processing workload.</p>\n\n<p>Amazon EC2 Auto Scaling Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision an EC2 instance to poll the messages from an SQS queue</strong> - Just using a single EC2 instance may not be sufficient to handle a sudden spike in the number of incoming messages.</p>\n\n<p><strong>Contact AWS Support to increase the Lambda timeout to 60 minutes</strong> - AWS Support cannot increase the Lambda timeout upper limit.</p>\n\n<p><strong>Use DynamoDB instead of RDS as database</strong> - This option has been added as a distractor, as changing the database would have no impact on the Lambda timeout while processing the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n",
            "question": "<p>A data analytics company ingests a large number of messages and stores them in an RDS database using Lambda. Because of the increased payload size, it is taking more than 15 minutes to process each message.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to process each message in the MOST scalable way?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</p>",
                "<p>Provision an EC2 instance to poll the messages from an SQS queue</p>",
                "<p>Contact AWS Support to increase the Lambda timeout to 60 minutes</p>",
                "<p>Use DynamoDB instead of RDS as database</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A data analytics company ingests a large number of messages and stores them in an RDS database using Lambda. Because of the increased payload size, it is taking more than 15 minutes to process each message.\n\nAs a Developer Associate, which of the following options would you recommend to process each message in the MOST scalable way?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334318,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</strong></p>\n\n<p>The CloudWatch agent enables you to do the following:</p>\n\n<p>Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p>\n\n<p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p>\n\n<p>To enable the CloudWatch agent to send data from an on-premises server, you must specify the access key and secret key of the IAM user that you created earlier.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch Logs to directly read the logs from the on-premises server</strong> - This is a made-up option as you cannot have CloudWatch Logs directly communicate with the on-premises server. You have to go via the CloudWatch Agent.</p>\n\n<p><strong>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</strong></p>\n\n<p><strong>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</strong></p>\n\n<p>Both these options require significant customizations and still will not be as neatly integrated with CloudWatch as compared to just using the CloudWatch Agent which is available off-the-shelf.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p>\n",
            "question": "<p>A development team has a mix of applications hosted on-premises as well as on EC2 instances. The on-premises application controls all applications deployed on the EC2 instances. In case of any errors, the team wants to leverage Amazon CloudWatch to monitor and troubleshoot the on-premises application.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</p>",
                "<p>Configure CloudWatch Logs to directly read the logs from the on-premises server</p>",
                "<p>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</p>",
                "<p>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A development team has a mix of applications hosted on-premises as well as on EC2 instances. The on-premises application controls all applications deployed on the EC2 instances. In case of any errors, the team wants to leverage Amazon CloudWatch to monitor and troubleshoot the on-premises application.\n\nAs a Developer Associate, which of the following solutions would you suggest to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334288,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-KMS</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> When retrieving objects encrypted server-side with SSE-C, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>SSE-S3</p>",
                "<p>SSE-C</p>",
                "<p>Client Side Encryption</p>",
                "<p>SSE-KMS</p>"
            ],
            "question": "<p>Your company has developers worldwide with access to the company's Amazon Simple Storage Service (S3) buckets. The objects in the buckets are encrypted at the server-side but need more flexibility with access control, auditing, rotation, and deletion of keys. You would also like to limit who can use the key.</p>\n\n<p>Which encryption mechanism best fits your needs?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "Your company has developers worldwide with access to the company's Amazon Simple Storage Service (S3) buckets. The objects in the buckets are encrypted at the server-side but need more flexibility with access control, auditing, rotation, and deletion of keys. You would also like to limit who can use the key.\n\nWhich encryption mechanism best fits your needs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334290,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>Route 53 is a DNS managed by AWS, but nothing prevents you from running your own DNS (it's just a software) on an EC2 instance. The trick of this question is that it's about EC2, running some software that needs a fixed IP, and not about Route 53 at all.</p>\n\n<p><strong>Elastic IP</strong></p>\n\n<p>DNS services are identified by a public IP, so you need to use Elastic IP.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Load Balancer and an auto-scaling group</strong> - Load balancers do not provide an IP, instead they provide a DNS name, so this option is ruled out.</p>\n\n<p><strong>Provide a static private IP</strong> - If you provide a private IP it will not be accessible from the internet, so this option is incorrect.</p>\n\n<p><strong>Use Route 53</strong> - Route 53 is a DNS service from AWS but the use-case talks about offering a DNS service using an EC2 instance, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use Route 53</p>",
                "<p>Create a Load Balancer and an auto scaling group</p>",
                "<p>Provide a static private IP</p>",
                "<p>Elastic IP</p>"
            ],
            "question": "<p>You are running a public DNS service on an EC2 instance where the DNS name is pointing to the IP address of the instance. You wish to upgrade your DNS service but would like to do it without any downtime.</p>\n\n<p>Which of the following options will help you accomplish this?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "You are running a public DNS service on an EC2 instance where the DNS name is pointing to the IP address of the instance. You wish to upgrade your DNS service but would like to do it without any downtime.\n\nWhich of the following options will help you accomplish this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334302,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudTrail</strong></p>\n\n<p>With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and code calls to the Amazon S3 APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon S3. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Access Logs</strong> - This captures records of access attempts made against objects in your bucket. Logs contain info for bucket request, time, remote IP, request-URI, and more. This option does not address the use-case mentioned in the question.</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. This option does not address the use-case mentioned in the question.</p>\n\n<p><strong>IAM</strong> - IAM allows you to add users, group, and set permissions but not for auditing API calls, so this is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>IAM</p>",
                "<p>S3 Access Logs</p>",
                "<p>VPC Flow Logs</p>",
                "<p>CloudTrail</p>"
            ],
            "question": "<p>You are a cloud security engineer working for a popular cyber-forensics company that offers vulnerability scanning solutions to government contractors. The scanning solutions are integrated with AWS resources to monitor EC2 and S3 API calls which then display results to users on an analytical dashboard.</p>\n\n<p>Which of the following AWS services makes this possible?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "You are a cloud security engineer working for a popular cyber-forensics company that offers vulnerability scanning solutions to government contractors. The scanning solutions are integrated with AWS resources to monitor EC2 and S3 API calls which then display results to users on an analytical dashboard.\n\nWhich of the following AWS services makes this possible?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334262,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong></p>\n\n<p>To create a new service you would use this command which creates a service in your default region called ecs-simple-service. The service uses the ecs-demo task definition and it maintains 10 instantiations of that task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong> - This command is referencing a different service called Amazon Elastic Container Registry (ECR) which's is a fully-managed Docker container registry</p>\n\n<p><strong><code>docker-compose create ecs-simple-service</code></strong> - This is a docker command to create containers for a service.</p>\n\n<p><strong><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></strong> - This is a valid command but used for starting a new task using a specified task definition.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html\">https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p><code>docker-compose create ecs-simple-service</code></p>",
                "<p><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
                "<p><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
                "<p><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></p>"
            ],
            "question": "<p>Your company is shifting towards Elastic Container Service (ECS) to deploy applications. The process should be automated using the AWS CLI to create a service where at least ten instances of a task definition are kept running under the default cluster.</p>\n\n<p>Which of the following commands should be executed?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "Your company is shifting towards Elastic Container Service (ECS) to deploy applications. The process should be automated using the AWS CLI to create a service where at least ten instances of a task definition are kept running under the default cluster.\n\nWhich of the following commands should be executed?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334286,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Versioning can be enabled only for a specific folder</strong></p>\n\n<p>The versioning state applies to all (never some) of the objects in that bucket. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Overwriting a file increases its versions</strong> - If you overwrite an object (file), it results in a new object version in the bucket. You can always restore the previous version.</p>\n\n<p><strong>Deleting a file is a recoverable operation</strong> - Correct, when you delete an object (file), Amazon S3 inserts a delete marker, which becomes the current object version and you can restore the previous version.</p>\n\n<p><strong>Any file that was unversioned before enabling versioning will have the 'null' version</strong> - Objects stored in your bucket before you set the versioning state have a version ID of null. Those existing objects in your bucket do not change.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Any file that was unversioned before enabling versioning will have the 'null' version</p>",
                "<p>Overwriting a file increases its versions</p>",
                "<p>Deleting a file is a recoverable operation</p>",
                "<p>Versioning can be enabled only for a specific folder</p>"
            ],
            "question": "<p>Your organization has a single Amazon Simple Storage Service (S3) bucket that contains folders labeled with customer names. Several administrators have IAM access to the S3 bucket and versioning is enabled to easily recover from unintended user actions.</p>\n\n<p>Which of the following statements about versioning is <strong>NOT</strong> true based on this scenario?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your organization has a single Amazon Simple Storage Service (S3) bucket that contains folders labeled with customer names. Several administrators have IAM access to the S3 bucket and versioning is enabled to easily recover from unintended user actions.\n\nWhich of the following statements about versioning is NOT true based on this scenario?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334316,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Annotations</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.</p>\n\n<p>X-Ray indexes up to 50 annotations per trace.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Metadata</strong> - Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.</p>\n\n<p><strong>Segments</strong> - The computing resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done.</p>\n\n<p><strong>Sampling</strong> - To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Annotations</p>",
                "<p>Sampling</p>",
                "<p>Segments</p>",
                "<p>Metadata</p>"
            ],
            "question": "<p>You have been collecting AWS X-Ray traces across multiple applications and you would now like to index your XRay traces to search and filter through them efficiently.</p>\n\n<p>What should you use in your instrumentation?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You have been collecting AWS X-Ray traces across multiple applications and you would now like to index your XRay traces to search and filter through them efficiently.\n\nWhat should you use in your instrumentation?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334266,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>The simplest way to use HTTPS with an Elastic Beanstalk environment is to assign a server certificate to your environment's load balancer. When you configure your load balancer to terminate HTTPS, the connection between the client and the load balancer is secure. Backend connections between the load balancer and EC2 instances use HTTP, so no additional configuration of the instances is required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a></p>\n\n<p><strong>Create a config file in the .ebextensions folder to configure the Load Balancer</strong></p>\n\n<p>To update your AWS Elastic Beanstalk environment to use HTTPS, you need to configure an HTTPS listener for the load balancer in your environment. Two types of load balancers support an HTTPS listener: Classic Load Balancer and Application Load Balancer.</p>\n\n<p>Example <code>.ebextensions/securelistener-alb.config</code></p>\n\n<p>Use this example when your environment has an Application Load Balancer. The example uses options in the aws:elbv2:listener namespace to configure an HTTPS listener on port 443 with the specified certificate. The listener routes traffic to the default process.</p>\n\n<pre><code>option_settings:\n  aws:elbv2:listener:443:\n    ListenerEnabled: 'true'\n    Protocol: HTTPS\n    SSLCertificateArns: arn:aws:acm:us-east-2:1234567890123:certificate/####################################\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</strong> - A separate CloudFormation template won't be able to mutate the state of a Load Balancer managed by Elastic Beanstalk, so this option is incorrect.</p>\n\n<p><strong>Open up the port 80 for the security group</strong> - Port 80 is for HTTP traffic, so this option is incorrect.</p>\n\n<p><strong>Configure Health Checks</strong> - Health Checks are not related to SSL certificates, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create a config file in the .ebextensions folder to configure the Load Balancer</p>",
                "<p>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</p>",
                "<p>Open up the port 80 for the security group</p>",
                "<p>Configure Health Checks</p>"
            ],
            "question": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint instead of an HTTP endpoint to get in-flight encryption between your clients and your web servers.</p>\n\n<p>What must be done to set up HTTPS on Beanstalk?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You would like your Elastic Beanstalk environment to expose an HTTPS endpoint instead of an HTTP endpoint to get in-flight encryption between your clients and your web servers.\n\nWhat must be done to set up HTTPS on Beanstalk?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334310,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one customer-managed policy with policy variables and attach it to a group of all users</strong></p>\n\n<p>You can assign access to \"dynamically calculated resources\" by using policy variables, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.</p>\n\n<p>This is ideal when you want want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket, as in the previous example. But don't create a separate policy for each user that explicitly specifies the user's name as part of the resource. Instead, create a single group policy that works for any user in that group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an S3 bucket policy and change it as users are added and removed</strong></p>\n\n<p>This doesn't scale and the S3 bucket policy size may be maxed out. The IAM policies bump up against a size limit (up to 2 kb for users, 5 kb for groups, and 10 kb for roles). S3 supports bucket policies of up 20 kb.</p>\n\n<p><strong>Create inline policies for each user as they are onboarded</strong>: This would work but doesn't scale and it's inefficient.</p>\n\n<p><strong>Create one customer-managed policy per user and attach them to the relevant users</strong>: This would work but doesn't scale and would be a nightmare to manage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n",
            "question": "<p>Your AWS account is now growing to 200 users and you would like to provide each of these users a personal space in the S3 bucket 'my_company_space' with the prefix <code>/home/&lt;username&gt;</code>, where they have read/write access.</p>\n\n<p>How can you do this efficiently?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create one customer-managed policy with policy variables and attach it to a group of all users</p>",
                "<p>Create inline policies for each user as they are onboarded</p>",
                "<p>Create one customer-managed policy per user and attach them to the relevant users</p>",
                "<p>Create an S3 bucket policy and change it as users are added and removed</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "Your AWS account is now growing to 200 users and you would like to provide each of these users a personal space in the S3 bucket 'my_company_space' with the prefix /home/&lt;username&gt;, where they have read/write access.\n\nHow can you do this efficiently?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334260,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Lambda@Edge</strong></p>\n\n<p>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running.</p>\n\n<p>How Lambda@Edge Works:\n<img src=\"https://d1.awsstatic.com/products/cloudfront/AWS-Lambda-at-Edge_How-It-Works-diagram.7ed76b49010fff37b53eb87467cd922b391a3cc7.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Global DynamoDB table as a Lambda source</strong> - A Lambda function can run off of DynamoDB Streams using Event Sources, however, it does not deploy the Lambda function globally.</p>\n\n<p><strong>Deploy Lambda in a Global VPC</strong> - This option is a distractor as there is no concept of Global VPC in AWS.</p>\n\n<p><strong>Deploy Lambda in S3</strong> - You can't deploy Lambda in S3 but can have your Lambda functions triggered by S3 events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use a Lambda@Edge</p>",
                "<p>Use a Global DynamoDB table as a Lambda source</p>",
                "<p>Deploy Lambda in a Global VPC</p>",
                "<p>Deploy Lambda in S3</p>"
            ],
            "question": "<p>You would like to deploy a Lambda function globally so that requests are filtered at the AWS edge locations.</p>\n\n<p>Which Lambda deployment mode do you need?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You would like to deploy a Lambda function globally so that requests are filtered at the AWS edge locations.\n\nWhich Lambda deployment mode do you need?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334254,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Lifecycle Policy</strong></p>\n\n<p>Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.</p>\n\n<p>You can avoid hitting the limit by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete old application versions or to delete application versions when the total number of versions for an application exceeds a specified number.</p>\n\n<p>Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a></p>\n\n<p>Incorrect options:\n<strong>Setup an <code>.ebextensions</code> files</strong> - You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. This does not help with managing versions.</p>\n\n<p><strong>Define a Lambda function</strong> - This could work but would require a lot of manual scripting, to achieve the same desired effect as the Lifecycle Policy EB feature.</p>\n\n<p><strong>Use Worker Environments</strong> - This won't help. If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Setup an <code>.ebextensions</code> file</p>",
                "<p>Define a Lambda function</p>",
                "<p>Use a Lifecycle Policy</p>",
                "<p>Use Worker Environments</p>"
            ],
            "question": "<p>Your organization has set up a full CI/CD pipeline leveraging CodePipeline and the deployment is done on Elastic Beanstalk. This pipeline has worked for over a year now but you are approaching the limits of Elastic Beanstalk in terms of how many versions can be stored in the service.</p>\n\n<p>How can you remove older versions that are not used by Elastic Beanstalk so that new versions can be created for your applications?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "Your organization has set up a full CI/CD pipeline leveraging CodePipeline and the deployment is done on Elastic Beanstalk. This pipeline has worked for over a year now but you are approaching the limits of Elastic Beanstalk in terms of how many versions can be stored in the service.\n\nHow can you remove older versions that are not used by Elastic Beanstalk so that new versions can be created for your applications?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334256,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Do a multi-part upload</strong></p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload as one part</strong> - As explained in the explanation above, if you upload as one part, then you are not maximizing the bandwidth and are not being efficient.</p>\n\n<p><strong>Use SSE-S3 encryption</strong> - Encryption won't help to increase the efficiency of the uploads to S3.</p>\n\n<p><strong>Zip the video file before sending</strong> - Video files are binary formats and should already be optimized in size. Applying zip compression on a video file won't help reduce its size. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Do a multi-part upload</p>",
                "<p>Upload as one part</p>",
                "<p>Use SSE-S3 encryption</p>",
                "<p>Zip the video file before sending</p>"
            ],
            "question": "<p>You are running a video website and provide users with S3 pre-signed URLs allowing your users to securely upload their video content onto your buckets. The average file size uploaded to your buckets is 500MB and you would like your users to efficiently send the content.</p>\n\n<p>What would you recommend doing in the client SDK?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are running a video website and provide users with S3 pre-signed URLs allowing your users to securely upload their video content onto your buckets. The average file size uploaded to your buckets is 500MB and you would like your users to efficiently send the content.\n\nWhat would you recommend doing in the client SDK?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334292,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a sidecar container</strong></p>\n\n<p>In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't deploy the agent on the EC2 instance or run an X-Ray agent container as a daemon set (only available for ECS classic).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p>\n\n<p><strong>Provide the correct IAM task role to the X-Ray container</strong></p>\n\n<p>For Fargate, we can only provide IAM roles to tasks, which is also the best security practice should we use EC2 instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a daemon set on ECS</strong> - As explained above, since we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't run an X-Ray agent container as a daemon set.</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a process on your EC2 instance</strong></p>\n\n<p><strong>Provide the correct IAM instance role to the EC2 instance</strong></p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p>\n",
            "question": "<p>You would like to run the X-Ray daemon for your Docker containers deployed using AWS Fargate.</p>\n\n<p>What do you need to do to ensure the setup will work? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Deploy the X-Ray daemon agent as a process on your EC2 instance</p>",
                "<p>Deploy the X-Ray daemon agent as a daemon set on ECS</p>",
                "<p>Provide the correct IAM task role to the X-Ray container</p>",
                "<p>Deploy the X-Ray daemon agent as a sidecar container</p>",
                "<p>Provide the correct IAM instance role to the EC2 instance</p>"
            ]
        },
        "correct_response": [
            "c",
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You would like to run the X-Ray daemon for your Docker containers deployed using AWS Fargate.\n\nWhat do you need to do to ensure the setup will work? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334308,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><strong>Put the function and the dependencies in one folder and zip them together</strong></p>\n\n<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3. This is the standard way of packaging Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</strong></p>\n\n<p><strong>Upload the code through the AWS console and upload the dependencies as a zip</strong></p>\n\n<p><strong>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</strong></p>\n\n<p>These three options are incorrect as there's only one way of deploying a Lambda function, which is to provide the zip file with all dependencies that it'll need.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</p>",
                "<p>Upload the code through the AWS console and upload the dependencies as a zip</p>",
                "<p>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</p>",
                "<p>Put the function and the dependencies in one folder and zip them together</p>"
            ],
            "question": "<p>Your Lambda function must use the Node.js drivers to connect to your RDS PostgreSQL database in your VPC.</p>\n\n<p>How do you bundle your Lambda function to add the dependencies?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your Lambda function must use the Node.js drivers to connect to your RDS PostgreSQL database in your VPC.\n\nHow do you bundle your Lambda function to add the dependencies?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334284,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>S3 Select</strong></p>\n\n<p>S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases in many cases you can get as much as a 400% improvement.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Inventory</strong> - Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs.</p>\n\n<p><strong>S3 Analytics</strong> - By using Amazon S3 analytics storage class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.</p>\n\n<p><strong>S3 Access Logs</strong> - Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n",
            "question": "<p>You would like to retrieve a subset of your dataset stored in S3 with the CSV format. You would like to retrieve a month of data and only 3 columns out of the 10.</p>\n\n<p>You need to minimize compute and network costs for this, what should you use?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>S3 Inventory</p>",
                "<p>S3 Select</p>",
                "<p>S3 Analytics</p>",
                "<p>S3 Access Logs</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You would like to retrieve a subset of your dataset stored in S3 with the CSV format. You would like to retrieve a month of data and only 3 columns out of the 10.\n\nYou need to minimize compute and network costs for this, what should you use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334274,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><strong>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p>You can upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block.</p>\n\n<p>The AWS::Lambda::Function resource creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code.</p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</strong></p>\n\n<p>The other option is to write the code inline for Node.js and Python as long as there are no dependencies for your code, besides the dependencies already provided by AWS in your Lambda Runtime (aws-sdk and cfn-response and many other AWS related libraries are preloaded via, for example, boto3 (python) in the lambda instances.)</p>\n\n<p>YAML template for creating a Lambda function:</p>\n\n<pre><code>Type: AWS::Lambda::Function\nProperties:\n  Code:\n    Code\n  DeadLetterConfig:\n    DeadLetterConfig\n  Description: String\n  Environment:\n    Environment\n  FileSystemConfigs:\n    - FileSystemConfig\n  FunctionName: String\n  Handler: String\n  KmsKeyArn: String\n  Layers:\n    - String\n  MemorySize: Integer\n  ReservedConcurrentExecutions: Integer\n  Role: String\n  Runtime: String\n  Tags:\n    - Tag\n  Timeout: Integer\n  TracingConfig:\n    TracingConfig\n  VpcConfig:\n    VpcConfig\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</p>",
                "<p>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</p>",
                "<p>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</p>",
                "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</p>",
                "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</p>"
            ],
            "question": "<p>Your company wants to move away from manually managing Lambda in the AWS console and wants to upload and update them using AWS CloudFormation.</p>\n\n<p>How do you declare an AWS Lambda function in CloudFormation? (Select two)</p>\n"
        },
        "correct_response": [
            "b",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company wants to move away from manually managing Lambda in the AWS console and wants to upload and update them using AWS CloudFormation.\n\nHow do you declare an AWS Lambda function in CloudFormation? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334304,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.</p>\n\n<p>A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Each KCL consumer application instance uses \"workers\" to process data in Kinesis shards.  At any given time, each shard of data records is bound to a particular worker via a lease. For the given use-case, an EC2 instance acts as the worker for the KCL application.  You can have at most one EC2 instance per shard in Kinesis for the given application. As we have 10 shards, the max number of EC2 instances is 10.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1</strong></p>\n\n<p><strong>6</strong></p>\n\n<p><strong>20</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>1</p>",
                "<p>6</p>",
                "<p>10</p>",
                "<p>20</p>"
            ],
            "question": "<p>One of your Kinesis Stream is experiencing increased traffic due to a sale day. Therefore your Kinesis Administrator has split shards and thus you went from having 6 shards to having 10 shards in your Kinesis Stream. Your consuming application is running a KCL-based application on EC2 instances.</p>\n\n<p>What is the maximum number of EC2 instances that can be deployed to process the shards?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "One of your Kinesis Stream is experiencing increased traffic due to a sale day. Therefore your Kinesis Administrator has split shards and thus you went from having 6 shards to having 10 shards in your Kinesis Stream. Your consuming application is running a KCL-based application on EC2 instances.\n\nWhat is the maximum number of EC2 instances that can be deployed to process the shards?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334268,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Docker multi-container platform</strong></p>\n\n<p>Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Multicontainer Docker platform if you need to run multiple containers on each instance. The Multicontainer Docker platform does not include a proxy server. Elastic Beanstalk uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multi-container Docker environments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Docker single-container platform</strong> - Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Single Container Docker platform if you only need to run a single Docker container on each instance in your environment. The single container platform includes an Nginx proxy server.</p>\n\n<p><strong>Custom Platform</strong> - Elastic Beanstalk supports custom platforms. A custom platform provides more advanced customization than a custom image in several ways. A custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances. This flexibility enables you to build a platform for an application that uses a language or other infrastructure software, for which Elastic Beanstalk doesn't provide a managed platform. Compare that to custom images, where you modify an Amazon Machine Image (AMI) for use with an existing Elastic Beanstalk platform, and Elastic Beanstalk still provides the platform scripts and controls the platform's software stack. Besides, with custom platforms, you use an automated, scripted way to create and maintain your customization, whereas with custom images you make the changes manually over a running instance.</p>\n\n<p><strong>Third Party Platform</strong> - This is a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker\">https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Docker single-container platform</p>",
                "<p>Custom platform</p>",
                "<p>Third-party platform</p>",
                "<p>Docker multi-container platform</p>"
            ],
            "question": "<p>Your client wants to deploy a service on EC2 instances, and as EC2 instances are added into an ASG, each EC2 instance should be running 3 different Docker Containers simultaneously.</p>\n\n<p>What Elastic Beanstalk platform should they choose?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Your client wants to deploy a service on EC2 instances, and as EC2 instances are added into an ASG, each EC2 instance should be running 3 different Docker Containers simultaneously.\n\nWhat Elastic Beanstalk platform should they choose?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334314,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>--projection-expression</strong></p>\n\n<p>A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n\n<p>To read data from a table, you use operations such as GetItem, Query, or Scan. DynamoDB returns all of the item attributes by default. To get just some, rather than all of the attributes, use a projection expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>--filter-expression</strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query will consume the same amount of read capacity, regardless of whether a filter expression is present.</p>\n\n<p><strong>--page-size</strong> - You can use the --page-size option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p><strong>--max-items</strong> - To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n",
            "question": "<p>Which of the following CLI options will allow you to retrieve a subset of the attributes coming from a DynamoDB scan?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>--filter-expression</p>",
                "<p>--projection-expression</p>",
                "<p>--page-size</p>",
                "<p>--max-items</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Which of the following CLI options will allow you to retrieve a subset of the attributes coming from a DynamoDB scan?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334272,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB Transactions</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB TTL</strong> - DynamoDB TTL allows you to expire data based on a timestamp, so this option is not correct.</p>\n\n<p><strong>DynamoDB Streams</strong> - DynamoDB Streams gives a changelog of changes that happened to your tables and then may even relay these to a Lambda function for further processing.</p>\n\n<p><strong>DynamoDB Indexes</strong> - GSI and LSI are used to allow you to query your tables using different partition/sort keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n",
            "question": "<p>You are implementing a banking application in which you need to update the Exchanges DynamoDB table and the AccountBalance DynamoDB table at the same time or not at all.</p>\n\n<p>Which DynamoDB feature should you use?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>DynamoDB Indexes</p>",
                "<p>DynamoDB Transactions</p>",
                "<p>DynamoDB TTL</p>",
                "<p>DynamoDB Streams</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are implementing a banking application in which you need to update the Exchanges DynamoDB table and the AccountBalance DynamoDB table at the same time or not at all.\n\nWhich DynamoDB feature should you use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334312,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><strong>Setup a Worker environment and a <code>cron.yaml</code> file</strong></p>\n\n<p>An environment is a collection of AWS resources running an application version. An environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>For a worker environment, you need a <code>cron.yaml</code> file to define the cron jobs and do repetitive tasks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup a Web Server environment and a <code>cron.yaml</code> file</strong></p>\n\n<p><strong>Setup a Worker environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><strong>Setup a Web Server environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><code>.ebextensions/</code> won't work to define cron jobs, and Web Server environments cannot be set up to perform repetitive and scheduled tasks. So these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n",
            "question": "<p>As part of your video processing application, you are looking to perform a set of repetitive and scheduled tasks asynchronously. Your application is deployed on Elastic Beanstalk.</p>\n\n<p>Which Elastic Beanstalk environment should you set up for performing the repetitive tasks?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Setup a Worker environment and a <code>cron.yaml</code> file</p>",
                "<p>Setup a Web Server environment and a <code>cron.yaml</code> file</p>",
                "<p>Setup a Worker environment and a <code>.ebextensions</code> file</p>",
                "<p>Setup a Web Server environment and a <code>.ebextensions</code> file</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As part of your video processing application, you are looking to perform a set of repetitive and scheduled tasks asynchronously. Your application is deployed on Elastic Beanstalk.\n\nWhich Elastic Beanstalk environment should you set up for performing the repetitive tasks?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334278,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket policy authorizes reads</strong></p>\n\n<p>When evaluating an IAM policy of an EC2 instance doing actions on S3, the least-privilege union of both the IAM policy of the EC2 instance and the bucket policy of the S3 bucket are taken into account.</p>\n\n<p>For the given use-case, as IAM role has been removed, therefore only the S3 bucket policy comes into effect which authorizes reads.</p>\n\n<p>Here is a great reference blog for understanding the various scenarios for using IAM policy vs S3 bucket policy -\n<a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is using cached temporary IAM credentials</strong> - As the IAM instance role has been removed that wouldn't be the case</p>\n\n<p><strong>Removing an instance role from an EC2 instance can take a few minutes before being active</strong> - It is immediately active and even if it wasn't, it wouldn't make sense as we can still do reads but not writes.</p>\n\n<p><strong>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</strong> - This is not true. Every single request is evaluated against IAM in the AWS model.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>The EC2 instance is using cached temporary IAM credentials</p>",
                "<p>Removing an instance role from an EC2 instance can take a few minutes before being active</p>",
                "<p>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</p>",
                "<p>The S3 bucket policy authorizes reads</p>"
            ],
            "question": "<p>An EC2 instance has an IAM instance role attached to it, providing it read and write access to the S3 bucket 'my_bucket'. You have tested the IAM instance role and both reads and writes are working. You then remove the IAM role from the EC2 instance and test both read and write again. Writes stopped working but reads are still working.</p>\n\n<p>What is the likely cause of this behavior?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An EC2 instance has an IAM instance role attached to it, providing it read and write access to the S3 bucket 'my_bucket'. You have tested the IAM instance role and both reads and writes are working. You then remove the IAM role from the EC2 instance and test both read and write again. Writes stopped working but reads are still working.\n\nWhat is the likely cause of this behavior?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334264,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Create a role in the target unified account and allow roles in each sub-account to assume the role</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use an IAM instance role</strong></p>\n\n<p>The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>X-Ray can also track requests flowing through applications or services across multiple AWS Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a></p>\n\n<p>You can create the necessary configurations for cross-account access via this reference documentation -\n<a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a user in the target unified account and generate access and secret keys</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use access and secret keys</strong></p>\n\n<p>These two options combined together would work but wouldn't be a best-practice security-wise. Therefore these are not correct.</p>\n\n<p><strong>Enable Cross Account collection in the X-Ray console</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n",
            "question": "<p>Your company likes to operate multiple AWS accounts so that teams have their environments. Services deployed across these accounts interact with one another, and now there's a requirement to implement X-Ray traces across all your applications deployed on EC2 instances and AWS accounts.</p>\n\n<p>As such, you would like to have a unified account to view all the traces. What should you in your X-Ray daemon set up to make this work? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create a role in the target unified account and allow roles in each sub-account to assume the role.</p>",
                "<p>Configure the X-Ray daemon to use an IAM instance role</p>",
                "<p>Create a user in the target unified account and generate access and secret keys</p>",
                "<p>Configure the X-Ray daemon to use access and secret keys</p>",
                "<p>Enable Cross Account collection in the X-Ray console</p>"
            ]
        },
        "correct_response": [
            "a",
            "b"
        ],
        "section": "Security",
        "question_plain": "Your company likes to operate multiple AWS accounts so that teams have their environments. Services deployed across these accounts interact with one another, and now there's a requirement to implement X-Ray traces across all your applications deployed on EC2 instances and AWS accounts.\n\nAs such, you would like to have a unified account to view all the traces. What should you in your X-Ray daemon set up to make this work? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334296,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>S3ReadPolicy</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>AWS SAM allows you to choose from a list of policy templates to scope the permissions of your Lambda functions to the resources that are used by your application.</p>\n\n<p>AWS SAM applications in the AWS Serverless Application Repository that use policy templates don't require any special customer acknowledgments to deploy the application from the AWS Serverless Application Repository.</p>\n\n<p>S3ReadPolicy =&gt; Gives read-only permission to objects in an Amazon S3 bucket.</p>\n\n<p>S3CrudPolicy =&gt; Gives create, read, update, and delete permission to objects in an Amazon S3 bucket.</p>\n\n<p>SQSPollerPolicy =&gt; Permits to poll an Amazon SQS Queue.</p>\n\n<p>LambdaInvokePolicy =&gt; Permits to invoke a Lambda function, alias, or version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SQSPollerPolicy</strong></p>\n\n<p><strong>S3CrudPolicy</strong></p>\n\n<p><strong>LambdaInvokePolicy</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>SQSPollerPolicy</p>",
                "<p>S3CrudPolicy</p>",
                "<p>S3ReadPolicy</p>",
                "<p>LambdaInvokePolicy</p>"
            ],
            "question": "<p>You are deploying Lambda functions that operate on your S3 buckets to read files and extract key metadata. The Lambda functions are managed using SAM.</p>\n\n<p>Which Policy should you insert in your serverless model template to give buckets read access?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "You are deploying Lambda functions that operate on your S3 buckets to read files and extract key metadata. The Lambda functions are managed using SAM.\n\nWhich Policy should you insert in your serverless model template to give buckets read access?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334300,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Write Through strategy</strong></p>\n\n<p>The write-through strategy adds data or updates data in the cache whenever data is written to the database.</p>\n\n<p>In a Write Through strategy, any new blog or update to the blog will be written to both the database layer and the caching layer, thus ensuring that the latest data is always served from the cache.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong></p>\n\n<p>Lazy Loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store.</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>In the case of Lazy Loading, the data is loaded onto the cache whenever the data is missing from the cache. In case the blog gets updated, it won't be updated from the cache unless that cache expires (in case you used a TTL). Time to live (TTL) is an integer value that specifies the number of seconds until the key expires. When an application attempts to read an expired key, it is treated as though the key is not found. The database is queried for the key and the cache is updated. Therefore, for a while, old data will be served to users which is a problem from a requirements perspective as we don't want any stale data.</p>\n\n<p><strong>Use DAX</strong> - This is a cache for DynamoDB based implementations, but in this question, we are considering ElastiCache. Therefore this option is not relevant.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
            "question": "<p>You are running a web application where users can author blogs and share them with their followers. Most of the workflow is read based, but when a blog is updated, you would like to ensure that the latest data is served to the users (no stale data). The Developer has already suggested using ElastiCache to cope with the read load but has asked you to implement a caching strategy that complies with the requirements of the site.</p>\n\n<p>Which strategy would you recommend?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use a Lazy Loading strategy with TTL</p>",
                "<p>Use a Write Through strategy</p>",
                "<p>Use a Lazy Loading strategy without TTL</p>",
                "<p>Use DAX</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Refactoring",
        "question_plain": "You are running a web application where users can author blogs and share them with their followers. Most of the workflow is read based, but when a blog is updated, you would like to ensure that the latest data is served to the users (no stale data). The Developer has already suggested using ElastiCache to cope with the read load but has asked you to implement a caching strategy that complies with the requirements of the site.\n\nWhich strategy would you recommend?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334282,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The GSI is throttling so you need to provision more RCU and WCU to the GSI</strong></p>\n\n<p>DynamoDB supports two types of secondary indexes:</p>\n\n<p>Global secondary index — An index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table.</p>\n\n<p>Local secondary index — An index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>If you perform heavy write activity on the table, but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.</p>\n\n<p>Incorrect options</p>\n\n<p><strong>The LSI is throttling so you need to provision more RCU and WCU to the LSI</strong> - LSI use the RCU and WCU of the main table, so you can't provision more RCU and WCU to the LSI.</p>\n\n<p><strong>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</strong> - This option has been added as a distractor. It is fine to have LSI and GSI together.</p>\n\n<p><strong>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</strong> - This could be a reason, but in this case, the GSI is at fault as the application has been running fine for months.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>The LSI is throttling so you need to provision more RCU and WCU to the LSI</p>",
                "<p>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</p>",
                "<p>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</p>",
                "<p>The GSI is throttling so you need to provision more RCU and WCU to the GSI</p>"
            ],
            "question": "<p>You have created a DynamoDB table to support your application and provisioned RCU and WCU to it so that your application has been running for over a year now without any throttling issues. Your application now requires a second type of query over your table and as such, you have decided to create an LSI and a GSI on a new table to support that use case. One month after having implemented such indexes, it seems your table is experiencing throttling.</p>\n\n<p>Upon looking at the table's metrics, it seems the RCU and WCU provisioned are still sufficient. What's happening?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You have created a DynamoDB table to support your application and provisioned RCU and WCU to it so that your application has been running for over a year now without any throttling issues. Your application now requires a second type of query over your table and as such, you have decided to create an LSI and a GSI on a new table to support that use case. One month after having implemented such indexes, it seems your table is experiencing throttling.\n\nUpon looking at the table's metrics, it seems the RCU and WCU provisioned are still sufficient. What's happening?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334294,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>ACM</strong></p>\n\n<p>AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3</strong> - This is used for object storage. Although you could store SSL certificate on S3, this wouldn't be an efficient solution.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.</p>\n\n<p><strong>IAM</strong> - IAM service is used to manage users, groups, roles and policies. Use IAM as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/certificate-manager/\">https://aws.amazon.com/certificate-manager/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>ACM</p>",
                "<p>S3</p>",
                "<p>KMS</p>",
                "<p>IAM</p>"
            ],
            "question": "<p>You need to load SSL certificates onto your Load Balancers and also have EC2 instances dynamically retrieve them when needed for service to service two-way TLS communication.</p>\n\n<p>What service should you use to centrally manage and automatically renew these SSL certificates?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "You need to load SSL certificates onto your Load Balancers and also have EC2 instances dynamically retrieve them when needed for service to service two-way TLS communication.\n\nWhat service should you use to centrally manage and automatically renew these SSL certificates?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334298,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the local directory /tmp</strong></p>\n\n<p>This is 512MB of temporary space you can use for your Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a tmp/ directory in the source zip file and use it</strong> - This option has been added as a distractor, as you can't access a directory within a zip file.</p>\n\n<p><strong>Use the local directory /opt</strong> - This option has been added as a distractor. This path is not accessible.</p>\n\n<p><strong>Use an S3 bucket</strong> - This won't be temporary after the Lambda function is deleted, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/limits.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use the local directory /opt</p>",
                "<p>Create a tmp/ directory in the source zip file and use it</p>",
                "<p>Use an S3 bucket</p>",
                "<p>Use the local directory /tmp</p>"
            ],
            "question": "<p>Your Lambda function processes files for your customers and as part of that process, it creates a lot of intermediary files it needs to store on its disk and then discard.</p>\n\n<p>What is the best way to store temporary files for your Lambda functions that will be discarded when the function stops running?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your Lambda function processes files for your customers and as part of that process, it creates a lot of intermediary files it needs to store on its disk and then discard.\n\nWhat is the best way to store temporary files for your Lambda functions that will be discarded when the function stops running?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334270,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><strong>MessageGroupId</strong></p>\n\n<p>The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageDeduplicationId</strong> - The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p><strong>MessageOrderId</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>MessageHash</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>MessageGroupId</p>",
                "<p>MessageDeduplicationId</p>",
                "<p>MessageOrderId</p>",
                "<p>MessageHash</p>"
            ],
            "question": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis.</p>\n\n<p>As a developer, which message parameter should you set the value of <code>user_id</code> to guarantee the ordering?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are using AWS SQS FIFO queues to get the ordering of messages on a per user_id basis.\n\nAs a developer, which message parameter should you set the value of user_id to guarantee the ordering?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334320,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>--max-items</strong></p>\n\n<p><strong>--starting-token</strong></p>\n\n<p>For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) has three options to control the number of items included in the output when the AWS CLI calls a service's API to populate the list.</p>\n\n<p><code>--page-size</code></p>\n\n<p><code>--max-items</code></p>\n\n<p><code>--starting-token</code></p>\n\n<p>By default, the AWS CLI uses a page size of 1000 and retrieves all available items. For example, if you run <code>aws s3api list-objects</code> on an Amazon S3 bucket that contains 3,500 objects, the AWS CLI makes four calls to Amazon S3, handling the service-specific pagination logic for you in the background and returning all 3,500 objects in the final output.</p>\n\n<p>Here's an example: <code>aws s3api list-objects --bucket my-bucket --max-items 100 --starting-token eyJNYXJrZXIiOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiAxfQ==</code></p>\n\n<p>Incorrect options:</p>\n\n<p>\"--page-size\" - You can use the <code>--page-size</code> option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p>\"--next-token\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>\"--limit\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html\">https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>--page-size</p>",
                "<p>--max-items</p>",
                "<p>--next-token</p>",
                "<p>--starting-token</p>",
                "<p>--limit</p>"
            ],
            "question": "<p>You would like to paginate the results of an S3 List to show 100 results per page to your users and minimize the number of API calls that you will use.</p>\n\n<p>Which CLI options should you use? (Select two)</p>\n"
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "You would like to paginate the results of an S3 List to show 100 results per page to your users and minimize the number of API calls that you will use.\n\nWhich CLI options should you use? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334322,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><strong>MessageDeduplicationId</strong></p>\n\n<p>The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageGroupId</strong> - The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p><strong>ReceiveRequestAttemptId</strong> - This parameter applies only to FIFO (first-in-first-out) queues. The token is used for deduplication of ReceiveMessage calls. If a networking issue occurs after a ReceiveMessage action, and instead of a response you receive a generic error, you can retry the same action with an identical ReceiveRequestAttemptId to retrieve the same set of messages, even if their visibility timeout has not yet expired.</p>\n\n<p><strong>ContentBasedDeduplication</strong> - This is not a message parameter, but a queue setting. Enable content-based deduplication to instruct Amazon SQS to use an SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html</a></p>\n",
            "question": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis. On top of this, you would like to make sure that duplicate messages should not be sent to SQS as this would cause application failure.</p>\n\n<p>As a developer, which message parameter should you set for deduplicating messages?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>ReceiveRequestAttemptId</p>",
                "<p>MessageGroupId</p>",
                "<p>MessageDeduplicationId</p>",
                "<p>ContentBasedDeduplication</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "You are using AWS SQS FIFO queues to get the ordering of messages on a per user_id basis. On top of this, you would like to make sure that duplicate messages should not be sent to SQS as this would cause application failure.\n\nAs a developer, which message parameter should you set for deduplicating messages?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334324,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeStar</strong></p>\n\n<p>AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster. AWS CodeStar makes it easy for your whole team to work together securely, allowing you to easily manage access and add owners, contributors, and viewers to your projects. Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. With the AWS CodeStar project dashboard, you can easily track progress across your entire software development process, from your backlog of work items to teams’ recent code deployments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeBuild</strong></p>\n\n<p><strong>CodeDeploy</strong></p>\n\n<p><strong>CodePipeline</strong></p>\n\n<p>All these options are individual services encompassed by CodeStar when you deploy a project. They have to be used individually and don't provide a unified \"project\" view.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/codestar/\">https://aws.amazon.com/codestar/</a></p>\n",
            "question": "<p>You would like to have a one-stop dashboard for all the CI/CD needs of one of your projects. You don't need heavy control of the individual configuration of each component in your CI/CD, but need to be able to get a holistic view of your projects.</p>\n\n<p>Which service do you recommend?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>CodeBuild</p>",
                "<p>CodeDeploy</p>",
                "<p>CodeStar</p>",
                "<p>CodePipeline</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "You would like to have a one-stop dashboard for all the CI/CD needs of one of your projects. You don't need heavy control of the individual configuration of each component in your CI/CD, but need to be able to get a holistic view of your projects.\n\nWhich service do you recommend?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334326,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS X-Ray</strong></p>\n\n<p>AWS X-Ray is a service that collects data about requests that your application serves and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response but also about calls that your application makes to downstream AWS resources, microservices, databases and HTTP web APIs.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Incorrect options</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p>\n\n<p>Flow logs can help you with several tasks; for example, to troubleshoot why specific traffic is not reaching an instance, which in turn helps you diagnose overly restrictive security group rules. You can also use flow logs as a security tool to monitor the traffic that is reaching your instance.</p>\n\n<p><strong>CloudWatch Events</strong> - Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information.</p>\n\n<p><strong>CloudTrail</strong> - AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p>\n",
            "question": "<p>Your client has tasked you with finding a service that would enable you to get cross-account tracing and visualization.</p>\n\n<p>Which service do you recommend?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>AWS X-Ray</p>",
                "<p>VPC Flow Logs</p>",
                "<p>CloudWatch Events</p>",
                "<p>CloudTrail</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your client has tasked you with finding a service that would enable you to get cross-account tracing and visualization.\n\nWhich service do you recommend?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334328,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch Events</strong></p>\n\n<p>You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>Schedule Expressions for CloudWatch Events Rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3</strong></p>\n\n<p><strong>SQS</strong></p>\n\n<p><strong>Kinesis</strong></p>\n\n<p>These three AWS services don't have cron capabilities, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
            "question": "<p>You are looking to invoke an AWS Lambda function every hour (similar to a cron job) in a serverless way.</p>\n\n<p>Which event source should you use for your AWS Lambda function?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Amazon S3</p>",
                "<p>CloudWatch Events</p>",
                "<p>SQS</p>",
                "<p>Kinesis</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are looking to invoke an AWS Lambda function every hour (similar to a cron job) in a serverless way.\n\nWhich event source should you use for your AWS Lambda function?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334330,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use TTL</strong></p>\n\n<p>Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DynamoDB Streams</strong> - These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.</p>\n\n<p><strong>Use DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.</p>\n\n<p><strong>Use a Lambda function</strong> - This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This band-aid solution would never be as good as using the TTL feature in DynamoDB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use DynamoDB Streams</p>",
                "<p>Use DAX</p>",
                "<p>Use TTL</p>",
                "<p>Use a Lambda function</p>"
            ],
            "question": "<p>You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week.</p>\n\n<p>What should you use?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week.\n\nWhat should you use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334332,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Assign an SSL certificate to the Load Balancer</strong></p>\n\n<p>This ensures that the Load Balancer can expose an HTTPS endpoint.</p>\n\n<p><strong>Open up port 80 &amp; port 443</strong></p>\n\n<p>This ensures that the Load Balancer will allow both the HTTP (80) and HTTPS (443) protocol for incoming connections</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTP traffic to HTTPS</strong></p>\n\n<p>This ensures traffic originating from HTTP onto the Load Balancer forces a redirect to HTTPS by the EC2 instances before being correctly served, thus ensuring the traffic served is fully encrypted.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Only open up port 80</strong> - This is not correct as it would not allow HTTPS traffic (port 443).</p>\n\n<p><strong>Only open up port 443</strong> - This is not correct as it would not allow HTTP traffic (port 80).</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTPS traffic to HTTP</strong> - This is not correct as it would force HTTP traffic, instead of HTTPS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Open up port 80 &amp; port 443</p>",
                "<p>Configure your EC2 instances to redirect HTTPS traffic to HTTP</p>",
                "<p>Configure your EC2 instances to redirect HTTP traffic to HTTPS</p>",
                "<p>Only open up port 443</p>",
                "<p>Only open up port 80</p>",
                "<p>Assign an SSL certificate to the Load Balancer</p>"
            ],
            "question": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint and an HTTP endpoint. The HTTPS endpoint should be used to get in-flight encryption between your clients and your web servers, while the HTTP endpoint should only be used to redirect traffic to HTTPS and support URLs starting with http://.</p>\n\n<p>What must be done to configure this setup? (Select three)</p>\n"
        },
        "correct_response": [
            "a",
            "c",
            "f"
        ],
        "section": "Security",
        "question_plain": "You would like your Elastic Beanstalk environment to expose an HTTPS endpoint and an HTTP endpoint. The HTTPS endpoint should be used to get in-flight encryption between your clients and your web servers, while the HTTP endpoint should only be used to redirect traffic to HTTPS and support URLs starting with http://.\n\nWhat must be done to configure this setup? (Select three)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334334,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Make a snapshot of the database before it gets deleted</strong></p>\n\n<p>Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment.</p>\n\n<p>Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance.</p>\n\n<p>Note: An RDS DB instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you lose your data because the RDS DB instance is deleted by the environment. For more information, see Using Elastic Beanstalk with Amazon RDS.</p>\n\n<p>This is the only way to recover the database data before it gets deleted by Elastic Beanstalk.</p>\n\n<p>Please review this excellent document that addresses this use-case :</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a selective delete in Elastic Beanstalk</strong> - This is not a feature in Elastic Beanstalk.</p>\n\n<p><strong>Change the Elastic Beanstalk environment variables</strong> - Environment variables won't help with the provisioned RDS database.</p>\n\n<p><strong>Convert the Elastic Beanstalk environment to a worker environment</strong> - You can't convert Elastic Beanstalk environments, you can only change their settings.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n",
            "question": "<p>You have created a test environment in Elastic Beanstalk and as part of that environment, you have created an RDS database.</p>\n\n<p>How can you make sure the database can be explored after the environment is destroyed?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Change the Elastic Beanstalk environment variables</p>",
                "<p>Make a selective delete in Elastic Beanstalk</p>",
                "<p>Make a snapshot of the database before it gets deleted</p>",
                "<p>Convert the Elastic Beanstalk environment to a worker environment</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "You have created a test environment in Elastic Beanstalk and as part of that environment, you have created an RDS database.\n\nHow can you make sure the database can be explored after the environment is destroyed?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334336,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>Lazy loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store. Your datastore then returns the data to your application.</p>\n\n<p>In this case, data that is actively requested by users will be cached in ElastiCache, and thanks to the TTL, we can expire that data after a minute to limit the data staleness.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q42-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a></p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong> - This fits the read requirements, but won't help expiring stale data, so we need TTL.</p>\n\n<p><strong>Use a Write Through strategy with TTL</strong></p>\n\n<p><strong>Use a Write Through strategy without TTL</strong></p>\n\n<p>The problem with these two options for the write-through strategy is that we would fill up the cache with unnecessary data and as mentioned in the question we don't have enough space in the cache to fit all the dataset. Therefore we can't use a write-through strategy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a></p>\n",
            "question": "<p>You are creating a web application in which users can follow each other. Some users will be more popular than others and thus their data will be requested very often. Currently, the user data sits in RDS and it has been recommended by your Developer to use ElastiCache as a caching layer to improve the read performance. The whole dataset of users cannot sit in ElastiCache without incurring tremendous costs and therefore you would like to cache only the most often requested users profiles there. As your website is high traffic, it is accepted to have stale data for users for a while, as long as the stale data is less than a minute old.</p>\n\n<p>What caching strategy do you recommend implementing?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use a Lazy Loading strategy with TTL</p>",
                "<p>Use a Lazy Loading strategy without TTL</p>",
                "<p>Use a Write Through strategy with TTL</p>",
                "<p>Use a Write Through strategy without TTL</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "You are creating a web application in which users can follow each other. Some users will be more popular than others and thus their data will be requested very often. Currently, the user data sits in RDS and it has been recommended by your Developer to use ElastiCache as a caching layer to improve the read performance. The whole dataset of users cannot sit in ElastiCache without incurring tremendous costs and therefore you would like to cache only the most often requested users profiles there. As your website is high traffic, it is accepted to have stale data for users for a while, as long as the stale data is less than a minute old.\n\nWhat caching strategy do you recommend implementing?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334338,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS_XRAY_DAEMON_ADDRESS</strong></p>\n\n<p>Set the host and port of the X-Ray daemon listener. By default, the SDK uses 127.0.0.1:2000 for both trace data (UDP) and sampling (TCP). Use this variable if you have configured the daemon to listen on a different port or if it is running on a different host.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_XRAY_TRACING_NAME</strong> - This sets a service name that the SDK uses for segments.</p>\n\n<p><strong>AWS_XRAY_CONTEXT_MISSING</strong> - This should be set to LOG_ERROR to avoid throwing exceptions when your instrumented code attempts to record data when no segment is open.</p>\n\n<p><strong>AWS_XRAY_DEBUG_MODE</strong> - This should be set to TRUE to configure the SDK to output logs to the console, instead of configuring a logger.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>AWS_XRAY_TRACING_NAME</p>",
                "<p>AWS_XRAY_CONTEXT_MISSING</p>",
                "<p>AWS_XRAY_DAEMON_ADDRESS</p>",
                "<p>AWS_XRAY_DEBUG_MODE</p>"
            ],
            "question": "<p>Which environment variable can be used by AWS X-Ray SDK to ensure that the daemon is correctly discovered on ECS?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Which environment variable can be used by AWS X-Ray SDK to ensure that the daemon is correctly discovered on ECS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334340,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead Letter Queue</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that cannot be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Incorrect:</p>\n\n<p><strong>Use DeleteMessage</strong> - This API call deletes the message in the queue but does not help you find the issue.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. If you reduce the VisibilityTimeout, more consumers will get the failed message</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - It won't help because you don't need more time but rather an isolated place to debug.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
            "question": "<p>Applications running on EC2 instances process messages from an SQS queue but sometimes they experience errors due to messages not being processed.</p>\n\n<p>To isolate the messages, which option will help with further debugging?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Increase the VisibilityTimeout</p>",
                "<p>Use DeleteMessage</p>",
                "<p>Reduce the VisibilityTimeout</p>",
                "<p>Implement a Dead Letter Queue</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Applications running on EC2 instances process messages from an SQS queue but sometimes they experience errors due to messages not being processed.\n\nTo isolate the messages, which option will help with further debugging?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334342,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Your application needs to renew the credentials after 1 hour when they expire</strong></p>\n\n<p>AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). By default, AWS Security Token Service (STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com.</p>\n\n<p>Credentials that are created by using account credentials can range from 900 seconds (15 minutes) up to a maximum of 3,600 seconds (1 hour), with a default of 1 hour. Hence you need to renew the credentials post expiry.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your IAM policy is wrong</strong> - If your policy was wrong, a reboot would not solve the issue.</p>\n\n<p><strong>A lambda function revokes your access every hour</strong> - Revoking can be done by an IAM policy. Lambda function cannot revoke access.</p>\n\n<p><strong>The IAM service is experiencing downtime once an hour</strong> - The IAM service is reliable as it's managed by AWS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>The IAM service is experiencing downtime once an hour</p>",
                "<p>Your IAM policy is wrong</p>",
                "<p>A lambda function revokes your access every hour</p>",
                "<p>Your application needs to renew the credentials after 1 hour when they expire</p>"
            ],
            "question": "<p>A developer created an online shopping application that runs on EC2 instances behind load balancers. The same web application version is hosted on several EC2 instances and the instances run in an Auto Scaling group. The application uses STS to request credentials but after an hour your application stops working.</p>\n\n<p>What is the most likely cause of this issue?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A developer created an online shopping application that runs on EC2 instances behind load balancers. The same web application version is hosted on several EC2 instances and the instances run in an Auto Scaling group. The application uses STS to request credentials but after an hour your application stops working.\n\nWhat is the most likely cause of this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334344,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Missing IAM permissions for the CodeBuild Service</strong></p>\n\n<p>By default, IAM users don't have permission to create or modify Amazon Elastic Container Registry (Amazon ECR) resources or perform tasks using the Amazon ECR API. A user who uses the AWS CodeBuild console must have a minimum set of permissions that allows the user to describe other AWS resources for the AWS account.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Docker image is missing some tags</strong> - Tags are optional for naming purposes</p>\n\n<p><strong>CodeBuild cannot work with custom Docker images</strong> - Custom docker images are supported, so this option is incorrect.</p>\n\n<p><strong>The Docker image is too big</strong> - It is good to properly design the image but in this case, it does not affect the CodeBuild. You can also look at multi-stage builds, which are a new feature requiring Docker 17.05 or higher on the daemon and client. Multistage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>The Docker image is too big</p>",
                "<p>The Docker image is missing some tags</p>",
                "<p>CodeBuild cannot work with custom Docker images</p>",
                "<p>Missing IAM permissions for the CodeBuild Service</p>"
            ],
            "question": "<p>Your team lead has finished creating a CodeBuild project in the management console and a build spec has been defined for the project. After the build is run, CodeBuild fails to pull a Docker image into the build environment.</p>\n\n<p>What is the most likely cause?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "Your team lead has finished creating a CodeBuild project in the management console and a build spec has been defined for the project. After the build is run, CodeBuild fails to pull a Docker image into the build environment.\n\nWhat is the most likely cause?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334346,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><strong>STS</strong></p>\n\n<p>The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, is not supported at the time with API Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM permissions with sigv4</strong> - They can be applied to an entire API or individual methods.</p>\n\n<p><strong>Lambda Authorizer</strong> - Control access to REST API methods using bearer token authentication as well as information described by headers, paths, query strings, stage variables, or context variables request parameter.</p>\n\n<p><strong>Cognito User Pools</strong> - Use Cognito User Pools to create customizable authentication and authorization solutions for your REST APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
            "question": "<p>You've just deployed an AWS Lambda function. The lambda function will be invoked via the API Gateway. The API Gateway will need to control access to it.</p>\n\n<p>Which of the following mechanisms is not supported for API Gateway?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>IAM permissions with sigv4</p>",
                "<p>STS</p>",
                "<p>Lambda Authorizer</p>",
                "<p>Cognito User Pools</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "You've just deployed an AWS Lambda function. The lambda function will be invoked via the API Gateway. The API Gateway will need to control access to it.\n\nWhich of the following mechanisms is not supported for API Gateway?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334348,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SNS</strong></p>\n\n<p>Amazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.</p>\n\n<p>Amazon SNS follows the 'publish-subscribe' (pub-sub) messaging paradigm, with notifications being delivered to clients using a 'push' mechanism that eliminates the need to periodically check or 'poll' for new information and updates.</p>\n\n<p>How SNS Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png\">\nvia - <a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SQS</strong> - SQS is a distributed queuing system. Messages are not pushed to receivers. Receivers have to poll SQS to receive messages</p>\n\n<p><strong>Kinesis</strong> - This is used for processing real-time streams meant for big data workloads.</p>\n\n<p><strong>SES</strong> - Amazon SES is an inexpensive way to send and receive emails.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n",
            "question": "<p>Your development team has created a popular mobile app written for Android. The team is looking for a technology that can send messages to mobile devices using a mobile app.</p>\n\n<p>Mobile app users will register and be given permissions to access AWS resources. Which technology would you recommend for subscribing users to messages?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>SES</p>",
                "<p>SQS</p>",
                "<p>Kinesis</p>",
                "<p>SNS</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your development team has created a popular mobile app written for Android. The team is looking for a technology that can send messages to mobile devices using a mobile app.\n\nMobile app users will register and be given permissions to access AWS resources. Which technology would you recommend for subscribing users to messages?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334350,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>RDS</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB</strong> - RDS can run expensive joins which DynamoDB does not support. DynamoDB is a better choice for scaling by storing complex hierarchical data within a single item.</p>\n\n<p><strong>Redshift</strong> - RDS is your best choice here but Amazon Redshift provides an excellent scale-out option as your data and query complexity grows. Redshift is a data warehouse.</p>\n\n<p><strong>ElastiCache</strong> - You can use ElastiCache in combination with RDS. This would be a good option for slow performing database queries in RDS that need to be cached for your application users.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/\">https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Redshift</p>",
                "<p>DynamoDB</p>",
                "<p>RDS</p>",
                "<p>ElastiCache</p>"
            ],
            "question": "<p>You are working for a small organization that does not have a database administrator and the organization needs to install a database on the cloud quickly to support an accounting application used by thousands of users. The application will act as a backend and will perform (CRUD) operations such as create, read, update and delete as well as inner joins.</p>\n\n<p>Which database is best suited for this scenario?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are working for a small organization that does not have a database administrator and the organization needs to install a database on the cloud quickly to support an accounting application used by thousands of users. The application will act as a backend and will perform (CRUD) operations such as create, read, update and delete as well as inner joins.\n\nWhich database is best suited for this scenario?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334352,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Canary Deployment</strong></p>\n\n<p>In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stage Variables</strong> - They act like environment variables and can be used in your API setup.</p>\n\n<p><strong>Mapping Templates</strong> - Its a script to map the payload from a method request to the corresponding integration request and also maps the integration response to the corresponding method response.</p>\n\n<p><strong>Custom Authorizers</strong> - Used for authentication purposes and must return AWS Identity and Access Management (IAM) policies.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Custom Authorizers</p>",
                "<p>Stage Variables</p>",
                "<p>Mapping Templates</p>",
                "<p>Canary Deployment</p>"
            ],
            "question": "<p>You are a developer working on AWS Lambda functions that are triggered by Amazon API Gateway and would like to perform testing on a low volume of traffic for new API versions.</p>\n\n<p>Which of the following features will accomplish this task?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "You are a developer working on AWS Lambda functions that are triggered by Amazon API Gateway and would like to perform testing on a low volume of traffic for new API versions.\n\nWhich of the following features will accomplish this task?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334354,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>It should be accessible by one admin only after enabling Multi-factor authentication</strong></p>\n\n<p>AWS Root Account Security Best Practices:\n<img src=\"\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a></p>\n\n<p>If you continue to use the root user credentials, we recommend that you follow the security best practice to enable multi-factor authentication (MFA) for your account. Because your root user can perform sensitive operations in your account, adding a layer of authentication helps you to better secure your account. Multiple types of MFA are available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It should be accessible by 3 to 6 members of the IT team</strong> - Only the owner of the AWS account should have access to the root account credentials. You should create an IT group with admin permissions via IAM and then assign a few users to this group.</p>\n\n<p><strong>It should be accessible using the access key id and secret access key</strong> - AWS recommends that you should not use the access key id and secret access key for the AWS account root user.</p>\n\n<p><strong>It should be accessible by no one, throw away the passwords after creating the account</strong> - You will still need to store the password somewhere for your root account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>It should be accessible by no one, throw away the passwords after creating the account</p>",
                "<p>It should be accessible by 3 to 6 members of the IT team</p>",
                "<p>It should be accessible using the access key id and secret access key</p>",
                "<p>It should be accessible by one admin only after enabling Multi-factor authentication</p>"
            ],
            "question": "<p>When your company first created an AWS account, you began with a single sign-in principal called a root user account that had complete access to all AWS services and resources.</p>\n\n<p>What should you do to adhere to best practices for using the root user account?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "When your company first created an AWS account, you began with a single sign-in principal called a root user account that had complete access to all AWS services and resources.\n\nWhat should you do to adhere to best practices for using the root user account?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334356,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct answer</p>\n\n<p><strong>Create a bucket policy</strong></p>\n\n<p>Bucket policy is an access policy option available for you to grant permission to your Amazon S3 resources. It uses JSON-based access policy language.</p>\n\n<p>If you want to configure an existing bucket as a static website that has public access, you must edit block public access settings for that bucket. You may also have to edit your account-level block public access settings.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p>Incorrect:</p>\n\n<p><strong>Create an IAM role</strong> - This will not help because IAM roles are attached to services and in this case, we have public users.</p>\n\n<p><strong>Enable CORS</strong> - CORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. Here we are not dealing with cross domains.</p>\n\n<p><strong>Enable Encryption</strong> - For the most part, encryption does not have an effect on access denied/forbidden errors. On the website endpoint, if a user requests an object that doesn't exist, Amazon S3 returns HTTP response code 404 (Not Found). If the object exists but you haven't granted read permission on it, the website endpoint returns HTTP response code 403 (Access Denied).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html</a></p>\n",
            "question": "<p>Your company is new to cloud computing and would like to host a static HTML5 website on the cloud and be able to access it via domain www.mycompany.com. You have created a bucket in Amazon Simple Storage Service (S3), enabled website hosting, and set the index.html as the default page. Finally, you create an Alias record in Amazon Route 53 that points to the S3 website endpoint of your S3 bucket.</p>\n\n<p>When you test the domain www.mycompany.com you get the following error: 'HTTP response code 403 (Access Denied)'. What can you do to resolve this error?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Enable CORS</p>",
                "<p>Create an IAM role</p>",
                "<p>Create a bucket policy</p>",
                "<p>Enable Encryption</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "Your company is new to cloud computing and would like to host a static HTML5 website on the cloud and be able to access it via domain www.mycompany.com. You have created a bucket in Amazon Simple Storage Service (S3), enabled website hosting, and set the index.html as the default page. Finally, you create an Alias record in Amazon Route 53 that points to the S3 website endpoint of your S3 bucket.\n\nWhen you test the domain www.mycompany.com you get the following error: 'HTTP response code 403 (Access Denied)'. What can you do to resolve this error?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334358,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Specify a KMS key to use</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.</p>\n\n<p>For AWS CodeBuild to encrypt its build output artifacts, it needs access to an AWS KMS customer master key (CMK). By default, AWS CodeBuild uses the AWS-managed CMK for Amazon S3 in your AWS account. The following environment variable provides these details:</p>\n\n<p>CODEBUILD_KMS_KEY_ID: The identifier of the AWS KMS key that CodeBuild is using to encrypt the build output artifact (for example, arn:aws:kms:region-ID:account-ID:key/key-ID or alias/key-alias).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an AWS Lambda Hook</strong> - Code hook is used for integration with Lambda and is not relevant for the given use-case.</p>\n\n<p><strong>Use the AWS Encryption SDK</strong> - The SDK just makes it easier for you to implement encryption best practices in your application and is not relevant for the given use-case.</p>\n\n<p><strong>Use In-Flight encryption (SSL)</strong> - SSL is usually for internet traffic which in this case will be using internal traffic through AWS and is not relevant for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Specify a KMS key to use</p>",
                "<p>Use an AWS Lambda Hook</p>",
                "<p>Use the AWS Encryption SDK</p>",
                "<p>Use In Flight encryption (SSL)</p>"
            ],
            "question": "<p>You were assigned to a project that requires the use of the AWS CLI to build a project with AWS CodeBuild. Your project's root directory includes the buildspec.yml file to run build commands and would like your build artifacts to be automatically encrypted at the end.</p>\n\n<p>How should you configure CodeBuild to accomplish this?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "You were assigned to a project that requires the use of the AWS CLI to build a project with AWS CodeBuild. Your project's root directory includes the buildspec.yml file to run build commands and would like your build artifacts to be automatically encrypted at the end.\n\nHow should you configure CodeBuild to accomplish this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334360,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups</strong></p>\n\n<p>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance.</p>\n\n<p>Check the security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM Roles</strong> - Usually you run into issues with authorization of APIs with roles but not for timeout, so this option does not fit the given use-case.</p>\n\n<p><strong>The application is down</strong> - Although you can set a health check for application ping or HTTP, timeouts are usually caused by blocked firewall access.</p>\n\n<p><strong>The ALB is warming up</strong> - ALB has a slow start mode which allows a warm-up period before being able to respond to requests with optimal performance. So this is not the issue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout</a></p>\n",
            "question": "<p>You are responsible for an application that runs on multiple Amazon EC2 instances. In front of the instances is an Internet-facing load balancer that takes requests from clients over the internet and distributes them to the EC2 instances. A health check is configured to ping the index.html page found in the root directory for the health status. When accessing the website via the internet visitors of the website receive timeout errors.</p>\n\n<p>What should be checked first to resolve the issue?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Security Groups</p>",
                "<p>IAM Roles</p>",
                "<p>The application is down</p>",
                "<p>The ALB is warming up</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You are responsible for an application that runs on multiple Amazon EC2 instances. In front of the instances is an Internet-facing load balancer that takes requests from clients over the internet and distributes them to the EC2 instances. A health check is configured to ping the index.html page found in the root directory for the health status. When accessing the website via the internet visitors of the website receive timeout errors.\n\nWhat should be checked first to resolve the issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334362,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects.</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
            "question": "<p>A security company is requiring all developers to perform server-side encryption with customer-provided encryption keys when performing operations in AWS S3. Developers should write software with C# using the AWS SDK and implement the requirement in the PUT, GET, Head, and Copy operations.</p>\n\n<p>Which of the following encryption methods meets this requirement?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Client-Side Encryption</p>",
                "<p>SSE-KMS</p>",
                "<p>SSE-C</p>",
                "<p>SSE-S3</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "A security company is requiring all developers to perform server-side encryption with customer-provided encryption keys when performing operations in AWS S3. Developers should write software with C# using the AWS SDK and implement the requirement in the PUT, GET, Head, and Copy operations.\n\nWhich of the following encryption methods meets this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334364,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</p>",
                "<p>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</p>",
                "<p>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</p>",
                "<p>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.</p>\n\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p>\n\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.</p>\n\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.</p>\n\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor.</p>\n\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.</p>\n\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a></p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A company's e-commerce application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The development team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.</p>\n\n<p>As a developer associate, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company's e-commerce application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The development team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.\n\nAs a developer associate, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334366,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>DynamoDB</p>\n\n<p>You should note that S3 now supports both gateway endpoints as well as the interface endpoints.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong></p>\n\n<p><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong></p>\n\n<p>DynamoDB does not support interface endpoints, so these two options are incorrect.</p>\n\n<p><strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</strong> - There is no such thing as an API endpoint for S3. API endpoints are used with AWS API Gateway. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p>\n",
            "answers": [
                "<p>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</p>",
                "<p>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
                "<p>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</p>",
                "<p>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.\n\nHow would you go about providing private access to these AWS resources which are not part of this custom VPC?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334368,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A company ingests real-time data into its on-premises data center and subsequently a daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.</p>\n\n<p>Which of the following is the fastest way to upload the daily compressed file into S3?</p>\n",
            "answers": [
                "<p>Upload the compressed file in a single operation</p>",
                "<p>Upload the compressed file using multipart upload</p>",
                "<p>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</p>",
                "<p>Upload the compressed file using multipart upload with S3 transfer acceleration</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong></p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p><strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p><strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company ingests real-time data into its on-premises data center and subsequently a daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.\n\nWhich of the following is the fastest way to upload the daily compressed file into S3?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334370,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A developer has created a new Application Load Balancer but has not registered any targets with the target groups.</p>\n\n<p>Which of the following errors would be generated by the Load Balancer?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503: Service unavailable</strong></p>\n\n<p>The Load Balancer generates the <code>HTTP 503: Service unavailable</code> error when the target groups for the load balancer have no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500: Internal server error</strong></p>\n\n<p><strong>HTTP 502: Bad gateway</strong></p>\n\n<p><strong>HTTP 504: Gateway timeout</strong></p>\n\n<p>Here is a summary of the possible causes for these error types:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
            "answers": [
                "<p>HTTP 503: Service unavailable</p>",
                "<p>HTTP 500: Internal server error</p>",
                "<p>HTTP 502: Bad gateway</p>",
                "<p>HTTP 504: Gateway timeout</p>"
            ],
            "relatedLectureIds": ""
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer has created a new Application Load Balancer but has not registered any targets with the target groups.\n\nWhich of the following errors would be generated by the Load Balancer?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334372,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A company has recently launched a new gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The development team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.</p>\n\n<p>As a developer associate, which of the following solutions would you recommend so that it requires minimum development effort to address this requirement?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable storage auto-scaling for RDS MySQL</strong></p>\n\n<p>If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:</p>\n\n<p>Free available space is less than 10 percent of the allocated storage.</p>\n\n<p>The low-storage condition lasts at least five minutes.</p>\n\n<p>At least six hours have passed since the last storage modification.</p>\n\n<p>The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your application’s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "answers": [
                "<p>Enable storage auto-scaling for RDS MySQL</p>",
                "<p>Migrate RDS MySQL database to Aurora which offers storage auto-scaling</p>",
                "<p>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</p>",
                "<p>Create read replica for RDS MySQL</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A company has recently launched a new gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The development team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.\n\nAs a developer associate, which of the following solutions would you recommend so that it requires minimum development effort to address this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334374,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it's much more involved from a development perspective. For the given use-case, you should use DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
                "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
                "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
                "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>"
            ],
            "question": "<p>A financial services company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon analyzing the usage pattern, it's found that 80% of the read requests are shared across all users.</p>\n\n<p>As a Developer Associate, how can you improve the application performance while optimizing the cost with the least development effort?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A financial services company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon analyzing the usage pattern, it's found that 80% of the read requests are shared across all users.\n\nAs a Developer Associate, how can you improve the application performance while optimizing the cost with the least development effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334376,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p>Broadly, you can set up two types of caching strategies:</p>\n\n<ol>\n<li><p>Lazy Loading</p></li>\n<li><p>Write-Through</p></li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p><strong>Use a caching strategy to write to the backend first and then invalidate the cache</strong></p>\n\n<p>This option is similar to the write-through strategy wherein the application writes to the backend first and then invalidate the cache. As the cache gets invalidated, the caching engine would then fetch the latest value from the backend, thereby making sure that the product prices and product description stay consistent with the backend.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a caching strategy to update the cache and the backend at the same time</strong> - The cache and the backend cannot be updated at the same time via a single atomic operation as these are two separate systems. Therefore this option is incorrect.</p>\n\n<p><strong>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</strong> - This strategy could work if the TTL is really short. However, for the duration of this TTL, the cache would be out of sync with the backend, hence this option is not correct for the given use-case.</p>\n\n<p><strong>Use a caching strategy to write to the cache directly and sync the backend at a later time</strong> - This option is given as a distractor as this strategy is not viable to address the given use-case. The product prices and description on the cache must always stay consistent with the backend. You cannot sync the backend at a later time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
            "question": "<p>The development team at an e-commerce company is preparing for the upcoming Thanksgiving sale. The product manager wants the development team to implement appropriate caching strategy on Amazon ElastiCache to withstand traffic spikes on the website during the sale. A key requirement is to facilitate consistent updates to the product prices and product description, so that the cache never goes out of sync with the backend.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use a caching strategy to update the cache and the backend at the same time</p>",
                "<p>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</p>",
                "<p>Use a caching strategy to write to the cache directly and sync the backend at a later time</p>",
                "<p>Use a caching strategy to write to the backend first and then invalidate the cache</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at an e-commerce company is preparing for the upcoming Thanksgiving sale. The product manager wants the development team to implement appropriate caching strategy on Amazon ElastiCache to withstand traffic spikes on the website during the sale. A key requirement is to facilitate consistent updates to the product prices and product description, so that the cache never goes out of sync with the backend.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334378,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Query the metadata at http://169.254.169.254/latest/meta-data</strong> - Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. To view all categories of instance metadata from within a running instance, use the following URI - <code>http://169.254.169.254/latest/meta-data/</code>. The IP address 169.254.169.254 is a link-local address and is valid only from the instance. All instance metadata is returned as text (HTTP content type text/plain).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</strong> - The AWS CLI has a describe-instances API call needs instance ID as an input. So, this will not work for the current use case wherein we do not know the instance ID.</p>\n\n<p><strong>Query the user data at http://169.254.169.254/latest/user-data</strong> - This address retrieves the user data that you specified when launching your instance.</p>\n\n<p><strong>Query the user data at http://254.169.254.169/latest/meta-data</strong> - The IP address specified is wrong.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href=\"https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html\">https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</p>",
                "<p>Query the user data at http://169.254.169.254/latest/user-data</p>",
                "<p>Query the metadata at http://169.254.169.254/latest/meta-data</p>",
                "<p>Query the user data at http://254.169.254.169/latest/meta-data</p>"
            ],
            "question": "<p>You are working with a t2.small instance bastion host that has the AWS CLI installed to help manage all the AWS services installed on it. You would like to know the security group and the instance id of the current instance.</p>\n\n<p>Which of the following will help you fetch the needed information?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "You are working with a t2.small instance bastion host that has the AWS CLI installed to help manage all the AWS services installed on it. You would like to know the security group and the instance id of the current instance.\n\nWhich of the following will help you fetch the needed information?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334380,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB DAX</strong></p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB Streams</strong> - A stream record contains information about a data modification to a single item in a DynamoDB table. This is not the correct option for the given use-case.</p>\n\n<p><strong>ElastiCache</strong> - ElastiCache can cache the results from anything but you will need to modify your code to check the cache before querying the main query store. As the given use-case mandates minimal effort, so this option is not correct.</p>\n\n<p><strong>More partitions</strong> - This option has been added as a distractor as DynamoDB handles that for you automatically.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>More partitions</p>",
                "<p>DynamoDB Streams</p>",
                "<p>ElastiCache</p>",
                "<p>DynamoDB DAX</p>"
            ],
            "question": "<p>A popular mobile app retrieves data from an AWS DynamoDB table that was provisioned with read-capacity units (RCU’s) that are evenly shared across four partitions. One of those partitions is receiving more traffic than the other partitions, causing hot partition issues.</p>\n\n<p>What technology will allow you to reduce the read traffic on your AWS DynamoDB table with minimal effort?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A popular mobile app retrieves data from an AWS DynamoDB table that was provisioned with read-capacity units (RCU’s) that are evenly shared across four partitions. One of those partitions is receiving more traffic than the other partitions, causing hot partition issues.\n\nWhat technology will allow you to reduce the read traffic on your AWS DynamoDB table with minimal effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 43334382,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>!ImportValue</code></strong></p>\n\n<p>The intrinsic function <code>Fn::ImportValue</code> returns the value of an output exported by another stack. You typically use this function to create cross-stack references.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Ref</code></strong> - Returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!GetAtt</code></strong> - Returns the value of an attribute from a resource in the template.</p>\n\n<p><strong><code>!Sub</code></strong> - Substitutes variables in an input string with values that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p><code>!Ref</code></p>",
                "<p><code>!ImportValue</code></p>",
                "<p><code>!GetAtt</code></p>",
                "<p><code>!Sub</code></p>"
            ],
            "question": "<p>An IT company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a developer creates a stack called <code>NetworkStack</code> which will export the <code>subnetId</code> that can be used when creating EC2 instances in another stack.</p>\n\n<p>To use the exported value in another stack, which of the following functions must be used?</p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An IT company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a developer creates a stack called NetworkStack which will export the subnetId that can be used when creating EC2 instances in another stack.\n\nTo use the exported value in another stack, which of the following functions must be used?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727566,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role in account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</strong></p>\n\n<p>You can give a Lambda function created in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call.</p>\n\n<p>Sample use-case to configure a Lambda function to assume a role from another AWS account:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</strong> - Creating a clone of the Lambda function is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</strong> - You cannot attach a resource policy to a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</strong> - As mentioned in the explanation above, you need to modify the trust policy of the IAM role in Account B so that it allows the execution role of Lambda function in account A to assume the IAM role in Account B.</p>\n\n<p>Reference:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>The development team at a retail organization wants to allow a Lambda function in its AWS Account A to access a DynamoDB table in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</p>",
                "<p>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</p>",
                "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</p>",
                "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a retail organization wants to allow a Lambda function in its AWS Account A to access a DynamoDB table in another AWS Account B.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727568,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>The development team at an e-commerce company wants to run a serverless data store service on two docker containers that share resources.</p>\n\n<p>Which of the following ECS configurations can be used to facilitate this use-case?</p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Put the two containers into a single task definition using a Fargate Launch Type</p>",
                "<p>Put the two containers into two separate task definitions using a Fargate Launch Type</p>",
                "<p>Put the two containers into two separate task definitions using an EC2 Launch Type</p>",
                "<p>Put the two containers into a single task definition using an EC2 Launch Type</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Put the two containers into a single task definition using a Fargate Launch Type</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>As the development team is looking for a serverless data store service, therefore the two containers should be launched into a single task definition using a Fargate Launch Type. Using a single task definition allows the two containers to share resources. Please see these use-cases for Fargate Launch type when you should put multiple containers into the same task definition:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a></p>\n\n<p>For a deep-dive on understanding how Amazon ECS manages CPU and memory resources, please review this excellent blog-\n<a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Put the two containers into two separate task definitions using a Fargate Launch Type</strong> - This option contradicts the details provided in the explanation above, so this option is ruled out.</p>\n\n<p><strong>Put the two containers into two separate task definitions using an EC2 Launch Type</strong></p>\n\n<p><strong>Put the two containers into a single task definition using an EC2 Launch Type</strong></p>\n\n<p>As the development team is looking for a serverless data store service, therefore EC2 Launch Type is ruled out. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Deployment",
        "question_plain": "The development team at an e-commerce company wants to run a serverless data store service on two docker containers that share resources.\n\nWhich of the following ECS configurations can be used to facilitate this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727570,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets.</p>\n\n<p>To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.</p>\n\n<p>You cannot use the Ref intrinsic function to import the value.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
                "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>",
                "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
                "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>"
            ],
            "question": "<p>The development team at an IT company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "The development team at an IT company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727572,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A photo-sharing application manages its EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The development team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case with minimal development effort?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</p>",
                "<p>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</p>",
                "<p>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</p>",
                "<p>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Please make sure that you adhere to the following configurations while using CloudFront distribution in front of your Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/\">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a></p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A photo-sharing application manages its EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The development team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.\n\nAs a Developer Associate, which of the following solutions would you recommend to address this use-case with minimal development effort?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727574,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "answers": [
                "<p>Use ChangeMessageVisibility action to extend a message's visibility timeout</p>",
                "<p>Use DelaySeconds action to delay a message's visibility timeout</p>",
                "<p>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</p>",
                "<p>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</p>"
            ],
            "question": "<p>A video encoding application running on an EC2 instance takes about 20 seconds on average to process each raw footage file. The application picks the new job messages from an SQS queue. The development team needs to account for the use-case when the video encoding process takes longer than usual so that the same raw footage is not processed by multiple consumers.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ChangeMessageVisibility action to extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DelaySeconds action to delay a message's visibility timeout</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. To set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. You cannot use DelaySeconds to alter the visibility of a message which has been picked for processing.</p>\n\n<p><strong>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</strong></p>\n\n<p><strong>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. Both these options have been added as distractors as WaitTimeSeconds (via short polling or long polling) cannot be used to influence the message's visibility.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html</a></p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A video encoding application running on an EC2 instance takes about 20 seconds on average to process each raw footage file. The application picks the new job messages from an SQS queue. The development team needs to account for the use-case when the video encoding process takes longer than usual so that the same raw footage is not processed by multiple consumers.\n\nAs a Developer Associate, which of the following solutions would you recommend to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727576,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>The development team at an IT company has configured an Application Load Balancer (ALB) with a Lambda function A as the target but the Lambda function A is not able to process any request from the ALB. Upon investigation, the team finds that there is another Lambda function B in the AWS account that is exceeding the concurrency limits.</p>\n\n<p>How can the development team address this issue?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>Please review this note to understand how reserved concurrency works:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a></p>\n\n<p>Therefore using reserved concurrency for Lambda function B would limit its maximum concurrency and allow Lambda function A to execute without getting throttled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong> - You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function, so this option is incorrect.</p>\n\n<p><strong>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</strong> - This has been added as a distractor as using an API Gateway for Lambda function A has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p><strong>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</strong> - When you associate a CloudFront distribution with a Lambda function (known as Lambda@Edge), CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Again, this has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/</a></p>\n",
            "answers": [
                "<p>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
                "<p>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
                "<p>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</p>",
                "<p>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at an IT company has configured an Application Load Balancer (ALB) with a Lambda function A as the target but the Lambda function A is not able to process any request from the ALB. Upon investigation, the team finds that there is another Lambda function B in the AWS account that is exceeding the concurrency limits.\n\nHow can the development team address this issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727578,
        "assessment_type": "multi-select",
        "prompt": {
            "question": "<p>An e-commerce company has a fleet of EC2 based web servers running into very high CPU utilization issues. The development team has determined that serving secure traffic via HTTPS is a major contributor to the high CPU load.</p>\n\n<p>Which of the following steps can take the high CPU load off the web servers? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "answers": [
                "<p>Create an HTTP listener on the Application Load Balancer with SSL termination</p>",
                "<p>Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)</p>",
                "<p>Create an HTTPS listener on the Application Load Balancer with SSL termination</p>",
                "<p>Create an HTTPS listener on the Application Load Balancer with SSL pass-through</p>",
                "<p>Create an HTTP listener on the Application Load Balancer with SSL pass-through</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)\"</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances.</p>\n\n<p>Please review this resource to understand how to associate an ACM SSL/TLS certificate with an Application Load Balancer:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL pass-through\" - If you use an HTTPS listener with SSL pass-through, then the EC2 instances would continue to be under heavy CPU load as they would still need to decrypt the secure traffic\nat the instance level. Hence this option is incorrect.</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL pass-through\"</p>\n\n<p>You cannot have an HTTP listener for an Application Load Balancer to support SSL termination or SSL pass-through, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n"
        },
        "correct_response": [
            "b",
            "c"
        ],
        "section": "Security",
        "question_plain": "An e-commerce company has a fleet of EC2 based web servers running into very high CPU utilization issues. The development team has determined that serving secure traffic via HTTPS is a major contributor to the high CPU load.\n\nWhich of the following steps can take the high CPU load off the web servers? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727580,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p><strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
            "answers": [
                "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
                "<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</p>",
                "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
                "<p>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</p>"
            ],
            "question": "<p>An IT company has a HealthCare application with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption keys.</p>\n\n<p>Which of the following S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?</p>\n",
            "relatedLectureIds": ""
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "An IT company has a HealthCare application with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption keys.\n\nWhich of the following S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727582,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>Exam alert:</p>\n\n<p>Please remember that Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>AWS Kinesis Data Streams</p>",
                "<p>AWS Kinesis Data Firehose</p>",
                "<p>AWS Kinesis Data Analytics</p>",
                "<p>Amazon SQS</p>"
            ],
            "question": "<p>A data analytics company wants to use clickstream data for Machine Learning tasks, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these business units works independently and would need real-time access to this clickstream data for their applications.</p>\n\n<p>As a Developer Associate, which of the following AWS services would you recommend such that it provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a simultaneous feed of the data stream to the consumer applications?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A data analytics company wants to use clickstream data for Machine Learning tasks, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these business units works independently and would need real-time access to this clickstream data for their applications.\n\nAs a Developer Associate, which of the following AWS services would you recommend such that it provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a simultaneous feed of the data stream to the consumer applications?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727584,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>Deploy the new application version using 'All at once' deployment policy</p>",
                "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
                "<p>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</p>",
                "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>"
            ],
            "question": "<p>A retail company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case deployment goes wrong.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to the development team?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong></p>\n\n<p>With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A retail company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case deployment goes wrong.\n\nAs a Developer Associate, which of the following options would you recommend to the development team?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727586,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p><code>lead_actor_name</code></p>",
                "<p><code>producer_name</code></p>",
                "<p><code>movie_id</code></p>",
                "<p><code>movie_language</code></p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB).</p>\n\n<p>DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.</p>\n\n<p>Please see these details for the DynamoDB Partition Keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n\n<p><strong><code>movie_id</code></strong></p>\n\n<p>The <code>movie_id</code> attribute has high-cardinality across the entire collection of the movie database, hence it is the most suitable candidate for the partition key in this use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>producer_name</code></strong>  - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>lead_actor_name</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>movie_language</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n",
            "question": "<p>You're a developer for 'Movie Gallery', a company that just migrated to the cloud. A database must be created using NoSQL technology to hold movies that are listed for public viewing. You are taking an important step in designing the database with DynamoDB and need to choose the appropriate partition key.</p>\n\n<p>Which of the following unique attributes satisfies this requirement?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You're a developer for 'Movie Gallery', a company that just migrated to the cloud. A database must be created using NoSQL technology to hold movies that are listed for public viewing. You are taking an important step in designing the database with DynamoDB and need to choose the appropriate partition key.\n\nWhich of the following unique attributes satisfies this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727588,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></p>",
                "<p><code>ECS_ENGINE_AUTH_DATA</code></p>",
                "<p><code>ECS_ENABLE_TASK_IAM_ROLE</code></p>",
                "<p><code>ECS_CLUSTER</code></p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong><code>ECS_ENABLE_TASK_IAM_ROLE</code></strong></p>\n\n<p>This configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>ECS_ENGINE_AUTH_DATA</code></strong> - This refers to the authentication data within a Docker configuration file, so this is not the correct option.</p>\n\n<p><strong><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></strong> - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver.</p>\n\n<p><strong><code>ECS_CLUSTER</code></strong> - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html</a></p>\n",
            "question": "<p>A company wants to automate the creation of ECS clusters using CloudFormation. The process has worked for a while, but after creating task definitions and assigning roles, the development team discovers that the tasks for containers are not using the permissions assigned to them.</p>\n\n<p>Which ECS config must be set in <code>/etc/ecs/ecs.config</code> to allow ECS tasks to use IAM roles?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A company wants to automate the creation of ECS clusters using CloudFormation. The process has worked for a while, but after creating task definitions and assigning roles, the development team discovers that the tasks for containers are not using the permissions assigned to them.\n\nWhich ECS config must be set in /etc/ecs/ecs.config to allow ECS tasks to use IAM roles?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727590,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "question": "<p>As an AWS Certified Developer Associate, you are writing a CloudFormation template in YAML. The template consists of an EC2 instance creation and one RDS resource. Once your resources are created you would like to output the connection endpoint for the RDS database.</p>\n\n<p>Which intrinsic function returns the value needed?</p>\n",
            "answers": [
                "<p><code>!Sub</code></p>",
                "<p><code>!Ref</code></p>",
                "<p><code>!GetAtt</code></p>",
                "<p><code>!FindInMap</code></p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation provides several built-in functions that help you manage your stacks. Intrinsic functions are used in templates to assign values to properties that are not available until runtime.</p>\n\n<p><strong><code>!GetAtt</code></strong> - The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. This example snippet returns a string containing the DNS name of the load balancer with the logical name myELB -\nYML :   !GetAtt myELB.DNSName\nJSON :   \"Fn::GetAtt\" : [ \"myELB\" , \"DNSName\" ]</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Sub</code></strong> - The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren't available until you create or update a stack.</p>\n\n<p><strong><code>!Ref</code></strong> - The intrinsic function Ref returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!FindInMap</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. For example, you can use this in the Mappings section that contains a single map, RegionMap, that associates AMIs with AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a></p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As an AWS Certified Developer Associate, you are writing a CloudFormation template in YAML. The template consists of an EC2 instance creation and one RDS resource. Once your resources are created you would like to output the connection endpoint for the RDS database.\n\nWhich intrinsic function returns the value needed?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727592,
        "assessment_type": "multi-select",
        "prompt": {
            "question": "<p>You are a developer working at a cloud company that embraces serverless. You have performed your initial deployment and would like to work towards adding API Gateway stages and associate them with existing deployments. Your stages will include prod, test, and dev and will need to match a Lambda function variant that can be updated over time.</p>\n\n<p>Which of the following features must you add to achieve this? (select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Lambda X-Ray integration</p>",
                "<p>Stage Variables</p>",
                "<p>Lambda Versions</p>",
                "<p>Lambda Aliases</p>",
                "<p>Mapping Templates</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>Stage Variables</strong></p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com).</p>\n\n<p><strong>Lambda Aliases</strong></p>\n\n<p>A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p>\n\n<p>Lambda Aliases allow you to create a \"mutable\" Lambda version that points to whatever version you want in the backend. This allows you to have a \"dev\", \"test\", prod\" Lambda alias that can remain stable over time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda Versions</strong> - Versions are immutable and cannot be updated over time. So this option is not correct.</p>\n\n<p><strong>Lambda X-Ray integration</strong> - This is good for tracing and debugging requests so it can be looked at as a good option for troubleshooting issues in the future. This is not the right fit for the given use-case.</p>\n\n<p><strong>Mapping Templates</strong> - Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly, and override status codes returned by your integration endpoint. This is not the right fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n"
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a developer working at a cloud company that embraces serverless. You have performed your initial deployment and would like to work towards adding API Gateway stages and associate them with existing deployments. Your stages will include prod, test, and dev and will need to match a Lambda function variant that can be updated over time.\n\nWhich of the following features must you add to achieve this? (select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727594,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A company has AWS Lambda functions where each is invoked by other AWS services such as Amazon Kinesis Data Firehose, Amazon API Gateway, Amazon Simple Storage Service, or Amazon CloudWatch Events. What these Lambda functions have in common is that they process heavy workloads such as big data analysis, large file processing, and statistical computations.</p>\n\n<p>What should you do to improve the performance of your AWS Lambda functions without changing your code?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the RAM assigned to your Lambda function</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second). You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the instance type for your Lambda function</strong> - Instance types apply to the EC2 service and not to Lambda function as its a serverless service.</p>\n\n<p><strong>Change your Lambda function runtime to use Golang</strong> - This changes programming language which requires code changes, so this option is not correct. Besides, changing the runtime may not even address the performance issues.</p>\n\n<p><strong>Increase the Lambda function timeout</strong> - This option would increase the amount of time for which the Lambda function executes, which may help in case you have some heavy processing, but won't help with the actual performance of your Lambda function.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
            "answers": [
                "<p>Change the instance type for your Lambda function</p>",
                "<p>Increase the RAM assigned to your Lambda function</p>",
                "<p>Change your Lambda function runtime to use Golang</p>",
                "<p>Increase the Lambda function timeout</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company has AWS Lambda functions where each is invoked by other AWS services such as Amazon Kinesis Data Firehose, Amazon API Gateway, Amazon Simple Storage Service, or Amazon CloudWatch Events. What these Lambda functions have in common is that they process heavy workloads such as big data analysis, large file processing, and statistical computations.\n\nWhat should you do to improve the performance of your AWS Lambda functions without changing your code?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727596,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A cyber forensics application, running behind an ALB, wants to analyze patterns for the client IPs.</p>\n\n<p>Which of the following headers can be used for this requirement?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>HTTP requests and HTTP responses use header fields to send information about the HTTP messages. Header fields are colon-separated name-value pairs that are separated by a carriage return (CR) and a line feed (LF).</p>\n\n<p><strong>X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>X-Forwarded-Proto</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer. Your server access logs contain only the protocol used between the server and the load balancer; they contain no information about the protocol used between the client and the load balancer. To determine the protocol used between the client and the load balancer, use the X-Forwarded-Proto request header.</p>\n\n<p><strong>X-Forwarded-Port</strong> - The X-Forwarded-Port request header helps you identify the destination port that the client used to connect to the load balancer.</p>\n\n<p><strong>X-Forwarded-IP</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for</a></p>\n",
            "answers": [
                "<p>X-Forwarded-Proto</p>",
                "<p>X-Forwarded-Port</p>",
                "<p>X-Forwarded-IP</p>",
                "<p>X-Forwarded-For</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Security",
        "question_plain": "A cyber forensics application, running behind an ALB, wants to analyze patterns for the client IPs.\n\nWhich of the following headers can be used for this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727680,
        "assessment_type": "multi-select",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A new recruit is trying to understand the nuances of EC2 Auto Scaling. As an AWS Certified Developer Associate, you have been asked to mentor the new recruit.</p>\n\n<p>Can you identify and explain the correct statements about Auto Scaling to the new recruit? (Select two).</p>\n",
            "answers": [
                "<p>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</p>",
                "<p>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</p>",
                "<p>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</p>",
                "<p>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</p>",
                "<p>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.</p>\n\n<p><strong>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</strong> - A volume is attached to a new instance when it is added. Amazon EC2 Auto Scaling doesn't automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance.</p>\n\n<p><strong>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</strong> - Amazon EC2 Auto Scaling works with Application Load Balancers and Network Load Balancers including their health check feature.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</strong> - This is an incorrect statement. EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.</p>\n\n<p><strong>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</strong> - This is an incorrect statement. When you create an Auto Scaling group from an existing instance, it does not create a new AMI.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a></p>\n\n<p><strong>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</strong> - This is an incorrect statement. You don't have to use ELB to use Auto Scaling. You can use the EC2 health check to identify and replace unhealthy instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/autoscaling/faqs/\">https://aws.amazon.com/ec2/autoscaling/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a></p>\n"
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A new recruit is trying to understand the nuances of EC2 Auto Scaling. As an AWS Certified Developer Associate, you have been asked to mentor the new recruit.\n\nCan you identify and explain the correct statements about Auto Scaling to the new recruit? (Select two).",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727682,
        "assessment_type": "multi-select",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A company that specializes in cloud communications platform as a service allows software developers to programmatically use their services to send and receive text messages. The initial platform did not have a scalable architecture as all components were hosted on one server and should be redesigned for high availability and scalability.</p>\n\n<p>Which of the following options can be used to implement the new architecture? (select two)</p>\n",
            "answers": [
                "<p>ALB + ECS</p>",
                "<p>EBS + RDS</p>",
                "<p>SES + S3</p>",
                "<p>CloudWatch + CloudFront</p>",
                "<p>API Gateway + Lambda</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>ALB + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (ECS) is a highly scalable, high-performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances.</p>\n\n<p>How ECS Works:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>When you use ECS with a load balancer such as ALB deployed across multiple Availability Zones, it helps provide a scalable and highly available REST API.</p>\n\n<p><strong>API Gateway + Lambda</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. Using API Gateway, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as EC2 or Lambda functions.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway and Lambda help achieve the same purpose integrating some capabilities such as authentication in a serverless fashion, with fully scalable and highly available architectures.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SES + S3</strong> - The combination of these services only provide email and object storage services.</p>\n\n<p><strong>CloudWatch + CloudFront</strong> - The combination of these services only provide monitoring and fast content delivery network (CDN) services.</p>\n\n<p><strong>EBS + RDS</strong> - The combination of these services only provide elastic block storage and database services.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/\">https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/\">https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/</a></p>\n"
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company that specializes in cloud communications platform as a service allows software developers to programmatically use their services to send and receive text messages. The initial platform did not have a scalable architecture as all components were hosted on one server and should be redesigned for high availability and scalability.\n\nWhich of the following options can be used to implement the new architecture? (select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727684,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>As an AWS Certified Developer Associate, you have been hired to consult with a company that uses the NoSQL database for mobile applications. The developers are using DynamoDB to perform operations such as <code>GetItem</code> but are limited in knowledge. They would like to be more efficient with retrieving some attributes rather than all.</p>\n\n<p>Which of the following recommendations would you provide?</p>\n",
            "answers": [
                "<p>Use a <code>FilterExpression</code></p>",
                "<p>Specify a <code>ProjectionExpression</code></p>",
                "<p>Use the <code>--query</code> parameter</p>",
                "<p>Use a <code>Scan</code></p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Specify a <code>ProjectionExpression</code></strong>: A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a <code>FilterExpression</code></strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present. A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.</p>\n\n<p><strong>Use the <code>--query</code> parameter</strong> - The Query operation in Amazon DynamoDB finds items based on primary key values. You must provide the name of the partition key attribute and a single value for that attribute. The Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results.</p>\n\n<p><strong>Use a <code>Scan</code></strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. You can also use the ProjectionExpression parameter so that Scan only returns some of the attributes, rather than all of them.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n"
        },
        "correct_response": [
            "b"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As an AWS Certified Developer Associate, you have been hired to consult with a company that uses the NoSQL database for mobile applications. The developers are using DynamoDB to perform operations such as GetItem but are limited in knowledge. They would like to be more efficient with retrieving some attributes rather than all.\n\nWhich of the following recommendations would you provide?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727686,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "answers": [
                "<p>16KB</p>",
                "<p>1MB</p>",
                "<p>10MB</p>",
                "<p>4KB</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>4 KB</strong></p>\n\n<p>You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information.</p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>10MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>16KB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
            "question": "<p>A developer at a university is encrypting a large XML payload transferred over the network using AWS KMS and wants to test the application before going to production.</p>\n\n<p>What is the maximum data size supported by AWS KMS?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A developer at a university is encrypting a large XML payload transferred over the network using AWS KMS and wants to test the application before going to production.\n\nWhat is the maximum data size supported by AWS KMS?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727688,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>As a Developer, you are working on a mobile application that utilizes Amazon Simple Queue Service (SQS) for sending messages to downstream systems for further processing. One of the requirements is that the messages should be stored in the queue for a period of 12 days.</p>\n\n<p>How will you configure this requirement?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the queue message retention setting</strong> - Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Long Polling for the SQS queue</strong> - Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). This feature is not useful for the current use case.</p>\n\n<p><strong>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</strong> - This is an incorrect statement. Retention period of up to 14 days is possible.</p>\n\n<p><strong>Use a FIFO SQS queue</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. This is not useful for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Enable Long Polling for the SQS queue</p>",
                "<p>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</p>",
                "<p>Change the queue message retention setting</p>",
                "<p>Use a FIFO SQS queue</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "As a Developer, you are working on a mobile application that utilizes Amazon Simple Queue Service (SQS) for sending messages to downstream systems for further processing. One of the requirements is that the messages should be stored in the queue for a period of 12 days.\n\nHow will you configure this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727690,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use CloudTrail service</p>",
                "<p>Use API Gateway service</p>",
                "<p>Use CloudWatch service</p>",
                "<p>Use X-Ray service</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "question": "<p>A company uses microservices-based infrastructure to process the API calls from clients, perform request filtering and cache requests using the AWS API Gateway. Users report receiving 501 error code and you have been contacted to find out what is failing.</p>\n\n<p>Which service will you choose to help you troubleshoot?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use X-Ray service</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p>\n\n<p>X-Ray Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>AWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. You can create dependency trees, perform cross-availability zone or region call detections, and more.</p>\n\n<p>X-Ray Service maps:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>X-Ray Traces:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail service</strong> - With CloudTrail, you can get a history of AWS API calls for your account - including API calls made via the AWS Management Console, AWS SDKs, command-line tools, and higher-level AWS services (such as AWS CloudFormation). This is a very useful service for general monitoring and tracking. But, it will not give a detailed analysis of the outcome of microservices or drill into specific issues. For the current use case, X-Ray offers a better solution.</p>\n\n<p><strong>Use API Gateway service</strong> - Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway will not be able to drill into the flow between different microservices or their issues.</p>\n\n<p><strong>Use CloudWatch service</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it can't help you debug microservices specific issues on AWS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/\">https://aws.amazon.com/cloudwatch/features/</a></p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company uses microservices-based infrastructure to process the API calls from clients, perform request filtering and cache requests using the AWS API Gateway. Users report receiving 501 error code and you have been contacted to find out what is failing.\n\nWhich service will you choose to help you troubleshoot?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727692,
        "assessment_type": "multi-select",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</strong> - You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><strong>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted</strong> - By default, the CMK that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</strong> - This is an incorrect statement. There is no direct way to encrypt an existing unencrypted volume or snapshot. You can encrypt an unencrypted snapshot by copying and enabling encryption while copying the snapshot. To encrypt an EBS volume, you need to create a snapshot and then encrypt the snapshot as described earlier. From this new encrypted snapshot, you can then create an encrypted volume.</p>\n\n<p><strong>A snapshot of an encrypted volume can be encrypted or unencrypted</strong> - This is an incorrect statement. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><strong>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</strong> - This is an incorrect statement. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted</a></p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A Company uses a large set of EBS volumes for their fleet of Amazon EC2 instances. As an AWS Certified Developer Associate, your help has been requested to understand the security features of the EBS volumes. The company does not want to build or maintain their own encryption key management infrastructure.</p>\n\n<p>Can you help them understand what works for Amazon EBS encryption? (Select two)</p>\n",
            "answers": [
                "<p>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</p>",
                "<p>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</p>",
                "<p>A snapshot of an encrypted volume can be encrypted or unencrypted</p>",
                "<p>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted</p>",
                "<p>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</p>"
            ]
        },
        "correct_response": [
            "a",
            "d"
        ],
        "section": "Security",
        "question_plain": "A Company uses a large set of EBS volumes for their fleet of Amazon EC2 instances. As an AWS Certified Developer Associate, your help has been requested to understand the security features of the EBS volumes. The company does not want to build or maintain their own encryption key management infrastructure.\n\nCan you help them understand what works for Amazon EBS encryption? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727694,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A multi-national company runs its technology operations on AWS Cloud. As part of their storage solution, they use a large number of EBS volumes, with AWS Config and CloudTrail activated. A manager has tried to find the user name that created an EBS volume by searching CloudTrail events logs but wasn't successful.</p>\n\n<p>As a Developer Associate, which of the following would you recommend as the correct solution?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon Elastic Compute Cloud (Amazon EC2) launch.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - Event 'ManageVolume' is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Amazon EBS CloudWatch metrics are disabled</strong> - Amazon Elastic Block Store (Amazon EBS) sends data points to CloudWatch for several metrics. Data is only reported to CloudWatch when the volume is attached to an instance. CloudWatch metrics are useful in tracking the status or life cycle changes of an EBS volume, they are not useful in knowing about the metadata of EBS volumes.</p>\n\n<p><strong>EBS volume status checks are disabled</strong> - Volume status checks enable you to better understand, track and manage potential inconsistencies in the data on an Amazon EBS volume. They are designed to provide you with the information that you need to determine whether your Amazon EBS volumes are impaired, and to help you control how a potentially inconsistent volume is handled. Our current use case requires us to pull data about EBS volume metadata, which is not possible with this feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/\">https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html</a></p>\n",
            "answers": [
                "<p>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
                "<p>Amazon EBS CloudWatch metrics are disabled</p>",
                "<p>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
                "<p>EBS volume status checks are disabled</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A multi-national company runs its technology operations on AWS Cloud. As part of their storage solution, they use a large number of EBS volumes, with AWS Config and CloudTrail activated. A manager has tried to find the user name that created an EBS volume by searching CloudTrail events logs but wasn't successful.\n\nAs a Developer Associate, which of the following would you recommend as the correct solution?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727598,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p>\n\n<p>How CloudTrail works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it does not help in user activity logging.</p>\n\n<p><strong>AWS X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray is a very important tool in troubleshooting but is not useful in logging user activity.</p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This does not log User activity at the account level.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n",
            "relatedLectureIds": "",
            "question": "<p>An AWS CodePipeline was configured to be triggered by Amazon CloudWatch Events. Recently the pipeline failed and upon investigation, the Team Lead noticed that the source was changed from AWS CodeCommit to Amazon Simple Storage Service (S3). The Team Lead has requested you to find the user who had made the changes.</p>\n\n<p>Which service will help you solve this?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Amazon CloudWatch</p>",
                "<p>Amazon Inspector</p>",
                "<p>AWS X-Ray</p>",
                "<p>AWS CloudTrail</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An AWS CodePipeline was configured to be triggered by Amazon CloudWatch Events. Recently the pipeline failed and upon investigation, the Team Lead noticed that the source was changed from AWS CodeCommit to Amazon Simple Storage Service (S3). The Team Lead has requested you to find the user who had made the changes.\n\nWhich service will help you solve this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727600,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an LSI</strong></p>\n\n<p>LSI stands for Local Secondary Index. Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Call Scan</strong> - Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue a Scan requests again.</p>\n\n<p><strong>Create a GSI</strong> - GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.</p>\n\n<p><strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Create a GSI</p>",
                "<p>Call Scan</p>",
                "<p>Create a LSI</p>",
                "<p>Migrate away from DynamoDB</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate sort key. This option must be added when first creating tables otherwise changes cannot be made afterward.</p>\n\n<p>Which of the following actions should you take?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate sort key. This option must be added when first creating tables otherwise changes cannot be made afterward.\n\nWhich of the following actions should you take?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727602,
        "assessment_type": "multi-select",
        "prompt": {
            "question": "<p>An organization with high data volume workloads have successfully moved to DynamoDB after having many issues with traditional database systems. However, a few months into production, DynamoDB tables are consistently recording high latency.</p>\n\n<p>As a Developer Associate, which of the following would you suggest to reduce the latency? (Select two)</p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</p>",
                "<p>Consider using Global tables if your application is accessed by globally distributed users</p>",
                "<p>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</p>",
                "<p>Use eventually consistent reads in place of strongly consistent reads whenever possible</p>",
                "<p>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup, and restore and in-memory caching for internet-scale applications.</p>\n\n<p><strong>Consider using Global tables if your application is accessed by globally distributed users</strong> - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users. So, reducing the distance between the client and the DynamoDB endpoint is an important performance fix to be considered.</p>\n\n<p><strong>Use eventually consistent reads in place of strongly consistent reads whenever possible</strong> - If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</strong> - This statement is incorrect. The right way is to reduce the request timeout settings. This causes the client to abandon high latency requests after the specified time period and then send a second request that usually completes much faster than the first.</p>\n\n<p><strong>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</strong> - This is not correct. When you're not making requests, consider having the client send dummy traffic to a DynamoDB table. Alternatively, you can reuse client connections or use connection pooling. All of these techniques keep internal caches warm, which helps keep latency low.</p>\n\n<p><strong>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</strong> - This is not correct. If your traffic is read-heavy, consider using a caching service such as DynamoDB Accelerator (DAX). DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n"
        },
        "correct_response": [
            "b",
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "An organization with high data volume workloads have successfully moved to DynamoDB after having many issues with traditional database systems. However, a few months into production, DynamoDB tables are consistently recording high latency.\n\nAs a Developer Associate, which of the following would you suggest to reduce the latency? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727604,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "question": "<p>A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process.</p>\n\n<p>Which of the following will you suggest?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations</strong> - You must use saved configurations to migrate an Elastic Beanstalk environment between AWS accounts.\nYou can save your environment's configuration as an object in Amazon Simple Storage Service (Amazon S3) that can be applied to other environments during environment creation, or applied to a running environment. Saved configurations are YAML formatted templates that define an environment's platform version, tier, configuration option settings, and tags.</p>\n\n<p>Download the saved configuration to your local machine. Change your account-specific parameters in the downloaded configuration file, and then save the changes. For example, change the key pair name, subnet ID, or application name (such as application-b-name). Upload the saved configuration from your local machine to an S3 bucket in Team B's account. From this account, create a new Beanstalk application by choosing 'Saved Configurations' from the navigation panel.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</strong> - There is no direct Export and Import\noption for migrating Elastic Beanstalk configurations.</p>\n\n<p><strong>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</strong> - This is an incorrect statement.</p>\n\n<p><strong>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of the Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</strong> - This contradicts the explanation provided earlier.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html</a></p>\n",
            "answers": [
                "<p>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</p>",
                "<p>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</p>",
                "<p>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</p>",
                "<p>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process.\n\nWhich of the following will you suggest?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727606,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</strong> - Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.</p>\n\n<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Someone could have created a new notification configuration and that has overridden your existing configuration</strong> - It is possible that the configuration can be overridden. But, in the current scenario, the team lead is receiving notifications for most of the events, which nullifies the claim that the configuration is overridden.</p>\n\n<p><strong>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</strong> - This is an incorrect statement. If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p><strong>Your notification action is writing to the same bucket that triggers the notification</strong> - If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. But it will not result in missing events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
            "answers": [
                "<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</p>",
                "<p>Someone could have created a new notification configuration and that has overridden your existing configuration</p>",
                "<p>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</p>",
                "<p>Your notification action is writing to the same bucket that triggers the notification</p>"
            ],
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>Your team-mate has configured an Amazon S3 event notification for an S3 bucket that holds sensitive audit data of a firm. As the Team Lead, you are receiving the SNS notifications for every event in this bucket. After validating the event data, you realized that few events are missing.</p>\n\n<p>What could be the reason for this behavior and how to avoid this in the future?</p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your team-mate has configured an Amazon S3 event notification for an S3 bucket that holds sensitive audit data of a firm. As the Team Lead, you are receiving the SNS notifications for every event in this bucket. After validating the event data, you realized that few events are missing.\n\nWhat could be the reason for this behavior and how to avoid this in the future?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727608,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A startup manages its Cloud resources with Elastic Beanstalk. The environment consists of few Amazon EC2 instances, an Auto Scaling Group (ASG), and an Elastic Load Balancer. Even after the Load Balancer marked an EC2 instance as unhealthy, the ASG has not replaced it with a healthy instance.</p>\n\n<p>As a Developer, suggest the necessary configurations to automate the replacement of unhealthy instance.</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</strong> - By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</strong> - This is an incorrect statement. Status checks, by definition, cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance.</p>\n\n<p><strong>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</strong> - Incorrect statement. As discussed above, if the health check type of ASG is changed from EC2 to ELB, Auto Scaling will be able to replace the unhealthy instance.</p>\n\n<p><strong>The ping path field of the Load Balancer is configured incorrectly</strong> - Ping path is a health check configuration field of Elastic Load Balancer. If the ping path is configured wrong, ELB will not be able to reach the instance and hence will consider the instance unhealthy. However, this would then apply to all instances, not just once instance. So it does not address the issue given in the use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</p>",
                "<p>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</p>",
                "<p>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</p>",
                "<p>The ping path field of the Load Balancer is configured incorrectly</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A startup manages its Cloud resources with Elastic Beanstalk. The environment consists of few Amazon EC2 instances, an Auto Scaling Group (ASG), and an Elastic Load Balancer. Even after the Load Balancer marked an EC2 instance as unhealthy, the ASG has not replaced it with a healthy instance.\n\nAs a Developer, suggest the necessary configurations to automate the replacement of unhealthy instance.",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727610,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a dead-letter queue to handle message processing failures</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.</p>\n\n<p><strong>Use short polling to handle message processing failures</strong></p>\n\n<p><strong>Use long polling to handle message processing failures</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>An e-commerce company uses Amazon SQS queues to decouple their application architecture. The development team has observed message processing failures for an edge case scenario when a user places an order for a particular product ID, but the product ID is deleted, thereby causing the application code to fail.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address such message failures?</p>\n",
            "answers": [
                "<p>Use a temporary queue to handle message processing failures</p>",
                "<p>Use a dead-letter queue to handle message processing failures</p>",
                "<p>Use short polling to handle message processing failures</p>",
                "<p>Use long polling to handle message processing failures</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "An e-commerce company uses Amazon SQS queues to decouple their application architecture. The development team has observed message processing failures for an edge case scenario when a user places an order for a particular product ID, but the product ID is deleted, thereby causing the application code to fail.\n\nAs a Developer Associate, which of the following solutions would you recommend to address such message failures?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727612,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>You manage a group of developers that are experienced with the AWS SDK for Java. You have given them a requirement to build a state machine workflow where each state executes an AWS Lambda function written in Java. Data payloads of 1KB in size will be passed between states and should allow for two retry attempts if the state fails.</p>\n\n<p>Which of the following options will assist your developers with this requirement?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>AWS Step Functions</p>",
                "<p>CloudWatch Rules</p>",
                "<p>Amazon Simple Workflow Service</p>",
                "<p>AWS ECS</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Step Functions</strong></p>\n\n<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.</p>\n\n<p>Using Step Functions, you can design and run workflows that stitch together services such as AWS Lambda and Amazon ECS into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch Rules</strong> - CloudWatch rules can integrate with an AWS Lambda function on a schedule. You can send the matching events to an AWS Step Functions state machine to start a workflow responding to the event of interest but CloudWatch rules alone do not create a state machine.</p>\n\n<p><strong>Amazon Simple Workflow Service</strong> - Amazon Simple Workflow Service is similar to AWS Step functions. With Amazon Simple Workflow Service you can write a program that separates activity steps, allows for more control but increases the complexity of the application. With Amazon Simple Workflow Service you create decider programs and with Step Functions, you define state machines.</p>\n\n<p><strong>AWS ECS</strong> - The underlying work in your state machine is done by tasks. A task performs work by using an activity, a Lambda function, or by sending parameters to the API actions of other services. You can use AWS ECS to host an activity.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You manage a group of developers that are experienced with the AWS SDK for Java. You have given them a requirement to build a state machine workflow where each state executes an AWS Lambda function written in Java. Data payloads of 1KB in size will be passed between states and should allow for two retry attempts if the state fails.\n\nWhich of the following options will assist your developers with this requirement?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727614,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>An analytics company is using Kinesis Data Streams (KDS) to process automobile health-status data from the taxis managed by a taxi ride-hailing service. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest for improving the performance for the given use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Kinesis Data Streams Fanout\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
            "answers": [
                "<p>Swap out Kinesis Data Streams with SQS Standard queues</p>",
                "<p>Swap out Kinesis Data Streams with SQS FIFO queues</p>",
                "<p>Use Enhanced Fanout feature of Kinesis Data Streams</p>",
                "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Security",
        "question_plain": "An analytics company is using Kinesis Data Streams (KDS) to process automobile health-status data from the taxis managed by a taxi ride-hailing service. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.\n\nAs a Developer Associate, which of the following options would you suggest for improving the performance for the given use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727616,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>System will trigger CloudWatch alarms to AWS support</p>",
                "<p>The desired capacity will go up to 4 and the maximum capacity will stay at 3</p>",
                "<p>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</p>",
                "<p>System will keep running as is</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A company has configured an Auto Scaling group with health checks. The configuration is set to the desired capacity value of 3 and maximum capacity value of 3. The EC2 instances of your Auto Scaling group are configured to scale when CPU utilization is at 60 percent and is now running at 80 percent utilization.</p>\n\n<p>Which of the following will take place?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>System will keep running as is</strong></p>\n\n<p>You are already running at max capacity. After you have created your Auto Scaling group, the Auto Scaling group starts by launching enough EC2 instances to meet its minimum capacity (or its desired capacity, if specified). If there are no other scaling conditions attached to the Auto Scaling group, the Auto Scaling group maintains this number of running instances even if an instance becomes unhealthy.</p>\n\n<p>Setting Capacity Limits for Your Auto Scaling Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will stay at 3</strong> - The desired capacity cannot go over the maximum capacity.</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</strong> - The maximum capacity cannot change on its own just because the desired capacity has been set to a higher value. You will have to make those changes to the maximum capacity manually.</p>\n\n<p><strong>System will trigger CloudWatch alarms to AWS support</strong> - This option has been added as a distractor. You already have alarms configured based on rules but AWS support will not intervene for you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n",
            "relatedLectureIds": ""
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A company has configured an Auto Scaling group with health checks. The configuration is set to the desired capacity value of 3 and maximum capacity value of 3. The EC2 instances of your Auto Scaling group are configured to scale when CPU utilization is at 60 percent and is now running at 80 percent utilization.\n\nWhich of the following will take place?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727618,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>After you send messages to a queue, you can receive and delete them. When you request messages from a queue, you can't specify which messages to retrieve. Instead, you specify the maximum number of messages (up to 10) that you want to retrieve.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>5</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>100</strong></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "answers": [
                "<p>100</p>",
                "<p>5</p>",
                "<p>20</p>",
                "<p>10</p>"
            ],
            "question": "<p>Your application sends messages to an Amazon Simple Queue Service (SQS) queue frequently, which are then polled by another application that specifies which message to retrieve.</p>\n\n<p>Which of the following options describe the maximum number of messages that can be retrieved at one time?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your application sends messages to an Amazon Simple Queue Service (SQS) queue frequently, which are then polled by another application that specifies which message to retrieve.\n\nWhich of the following options describe the maximum number of messages that can be retrieved at one time?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727620,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>You're a developer maintaining a web application written in .NET. The application makes references to public objects in a public S3 accessible bucket using a public URL. While doing a code review your colleague advises that the approach is not a best practice because some of the objects contain private data. After the administrator makes the S3 bucket private you can no longer access the S3 objects but you would like to create an application that will enable people to access some objects as needed with a time policy constraint.</p>\n\n<p>Which of the following options will give access to the objects?</p>\n",
            "answers": [
                "<p>Using bucket policy</p>",
                "<p>Using pre-signed URL</p>",
                "<p>Using Routing Policy</p>",
                "<p>Using IAM policy</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>\"Using pre-signed URL\"</p>\n\n<p>All objects by default are private, with object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p>Incorrect:</p>\n\n<p>\"Using bucket policy\" - You can use this policy to limit users from a source IP address however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>\"Using Routing Policy\" - This concept applies to DNS in Route 53, so this option is ruled out.</p>\n\n<p>\"Using IAM policy\" - You can use IAM policy to grant access toa specific bucket however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
            "relatedLectureIds": ""
        },
        "correct_response": [
            "b"
        ],
        "section": "Security",
        "question_plain": "You're a developer maintaining a web application written in .NET. The application makes references to public objects in a public S3 accessible bucket using a public URL. While doing a code review your colleague advises that the approach is not a best practice because some of the objects contain private data. After the administrator makes the S3 bucket private you can no longer access the S3 objects but you would like to create an application that will enable people to access some objects as needed with a time policy constraint.\n\nWhich of the following options will give access to the objects?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727622,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Define an <code>appspec.yml</code> file in the root directory</p>",
                "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
                "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
                "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong></p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>A build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - The file is correct but must be in the root directory.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n",
            "question": "<p>As a site reliability engineer, you work on building and running large-scale, distributed, fault-tolerant systems in the cloud using automation. You have just replaced the company's Jenkins based CI/CD platform with AWS CodeBuild and would like to programmatically define your build steps.</p>\n\n<p>Which of the following options should you choose?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": ""
        },
        "correct_response": [
            "b"
        ],
        "section": "Deployment",
        "question_plain": "As a site reliability engineer, you work on building and running large-scale, distributed, fault-tolerant systems in the cloud using automation. You have just replaced the company's Jenkins based CI/CD platform with AWS CodeBuild and would like to programmatically define your build steps.\n\nWhich of the following options should you choose?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727624,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>10</p>",
                "<p>40</p>",
                "<p>20</p>",
                "<p>5</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "question": "<p>You are a software engineer working for an IT company and are asked to contribute to a growing internal application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 4 KB in size each.</p>\n\n<p>How many Read Capacity Units (RCUs) are needed?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>10</strong></p>\n\n<p>One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 4KB / 4 KB = 1 read capacity unit.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second</p>\n\n<p>So, in the above case, 1 x 10 = 10 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>40</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>5</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a software engineer working for an IT company and are asked to contribute to a growing internal application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 4 KB in size each.\n\nHow many Read Capacity Units (RCUs) are needed?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727626,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A development team has inherited a web application running in the us-east-1 region with three availability zones (us-east-1a, us-east1-b, and us-east-1c) whose incoming web traffic is routed by a load balancer. When one of the EC2 instances hosting the web application crashes, the team realizes that the load balancer continues to route traffic to that instance causing intermittent issues.</p>\n\n<p>Which of the following should the development team do to minimize this problem?</p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Enable Health Checks</p>",
                "<p>Enable Stickiness</p>",
                "<p>Enable Multi AZ deployments</p>",
                "<p>Enable SSL</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Health Checks</strong></p>\n\n<p>To discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService.</p>\n\n<p>Load Balancer Health Checks:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Stickiness</strong> - Stickiness enables the load balancer to bind a user's session to a specific instance, it cannot be used for gauging the health of an instance.</p>\n\n<p><strong>Enable Multi-AZ deployments</strong> - It's a good practice to provision instances in more than one availability zone however you still need a way to check the health status of the instances, so this option is incorrect.</p>\n\n<p><strong>Enable SSL</strong> - This option has been added as a distractor. SSL encrypts the transmission of data between a web server and a browser.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n"
        },
        "correct_response": [
            "a"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A development team has inherited a web application running in the us-east-1 region with three availability zones (us-east-1a, us-east1-b, and us-east-1c) whose incoming web traffic is routed by a load balancer. When one of the EC2 instances hosting the web application crashes, the team realizes that the load balancer continues to route traffic to that instance causing intermittent issues.\n\nWhich of the following should the development team do to minimize this problem?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727628,
        "assessment_type": "multi-select",
        "prompt": {
            "explanation": "<p>Correct options:</p>\n\n<p><strong><code>$(aws ecr get-login --no-include-email)</code></strong></p>\n\n<p><strong><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong></p>\n\n<p>The get-login command retrieves a token that is valid for a specified registry for 12 hours, and then it prints a docker login command with that authorization token. You can execute the printed command to log in to your registry with Docker, or just run it automatically using the $() command wrapper. After you have logged in to an Amazon ECR registry with this command, you can use the Docker CLI to push and pull images from that registry until the token expires. The docker pull command is used to pull an image from the ECR registry.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></strong> - You cannot login to AWS ECR this way. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are only used by the CLI and not by docker.</p>\n\n<p><strong><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - docker push here is the wrong answer, you need to use docker pull.</p>\n\n<p><strong><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - This is a docker command that is used to build Docker images from a Dockerfile.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html\">https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html</a></p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "question": "<p>You are working for a technology startup building web and mobile applications. You would like to pull Docker images from the ECR repository called <code>demo</code> so you can start running local tests against the latest application version.</p>\n\n<p>Which of the following commands must you run to pull existing Docker images from ECR? (Select two)</p>\n",
            "answers": [
                "<p><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
                "<p><code>$(aws ecr get-login --no-include-email)</code></p>",
                "<p><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></p>",
                "<p><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
                "<p><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>"
            ]
        },
        "correct_response": [
            "b",
            "a"
        ],
        "section": "Deployment",
        "question_plain": "You are working for a technology startup building web and mobile applications. You would like to pull Docker images from the ECR repository called demo so you can start running local tests against the latest application version.\n\nWhich of the following commands must you run to pull existing Docker images from ECR? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727630,
        "assessment_type": "multi-select",
        "prompt": {
            "question": "<p>A video streaming application uses Amazon CloudFront for its data distribution. The development team has decided to use CloudFront with origin failover for high availability.</p>\n\n<p>Which of the following options are correct while configuring CloudFront with Origin Groups? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "relatedLectureIds": "",
            "answers": [
                "<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</p>",
                "<p>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</p>",
                "<p>To set up origin failover, you must have a distribution with at least three origins</p>",
                "<p>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</p>",
                "<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</p>"
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</strong></p>\n\n<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails.</p>\n\n<p><strong>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</strong></p>\n\n<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD, or OPTIONS. CloudFront does not failover when the viewer sends a different HTTP method (for example POST, PUT, and so on).</p>\n\n<p>How origin failover works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</strong> - When there’s a cache miss, CloudFront routes the request to the primary origin in the origin group. When there’s a cache hit, CloudFront returns the requested file.</p>\n\n<p><strong>To set up origin failover, you must have a distribution with at least three origins</strong> - Two origins are enough to set up an origin failover.</p>\n\n<p><strong>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</strong> - To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Only one origin can be set as primary.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n"
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Refactoring",
        "question_plain": "A video streaming application uses Amazon CloudFront for its data distribution. The development team has decided to use CloudFront with origin failover for high availability.\n\nWhich of the following options are correct while configuring CloudFront with Origin Groups? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727632,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
                "<p>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</p>",
                "<p>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
                "<p>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An investment firm wants to continuously generate time-series analytics of the stocks being purchased by its customers. The firm wants to build a live leaderboard with real-time analytics for these in-demand stocks.</p>\n\n<p>Which of the following represents a fully managed solution to address this use-case?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security.</p>\n\n<p>Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services.</p>\n\n<p>Amazon Kinesis Data Analytics provides built-in functions to filter, aggregate, and transform streaming data for advanced analytics. It processes streaming data with sub-second latencies, enabling you to analyze and respond to incoming data and events in real-time.</p>\n\n<p>Amazon Kinesis Data Analytics is serverless; there are no servers to manage. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to process incoming data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams needs manual provisioning of shards and a planning of shard requirements. The use-case clearly states that the company wants a fully managed solution as soon as possible with minimal effort. Kinesis Firehose is fully managed and requires no user input in provisioning of resources.</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</strong> - The Amazon Kinesis Client Library (KCL) is a pre-built library that helps you build consumer applications for reading and processing data from an Amazon Kinesis data stream. The KCL handles complex issues such as adapting to changes in data stream volume, load balancing streaming data, coordinating distributed services, and processing data with fault-tolerance. The KCL enables you to focus on business logic while building applications.</p>\n\n<p>If you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analytics. Use KCL if you need to build a custom processing solution whose requirements are not met by Kinesis Data Analytics, and you can manage the resulting consumer application.</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is used for running analytics on S3 based data. For running analytics on real-time streaming data, Kinesis Data Analytics is the right fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An investment firm wants to continuously generate time-series analytics of the stocks being purchased by its customers. The firm wants to build a live leaderboard with real-time analytics for these in-demand stocks.\n\nWhich of the following represents a fully managed solution to address this use-case?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727634,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</strong></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</strong> - Access Denied error is thrown when the accessing system does not have enough permissions. Since data was getting ingested into Data Streams before reaching the capacity, this error is not possible.</p>\n\n<p><strong>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</strong> - Partition key is used to segregate and route records to different shards of a data stream. A partition key is specified by your data producer while adding data to an Amazon Kinesis data stream. The use case talks about provisioning only one shard. It is not possible to set up more shards by simply changing the partition key. Hence, this choice is incorrect.</p>\n\n<p><strong>Contact AWS support to request an increase in the number of shards</strong> - This is a made-up option that acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
            "answers": [
                "<p>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</p>",
                "<p>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</p>",
                "<p>Contact AWS support to request an increase in the number of shards</p>",
                "<p>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</p>"
            ],
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A development team is configuring Kinesis Data Streams for ingesting real-time data from various appliances. The team has declared a shard capacity of one to test the configuration.</p>\n\n<p>What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A development team is configuring Kinesis Data Streams for ingesting real-time data from various appliances. The team has declared a shard capacity of one to test the configuration.\n\nWhat happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727636,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A development team has been using Amazon S3 service as an object store. With Amazon S3 turning strongly consistent, the team wants to understand the impact of this change on its data storage practices.</p>\n\n<p>As a developer associate, can you identify the key characteristics of the strongly consistent data model followed by S3? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</strong> - Bucket configurations have an eventual consistency model. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.</p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</strong> - Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.</p>\n\n<p>Amazon S3 data consistency model:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</strong></p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</strong> -</p>\n\n<p>These two options highlight an eventually consistent behavior. Amazon S3 is now strongly consistent and will not return any data as the object has been deleted. So both these options are incorrect.</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</strong> - Amazon S3 will return the new data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a></p>\n",
            "answers": [
                "<p>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</p>",
                "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</p>",
                "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</p>",
                "<p>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</p>",
                "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</p>"
            ]
        },
        "correct_response": [
            "a",
            "e"
        ],
        "section": "Refactoring",
        "question_plain": "A development team has been using Amazon S3 service as an object store. With Amazon S3 turning strongly consistent, the team wants to understand the impact of this change on its data storage practices.\n\nAs a developer associate, can you identify the key characteristics of the strongly consistent data model followed by S3? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727638,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>",
                "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>",
                "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n\n</code></pre>",
                "<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n\n</code></pre>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>You can use query string conditions to configure rules that route requests based on key/value pairs or values in the query string. The match evaluation is not case-sensitive. The following wildcard characters are supported: * (matches 0 or more characters) and ? (matches exactly 1 character). You can specify conditions when you create or modify a rule.</p>\n\n<p>Query parameters are often used along with the path component of the URL for applying a special logic to the resource being fetched.</p>\n\n<p>The query string component starts after the first \"?\" in a URI. Typically query strings contain key-value pairs separated by a delimiter \"&amp;\". Example: http://example.com/path/to/page?version=A&amp;gender=female</p>\n\n<p>The example condition given in the question is satisfied by requests with a query string that includes either a key/value pair of \"version=v1\" or any key set to \"example\".</p>\n\n<p>Incorrect options:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>These two options are malformed and are incorrect.</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n</code></pre>\n\n<p>** - This action redirects an HTTP request to an HTTPS request on port 443, with the same hostname, path, and query string as the HTTP request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions</a></p>\n",
            "question": "<p>A developer is configuring the redirect actions for an Application Load Balancer. The developer stumbled upon the following snippet of code.</p>\n\n<p>Which of the following is an example of a query string condition that the developer can use on AWS CLI?</p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "A developer is configuring the redirect actions for an Application Load Balancer. The developer stumbled upon the following snippet of code.\n\nWhich of the following is an example of a query string condition that the developer can use on AWS CLI?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727640,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A developer has just completed configuring the Application Load Balancer for the EC2 instances. Just as he started testing his configuration, he realized that he has missed assigning target groups to his ALB.</p>\n\n<p>Which error code should he expect in his debug logs?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503</strong> - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500</strong> - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules.</p>\n\n<p><strong>HTTP 504</strong> - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed.</p>\n\n<p><strong>HTTP 403</strong> - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
            "answers": [
                "<p>HTTP 500</p>",
                "<p>HTTP 503</p>",
                "<p>HTTP 504</p>",
                "<p>HTTP 403</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A developer has just completed configuring the Application Load Balancer for the EC2 instances. Just as he started testing his configuration, he realized that he has missed assigning target groups to his ALB.\n\nWhich error code should he expect in his debug logs?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727642,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A developer in your company has configured a build using AWS CodeBuild. The build fails and the developer needs to quickly troubleshoot the issue to see which commands or settings located in the BuildSpec file are causing an issue.</p>\n\n<p>Which approach will help them accomplish this?</p>\n",
            "answers": [
                "<p>Freeze the CodeBuild during its next execution</p>",
                "<p>SSH into the CodeBuild Docker container</p>",
                "<p>Run AWS CodeBuild locally using CodeBuild Agent</p>",
                "<p>Enable detailed monitoring</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Run AWS CodeBuild locally using CodeBuild Agent</strong></p>\n\n<p>AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.</p>\n\n<p>With the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine.</p>\n\n<p>By building an application on a local machine you can:</p>\n\n<p>Test the integrity and contents of a buildspec file locally.</p>\n\n<p>Test and build an application locally before committing.</p>\n\n<p>Identify and fix errors quickly from your local development environment.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSH into the CodeBuild Docker container</strong> - It is not possible to SSH into the CodeBuild Docker container, that's why you should test and fix errors locally.</p>\n\n<p><strong>Freeze the CodeBuild during its next execution</strong> - You cannot freeze the CodeBuild process but you can stop it. Please see more details on - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html</a></p>\n\n<p><strong>Enable detailed monitoring</strong> - Detailed monitoring is available for EC2 instances. You do not enable detailed monitoring but you can specify output logs to be captured via CloudTrail.</p>\n\n<p>AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html</a></p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A developer in your company has configured a build using AWS CodeBuild. The build fails and the developer needs to quickly troubleshoot the issue to see which commands or settings located in the BuildSpec file are causing an issue.\n\nWhich approach will help them accomplish this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727644,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Instance might be part of Auto Scaling Group and hence re-launched similar instance</strong> - Auto Scaling groups can be configured to launch an instance to replace an instance that is undergoing maintenance. This could have been the reason why an instance of the same type got launched automatically. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. If you wish to terminate an instance that is part of Auto Scaling Group, the configuration of the group should be changed to a reduced number of instances, so the automatic launch of instances does not happen when an unwanted instance is terminated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</strong> - This is an incorrect statement. If the user does not have enough permissions, then the action itself is unavailable for him. A user does not need root permissions to terminate an EC2 instance.</p>\n\n<p><strong>The instance could have been a part of the Application Load Balancer and hence was automatically started</strong> - Application Load Balancer is used to balance the incoming traffic requests equally among the available EC2 instances so keep the performance and availability at its best. ALBs are configured with Auto Scaling Groups, but this is not specified in the use-case. In the absence of Auto Scaling Group, ALB cannot launch instances by itself.</p>\n\n<p><strong>The instance could have been a part of Network Load Balancer and hence was automatically started</strong> - As explained above for ALB, a Network Load Balancer is not capable of launching instances by itself if it's not configured with an Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html</a></p>\n",
            "question": "<p>A developer while working on Amazon EC2 instances, realized that an instance was not needed and had shut it down. But another instance of the same type automatically got launched in the account.</p>\n\n<p>Which of the following options can attribute the given sequence of actions?</p>\n",
            "answers": [
                "<p>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</p>",
                "<p>Instance might be part of Auto Scaling Group and hence re-launched similar instance</p>",
                "<p>The instance could have been a part of Application Load Balancer and hence was automatically started</p>",
                "<p>The instance could have been a part of Network Load Balancer and hence was automatically started</p>"
            ]
        },
        "correct_response": [
            "b"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "A developer while working on Amazon EC2 instances, realized that an instance was not needed and had shut it down. But another instance of the same type automatically got launched in the account.\n\nWhich of the following options can attribute the given sequence of actions?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727646,
        "assessment_type": "multi-select",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>As a Developer Associate, you are responsible for the data management of the AWS Kinesis streams at your company. The security team has mandated stricter security requirements by leveraging mechanisms available with the Kinesis Data Streams service that won't require code changes on your end.</p>\n\n<p>Which of the following features meet the given requirements? (Select two)</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>Envelope Encryption</p>",
                "<p>SSE-C encryption</p>",
                "<p>Encryption in flight with HTTPS endpoint</p>",
                "<p>Client-Side Encryption</p>",
                "<p>KMS encryption for data at rest</p>"
            ],
            "explanation": "<p>Correct options:</p>\n\n<p><strong>KMS encryption for data at rest</strong></p>\n\n<p><strong>Encryption in flight with HTTPS endpoint</strong></p>\n\n<p>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C encryption</strong> - SSE-C is functionality in Amazon S3 where S3 encrypts your data, on your behalf, using keys that you provide. This does not apply for the given use-case.</p>\n\n<p><strong>Client-Side Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p><strong>Envelope Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\">https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html</a></p>\n"
        },
        "correct_response": [
            "e",
            "c"
        ],
        "section": "Security",
        "question_plain": "As a Developer Associate, you are responsible for the data management of the AWS Kinesis streams at your company. The security team has mandated stricter security requirements by leveraging mechanisms available with the Kinesis Data Streams service that won't require code changes on your end.\n\nWhich of the following features meet the given requirements? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727648,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudWatch Event Rules</strong></p>\n\n<p>Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nExamples of Amazon CloudWatch Events rules and targets:</p>\n\n<ol>\n<li><p>A rule that sends a notification when the instance state changes, where an EC2 instance is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that detects pipeline changes and invokes an AWS Lambda function.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</strong> - This is an incorrect statement. There is no such thing as CloudTrail Event Rule.</p>\n\n<p><strong>Use Lambda function with Amazon Simple Notification Service (SNS)</strong> - Lambda functions can be triggered by the use of CloudWatch Event Rules as discussed above. AWS CodePipeline does not trigger Lambda functions directly.</p>\n\n<p><strong>Use Lambda Event Rules</strong> - This is an incorrect statement. There is no such thing as Lambda Event Rule.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
            "answers": [
                "<p>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</p>",
                "<p>Use Lambda function with Amazon Simple Notification Service (SNS)</p>",
                "<p>Use Lambda Event Rules</p>",
                "<p>Use CloudWatch Event Rules</p>"
            ],
            "question": "<p>Your organization has developers that merge code changes regularly to an AWS CodeCommit repository. Your pipeline has AWS CodeCommit as the source and you would like to configure a rule that reacts to changes in CodeCommit.</p>\n\n<p>Which of the following options do you choose for this type of integration?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Monitoring and Troubleshooting",
        "question_plain": "Your organization has developers that merge code changes regularly to an AWS CodeCommit repository. Your pipeline has AWS CodeCommit as the source and you would like to configure a rule that reacts to changes in CodeCommit.\n\nWhich of the following options do you choose for this type of integration?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727650,
        "assessment_type": "multiple-choice",
        "prompt": {
            "explanation": "<p>Correct option:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong></p>\n\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p>To summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</p>",
                "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</p>",
                "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</p>",
                "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>An e-commerce application writes log files into Amazon S3. The application also reads these log files in parallel on a near real-time basis. The development team wants to address any data discrepancies that might arise when the application overwrites an existing log file and then tries to read that specific log file.</p>\n\n<p>Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?</p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "An e-commerce application writes log files into Amazon S3. The application also reads these log files in parallel on a near real-time basis. The development team wants to address any data discrepancies that might arise when the application overwrites an existing log file and then tries to read that specific log file.\n\nWhich of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727652,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>As a site reliability engineer, you are responsible for improving the company’s deployment by scaling and automating applications. As new application versions are ready for production you ensure that the application gets deployed to different sets of EC2 instances at different times allowing for a smooth transition.</p>\n\n<p>Using AWS CodeDeploy, which of the following options will allow you to do this?</p>\n",
            "answers": [
                "<p>CodeDeploy Hooks</p>",
                "<p>CodeDeploy Agent</p>",
                "<p>CodeDeploy Deployment Groups</p>",
                "<p>Define multiple CodeDeploy Applications</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeDeploy Deployment Groups</strong></p>\n\n<p>You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment. Most deployment group settings depend on the compute platform used by your application. Some settings, such as rollbacks, triggers, and alarms can be configured for deployment groups for any compute platform.</p>\n\n<p>In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeDeploy Agent</strong> - The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The agent connects the EC2 instances to the CodeDeploy service.</p>\n\n<p><strong>CodeDeploy Hooks</strong> - Hooks are found in the AppSec file used by AWS CodeDeploy to manage deployment. Hooks correspond to lifecycle events such as ApplicationStart, ApplicationStop, etc. to which you can assign a script.</p>\n\n<p><strong>Define multiple CodeDeploy Applications</strong> - This option has been added as a distractor. Instead, you want to use deployment groups to use the same deployment and maybe separate the times when a group of instances receives the software updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "As a site reliability engineer, you are responsible for improving the company’s deployment by scaling and automating applications. As new application versions are ready for production you ensure that the application gets deployed to different sets of EC2 instances at different times allowing for a smooth transition.\n\nUsing AWS CodeDeploy, which of the following options will allow you to do this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727654,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>Your company has a load balancer in a VPC configured to be internet facing. The public DNS name assigned to the load balancer is <code>myDns-1234567890.us-east-1.elb.amazonaws.com</code>. When your client applications first load they capture the load balancer DNS name and then resolve the IP address for the load balancer so that they can directly reference the underlying IP.</p>\n\n<p>It is observed that the client applications work well but unexpectedly stop working after a while. What is the reason for this?</p>\n",
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The load balancer is highly available and its public IP may change. The DNS name is constant</strong></p>\n\n<p>When your load balancer is created, it receives a public DNS name that clients can use to send requests. The DNS servers resolve the DNS name of your load balancer to the public IP addresses of the load balancer nodes for your load balancer. Never resolve the IP of a load balancer as it can change with time. You should always use the DNS name.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your security groups are not stable</strong> - You security groups to allow your load balancer to work with registered instances. It is stable if set correctly. If your application is working and stops after a while, the issue is not with the security groups.</p>\n\n<p><strong>You need to enable stickiness</strong> - This enables the load balancer to bind a user's session to a specific instance, so this has no impact on the issue described in the given use-case.</p>\n\n<p><strong>You need to disable multi-AZ deployments</strong> - This has been added as a distractor and this has no bearing on the use-case. The change is happening with the IP of the load balancer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html</a></p>\n",
            "answers": [
                "<p>The load balancer is highly available and its public IP may change. The DNS name is constant</p>",
                "<p>Your security groups are not stable</p>",
                "<p>You need to enable stickiness</p>",
                "<p>You need to disable multi-AZ deployments</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your company has a load balancer in a VPC configured to be internet facing. The public DNS name assigned to the load balancer is myDns-1234567890.us-east-1.elb.amazonaws.com. When your client applications first load they capture the load balancer DNS name and then resolve the IP address for the load balancer so that they can directly reference the underlying IP.\n\nIt is observed that the client applications work well but unexpectedly stop working after a while. What is the reason for this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727656,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</strong> - A stage is a named reference to a deployment, which is a snapshot of the API. You use a stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing.</p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</p>\n\n<p>With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</strong> - This is an incorrect option and has been added as a distractor.</p>\n\n<p><strong>Use an API Gateway Lambda authorizer to route API clients to the correct API version</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p><strong>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</strong> - Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked requestors. They are not meant for choosing the version of APIs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n",
            "question": "<p>A developer is creating a RESTful API service using an Amazon API Gateway with AWS Lambda integration. The service must support different API versions for testing purposes.</p>\n\n<p>As a Developer Associate, which of the following would you suggest as the best way to accomplish this?</p>\n",
            "answers": [
                "<p>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</p>",
                "<p>Use an API Gateway Lambda authorizer to route API clients to the correct API version</p>",
                "<p>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</p>",
                "<p>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</p>"
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Deployment",
        "question_plain": "A developer is creating a RESTful API service using an Amazon API Gateway with AWS Lambda integration. The service must support different API versions for testing purposes.\n\nAs a Developer Associate, which of the following would you suggest as the best way to accomplish this?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727658,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "question": "<p>A company wants to implement authentication for its new RESTful API service that uses Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to the authentication data in a DynamoDB table.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following would you recommend for implementing this authentication in API Gateway? </p>\n",
            "answers": [
                "<p>Develop an AWS Lambda authorizer that references the authentication data in the DynamoDB table</p>",
                "<p>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
                "<p>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
                "<p>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop an AWS Lambda authorizer that references the DynamoDB authentication table</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API.</p>\n\n<p>A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p>When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.</p>\n\n<p>There are two types of Lambda authorizers:</p>\n\n<ol>\n<li><p>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</p></li>\n<li><p>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, state variables, and $context variables.</p></li>\n</ol>\n\n<p>API Gateway Lambda authorization workflow:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - In API Gateway, a model defines the data structure of a payload. In API Gateway models are defined using the JSON schema draft 4. Models are not mandatory.</p>\n\n<p><strong>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - After setting up an API method, you must integrate it with an endpoint in the backend. A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action.</p>\n\n<p>An integration request is an HTTP request that API Gateway submits to the backend, passing along the client-submitted request data, and transforming the data, if necessary. The HTTP method (or verb) and URI of the integration request are dictated by the backend (that is, the integration endpoint). They can be the same as or different from the method request's HTTP method and URI, respectively.</p>\n\n<p><strong>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</strong> - As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.</p>\n\n<p>To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html</a></p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A company wants to implement authentication for its new RESTful API service that uses Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to the authentication data in a DynamoDB table.\n\nAs an AWS Certified Developer Associate, which of the following would you recommend for implementing this authentication in API Gateway?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727660,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>A development team has created AWS CloudFormation templates that are reusable by taking advantage of input parameters to name resources based on client names.</p>\n\n<p>You would like to save your templates on the cloud, which storage option should you choose?</p>\n",
            "answers": [
                "<p>EFS</p>",
                "<p>EBS</p>",
                "<p>S3</p>",
                "<p>ECR</p>"
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>S3</strong></p>\n\n<p>If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket.</p>\n\n<p>Selecting a stack template for CloudFormation:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS</strong> - An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. Amazon EBS is a recommended storage option when data must be quickly accessible and requires long-term persistence. EBS cannot be used for selecting a stack template for CloudFormation.</p>\n\n<p><strong>EFS</strong> - EFS is a file storage service where you mount the file system on an Amazon EC2 Linux-based instance which is not an option for CloudFormation.</p>\n\n<p><strong>ECR</strong> - Amazon ECR eliminates the need to operate your container repositories or worry about scaling the underlying infrastructure which does not apply to CloudFormation.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a></p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "A development team has created AWS CloudFormation templates that are reusable by taking advantage of input parameters to name resources based on client names.\n\nYou would like to save your templates on the cloud, which storage option should you choose?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727662,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>Your e-commerce company needs to improve its software delivery process and is moving away from the waterfall methodology. You decided that every application should be built using the best CI/CD practices and every application should be packaged and deployed as a Docker container. The Docker images should be stored in ECR and pushed with AWS CodePipeline and AWS CodeBuild.</p>\n\n<p>When you attempt to do this, the last step fails with an authorization issue. What is the most likely issue?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>The IAM permissions are wrong for the CodeBuild service</strong></p>\n\n<p>You can push your Docker or Open Container Initiative (OCI) images to an Amazon ECR repository with the docker push command.</p>\n\n<p>Amazon ECR users require permission to call ecr:GetAuthorizationToken before they can authenticate to a registry and push or pull any images from any Amazon ECR repository. Amazon ECR provides several managed policies to control user access at varying levels</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ECR repository is stale, you must delete and re-create it</strong> - You can delete a repository when you are done using it, stale is not a concept within ECR. This option has been added as a distractor.</p>\n\n<p><strong>CodeBuild cannot talk to ECR because of security group issues</strong> - A security group acts as a virtual firewall at the instance level and it is not related to pushing Docker images, so this option does not fit the given use-case.</p>\n\n<p><strong>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</strong> - The error Authorization is an indication that there is an access issue, therefore you should not look at your configuration first but rather permissions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a></p>\n",
            "answers": [
                "<p>CodeBuild cannot talk to ECR because of security group issues</p>",
                "<p>The ECR repository is stale, you must delete and re-create it</p>",
                "<p>The IAM permissions are wrong for the CodeBuild service</p>",
                "<p>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Deployment",
        "question_plain": "Your e-commerce company needs to improve its software delivery process and is moving away from the waterfall methodology. You decided that every application should be built using the best CI/CD practices and every application should be packaged and deployed as a Docker container. The Docker images should be stored in ECR and pushed with AWS CodePipeline and AWS CodeBuild.\n\nWhen you attempt to do this, the last step fails with an authorization issue. What is the most likely issue?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727664,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>You are a DynamoDB developer for an aerospace company that requires you to write 6 objects per second of 4.5KB in size each.</p>\n\n<p>What write capacity unit is needed for your project?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "answers": [
                "<p>46</p>",
                "<p>24</p>",
                "<p>15</p>",
                "<p>30</p>"
            ],
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>30</strong></p>\n\n<p>A write capacity unit represents one write per second, for an item up to 1 KB in size.</p>\n\n<p>Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. So, for the given use-case, each object is of size 4.5 KB, which will be rounded up to 5KB.</p>\n\n<p>Therefore, for 6 objects, you need 6x5 = 30 WCUs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>24</strong></p>\n\n<p><strong>15</strong></p>\n\n<p><strong>46</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n"
        },
        "correct_response": [
            "d"
        ],
        "section": "Development with AWS Services",
        "question_plain": "You are a DynamoDB developer for an aerospace company that requires you to write 6 objects per second of 4.5KB in size each.\n\nWhat write capacity unit is needed for your project?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727666,
        "assessment_type": "multiple-choice",
        "prompt": {
            "relatedLectureIds": "",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "question": "<p>Your web application front end consists of 5 EC2 instances behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.</p>\n\n<p>What should you do to identify the true IP address of the client?</p>\n",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Look into the X-Forwarded-For header in the backend</strong></p>\n\n<p>The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application.</p>\n\n<p><strong>Look into the X-Forwarded-Proto header in the backend</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.</p>\n\n<p><strong>Look into the client's cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n",
            "answers": [
                "<p>Look into the X-Forwarded-Proto header in the backend</p>",
                "<p>Modify the front-end of the website so that the users send their IP in the requests</p>",
                "<p>Look into the X-Forwarded-For header in the backend</p>",
                "<p>Look into the client's cookie</p>"
            ]
        },
        "correct_response": [
            "c"
        ],
        "section": "Refactoring",
        "question_plain": "Your web application front end consists of 5 EC2 instances behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.\n\nWhat should you do to identify the true IP address of the client?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727668,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>Amazon EC2 Spot Instances</p>",
                "<p>Amazon EC2 On Demand Instances</p>",
                "<p>Amazon EC2 Reserved Instances</p>",
                "<p>On-premise EC2 instance</p>"
            ],
            "relatedLectureIds": "",
            "question": "<p>Your team has just signed up an year-long contract with a client maintaining a three-tier web application, that needs to be moved to AWS Cloud. The application has steady traffic throughout the day and needs to be on a reliable system with no down-time or access issues. The solution needs to be cost-optimal for this startup.</p>\n\n<p>Which of the following options should you choose?</p>\n",
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 Reserved Instances</strong> - Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. You save money going with Reserved instances vs on-demand especially in a year's worth of time.</p>\n\n<p>Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. So, there is no performance difference between an On-Demand instance or a Reserved instance.</p>\n\n<p>How RIs work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instances are useful if your applications can be interrupted, like data analysis, batch jobs, background processing, and optional tasks. Spot instances can be pulled down anytime without prior notice. Hence, not the right choice for the current scenario.</p>\n\n<p><strong>Amazon EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. But, On-Demand instances cost a lot more than Reserved instances. Here, in our use case, we already know that the systems are required for a complete year, so making use of Reserved Instances discount makes a lot more sense.</p>\n\n<p><strong>On-premise EC2 instance</strong> - On-premise implies the client has to maintain the physical machines, their capacity provisioning and maintenance. Not an option when the client is planning to move to AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html</a></p>\n"
        },
        "correct_response": [
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "Your team has just signed up an year-long contract with a client maintaining a three-tier web application, that needs to be moved to AWS Cloud. The application has steady traffic throughout the day and needs to be on a reliable system with no down-time or access issues. The solution needs to be cost-optimal for this startup.\n\nWhich of the following options should you choose?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727670,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.</p>\n\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.</p>\n\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.</p>\n\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a></p>\n",
            "question": "<p>A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a developer associate, can you suggest a way to disable this flag while the instance is still running?</p>\n",
            "answers": [
                "<p>Set the <code>DeleteOnTermination</code> attribute to False using the command line</p>",
                "<p>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</p>",
                "<p>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</p>",
                "<p>Set the <code>DisableApiTermination</code> attribute of the instance using the API</p>"
            ],
            "relatedLectureIds": ""
        },
        "correct_response": [
            "a"
        ],
        "section": "Refactoring",
        "question_plain": "A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.\n\nAs a developer associate, can you suggest a way to disable this flag while the instance is still running?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727672,
        "assessment_type": "multi-select",
        "prompt": {
            "question": "<p>The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.</p>\n\n<p>Which of the following would you identify as correct for RDS Multi-AZ? (Select two)</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct options:</p>\n\n<p><strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong></p>\n\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:</p>\n\n<p>Perform maintenance on the standby.</p>\n\n<p>Promote the standby to primary.</p>\n\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p>\n\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.</p>\n\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.</p>\n\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n\n<p><strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
            "answers": [
                "<p>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</p>",
                "<p>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby</p>",
                "<p>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason</p>",
                "<p>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</p>",
                "<p>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "b",
            "c"
        ],
        "section": "Development with AWS Services",
        "question_plain": "The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.\n\nWhich of the following would you identify as correct for RDS Multi-AZ? (Select two)",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727674,
        "assessment_type": "multiple-choice",
        "prompt": {
            "answers": [
                "<p>SSE-S3</p>",
                "<p>SSE-C</p>",
                "<p>Client Side Encryption</p>",
                "<p>SSE-KMS</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
            "question": "<p>A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n\n<p>Which of the following options should you use?</p>\n",
            "relatedLectureIds": ""
        },
        "correct_response": [
            "a"
        ],
        "section": "Security",
        "question_plain": "A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.\n\nWhich of the following options should you use?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727676,
        "assessment_type": "multiple-choice",
        "prompt": {
            "feedbacks": [
                "",
                "",
                "",
                ""
            ],
            "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standard–IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.</p>\n\n<p>The Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, EFS standard storage class is not the right solution for the given use case.</p>\n\n<p><strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS volume cannot be accessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is needed in the use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html\">https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html</a></p>\n",
            "question": "<p>A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.</p>\n\n<p>Which of the following options represents the best solution for the given requirements?</p>\n",
            "relatedLectureIds": "",
            "answers": [
                "<p>Amazon Elastic File System (EFS) Standard–IA storage class</p>",
                "<p>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</p>",
                "<p>Amazon Elastic File System (EFS) Standard storage class</p>",
                "<p>Amazon Elastic Block Store (EBS)</p>"
            ]
        },
        "correct_response": [
            "a"
        ],
        "section": "Development with AWS Services",
        "question_plain": "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.\n\nWhich of the following options represents the best solution for the given requirements?",
        "related_lectures": []
    },
    {
        "_class": "assessment",
        "id": 38727678,
        "assessment_type": "multiple-choice",
        "prompt": {
            "question": "<p>A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.</p>\n\n<p>The rule *.sample.com matches which of the following?</p>\n",
            "relatedLectureIds": "",
            "explanation": "<p>Correct option:</p>\n\n<p><strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.</p>\n\n<p>A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)</p>\n\n<p>You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.</p>\n\n<p>The rule *.sample.com matches test.sample.com but doesn't match sample.com.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>sample.com</strong></p>\n\n<p><strong>sample.test.com</strong></p>\n\n<p><strong>SAMPLE.COM</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
            "answers": [
                "<p>sample.com</p>",
                "<p>sample.test.com</p>",
                "<p>SAMPLE.COM</p>",
                "<p>test.sample.com</p>"
            ],
            "feedbacks": [
                "",
                "",
                "",
                ""
            ]
        },
        "correct_response": [
            "d"
        ],
        "section": "Refactoring",
        "question_plain": "A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.\n\nThe rule *.sample.com matches which of the following?",
        "related_lectures": []
    }
]